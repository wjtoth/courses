\documentclass[12pt]{article}

\usepackage{hyperref}
\usepackage{apacite}
\usepackage{setspace}

\doublespace
\title{Working Title - CUT Research Paper}
\author{W. Justin Toth}
\begin{document}
\maketitle
\begin{abstract}
Research Question: ``How does assessment format impact student learning outcomes, and instructor measurement thereof''.

New Research Question: ``Can innovation assessment formats improve student learning outcomes, and an instructors capacity to measure their achievement.''

\textit{With a particular focus on mathematics, stem}
\end{abstract}

\section{Introduction}

\paragraph{}
A well-known truism in human social interaction is that how you say something often has as much, if not more, importance that the specific content of your speech. Yet when instructors are designing evalutions for their courses, it is all too common that the focus is squarely on what questions to ask to cover the course content, rather than how the problems should be presented to the students. In this survey we seek to synthesize the academic literature on the ``how'', rather than ``what'', of assessment methods and provide guidance for instructors on the impact that exam format can have on student learning outcomes and their ability to measure the attainment thereof.
\paragraph{}
In Section \ref{sec:written} we study the particulars of written finals. For written finals we will concentrate on two areas of focus in the literature. The first is on the use of multiple choice versus constructed response questions, and the second is on allowing students to use reference material such as the course textbook or handwritten notes during the exam. In Section \ref{sec:communication} we study the use of oral communication during exams. This breaks down in to two subsections. In Subsection \ref{subsec:oral} we survey the literature on implementing interview-style oral exams  between student and instructor in the classroom, and in Subsection \ref{subsec:collab} we discuss student-to-student collaboration in exams with a particular focus on the innovative two-stage exam format. We conclude by with Section \ref{sec:recommendations} where present recommendations for instructors on choosing exam formats for their courses based on the previously surveyed literature.
\section{Individual Written Finals}\label{sec:written}
\subsection{Multiple Choice}
\paragraph{}
When preparing a written final exam, a very basic choice appears early on in the design of the test. That is the choice between having students select their response to a question or generate their response. This is the decision between using a Multiple Choice question format or using a Constructed Response question format. In a Multiple Choice question, students are given the question prompt and a finite list, usually of length between two and five, of potential responses they are to select from to answer the question prompt. In a Constructed Response question, the students are given the question prompt, but instead of a list of responses they are simply given a blank space in which to write their response. Constructed Response questions can vary more significantly in format than Multiple Choice, encompassing for example fill-in-the-bank, short answer, and essay questions. Both formats have their potential advantages and disadvantages, which have been well-studied in the education research literature.
\paragraph{}
The literature has highlighted the pragmatic benefits of Multiple Choice questions~\cite{simkin2005multiple}. With machine-grading, Multiple Choice exams can be graded very quickly, shortening the delay between students writing the exam and receiving their feedback. Automated grading of Multiple Choice problems also reduces the grading burden of instructors and teaching assistants, freeing up their time to (hopefully) concentrate on improving the quality of the course in other ways. Multiple Choice exams also have the advantage of providing an objective scoring system during grading, removing all grader subjectivity from the process. This can increase the perceived fairness of the exam. From the perspective of students, Multiple Choice exams have the advantage of inducing less anxiety. Students perceive Multiple Choice exams as being easier and quicker to prepare for, and are comforted by having potential answers available to them and the option they have to guess at a solution~\cite{zeidner1987essay}.
\paragraph{}
Despite the advantages above, Multiple Choice questions have many disadvantages over Constructed Response questions. It is difficult to assess critical thinking skills with Multiple Choice questions~\cite{martinez1999cognition}, and it is essentially impossible to assess creativity with Multiple Choice questions. Students seem to be aware of this on some level, expressing a perception that Multiple Choice exams assess a more shallow, knowledge-level understanding of the course material~\cite{scouller1998influence}. In line with this viewpoint, students predominantly favor shallow-learning strategies like memorization for exams consisting primarily of Multiple Choice problems. In contrast, students perceive Construhcted Response exams as testing deeper understanding, and use deeper learning strategies to study for Constructed Response exams~\cite{scouller1998influence}
\paragraph{}
Stanger-Hall hypothesized that Multiple Choice exams hinder the development of critical-thinking skills in her introductory biology class students~\cite{stanger2012multiple}. Traditionally these classes are evaluated using Multiple Choice only exams. To test this hypothesis, she changed the exams to feature Multiple Choice problems alongside several Constructed Response problems of the short answer variety. She predicted that this adaptation would result a change in how students studied for the exams, and their performance on critical-thinking questions.
\paragraph{}
The author defined critical-thinking skills as those skills associated with the middle levels of Bloom's taxonomy: application, analysis, evaluation, and synthesis. The experiment was performed using two sections of introductory biology given by the same instructor in the same semester to large classes of about $200$ students. The sections were entirely indentical in course material (lectures, assignments, readings) with the exception of the final exam. One section received a Multiple Choice only final, and the other received a Multiple Choice plus short answer Constructed Response final. Through a series of four surveys, Stanger-Hall evaluated student approaches to studying and perception of the importance of critical-thinking skills in introductory biology. The Constructed Response group reported significantly higher cognitively active learning behaviours during weeks outside of the exam study period, and this was reflected in their significantly higher test scores, in particular on questions associated with critical-thinking levels of Bloom's taxonomy~\cite{stanger2012multiple}.
\paragraph{}
A major weakness of Multiple Choice exams is their tendency to over-estimate student learning achievement~\cite{funk2011multiple}. The authors conjectured that grades from Multiple Choice exams were artificially inflated due to the relative ease of recognizing a correct answer versus generating it oneself, and students employing educated guessing strategies on exams. They conducted a study wherein $50$ students in a personality psychology course were randomly assigned to take a $10$-time short answer Constructed Response pretest or protest on a $50$ question Multiple Choice exam. To control for content and grading subjectivity, they used the same question prompts on the short answer and Multiple Choice problems.
\paragraph{}
The results were very strong. The mean score on short answer pretests was $34.24$ percent, followed by $77.1$ percent on the Multiple Choice exam. For the group where short answer followed the Multiple Choice exam, the student group achieved a mean score of $78.64$ percent on Multiple Choice and $59.84$ percent on the Constructed Response. These results show it is a significantly more difficult task to have a working knowledge of the correct solutions, than it is to recognize them on sight in a Multiple Choice environment. When the short answer problems followed the Multiple Choice problems, students were able to use some of their recognition of correct answers to solve more short answer problems than the pretest group, but were unable to recover their Multiple Choice test scores. The questions in this test were aimed at the remember and understand levels of Bloom's taxonomy, indicating that even for the most basic forms of learning, Multiple Choice questions due a poor job of measuring student knowledge retention.
\subsection{Reference Material}
\paragraph{}
Another fundamental decision an instructor needs to make when designing an exam is whether or not they will allow students to make use of reference material during the exam. Perhaps the most common decision taken is to not allow reference material of any sort during an exam, the so-called Closed Book exam. On the other end of the reference material permissiveness spectrum is the Open Book exam. In an Open Book exam, students are permitted to use the primary reference material for the course, usually the recommended course textbook but alternatively all course notes may be permitted. For the purpose of our discussion an Open Book exam will be defined by the unmitigated allowance of a reference text where students can expect to find the answer to any reasonable basic fact about the subject they may need for an exam. Another option exists in the middle between Closed and Open Book exams. This option is what we will call Cheatsheet exams. In a Cheatsheet exam students are allowed to prepare a restricted set of notes to bring to the exam. Usually these notes are allowed to have any content the student may want on them, but with space and formatting restrictions which allow the notes they carry in to have significantly more compact information than an Open Book exam. Comparing the efficacy of Open versus Closed Book exams in the literature has led to mixed results.
\paragraph{}
On the positive side for Open Book exams, it has been shown that they lead to significantly lower test anxiety and stress among students leading up to the exam~\cite{broyles2005open}. This reduction in tension is hypothesized to lead to better study habits, such as focusing on understanding over rote memorization. It was found by~\cite{broyles2005open} that there are slight gains in student performance on Open Book exams over Closed Book exams. They also corroborate the findings of~\cite{broyles2005open} that student test anxiety is reduced in an Open Book exam setting. 
\paragraph{}
A study of the empirical literature on Open Book exams conducted by~\cite{theophilides2000study} supports the assertions that Open Book exams lead to lower test anxiety, and less focus on memorization over understanding. But their study also shows that Open Book examinations do not lead to higher test scores, in particular where critical-thinking questions are concerned. The findings of~\cite{agarwal2008examining} are even more lukewarm. They find that the effect of testing on student retention is equal between Open Book and Closed Book exams. Separate studies by~\cite{ioannidou1997testing} and~\cite{boniface1985candidates} found that student reliance on their permitted notes during an Open Book exam correlated with lower test scores. The findings of Ioannidou further indicate that critical-thinking questions are not effected with any statistical significance by the variable of whether an exam is Open or Closed Book. \cite{moore2007open} found that Open Book exams diminish long term retention of material among Biology students, and promote shallow-learning study strategies. The results of~\cite{heijne2011directing} support these findings, indicating that Closed Book exams do a better job of promoting deep-learning strategies among students than Open Book Exams.
\paragraph{}


\section{Oral Exams and Collaboration}\label{sec:communication}
\subsection{Oral Exams}\label{subsec:oral}
\paragraph{Implementation}
\paragraph{Results}
\paragraph{Benefits}
\paragraph{Drawbacks}
\paragraph{Data}

\subsection{Collaborative Exams}\label{subsec:collab}
\paragraph{Group}
\paragraph{Two-Stage}


\section{Recommendations}\label{sec:recommendations}
content
\subsection{Designing Exams to Encourage Good Learning Habits}
\subsection{Assessing a Variety of Cognitive Processes}
\subsection{Encouraging Genuine Engagement}
\nocite{*}
\bibliographystyle{apacite}
\bibliography{references.bib}

\end{document}