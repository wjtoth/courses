\documentclass[12pt]{article}

\usepackage{hyperref}
\usepackage{apacite}
\usepackage{setspace}

\doublespace
\title{Working Title - CUT Research Paper}
\author{W. Justin Toth}
\begin{document}
\maketitle
\begin{abstract}
Research Question: ``How does assessment format impact student learning outcomes, and instructor measurement thereof''.

New Research Question: ``Can innovation assessment formats improve student learning outcomes, and an instructors capacity to measure their achievement.''

\textit{With a particular focus on mathematics, stem}
\end{abstract}

\section{Introduction}

\paragraph{}
A well-known truism in human social interaction is that how you say something often has as much, if not more, importance that the specific content of your speech. Yet when instructors are designing evalutions for their courses, it is all too common that the focus is squarely on what questions to ask to cover the course content, rather than how the problems should be presented to the students. In this survey we seek to synthesize the academic literature on the ``how'', rather than ``what'', of assessment methods and provide guidance for instructors on the impact that exam format can have on student learning outcomes and their ability to measure the attainment thereof.
\paragraph{}
In Section \ref{sec:written} we study the particulars of written finals. For written finals we will concentrate on two areas of focus in the literature. The first is on the use of multiple choice versus constructed response questions, and the second is on allowing students to use reference material such as the course textbook or handwritten notes during the exam. In Section \ref{sec:communication} we study the use of oral communication during exams. This breaks down in to two subsections. In Subsection \ref{subsec:oral} we survey the literature on implementing interview-style oral exams  between student and instructor in the classroom, and in Subsection \ref{subsec:collab} we discuss student-to-student collaboration in exams with a particular focus on the innovative two-stage exam format. We conclude by with Section \ref{sec:recommendations} where present recommendations for instructors on choosing exam formats for their courses based on the previously surveyed literature.
\section{Individual Written Finals}\label{sec:written}
\subsection{Multiple Choice}
\paragraph{}
When preparing a written final exam, a very basic choice appears early on in the design of the test. That is the choice between having students select their response to a question or generate their response. This is the decision between using a Multiple Choice question format or using a Constructed Response question format. In a Multiple Choice question, students are given the question prompt and a finite list, usually of length between two and five, of potential responses they are to select from to answer the question prompt. In a Constructed Response question, the students are given the question prompt, but instead of a list of responses they are simply given a blank space in which to write their response. Constructed Response questions can vary more significantly in format than Multiple Choice, encompassing for example fill-in-the-bank, short answer, and essay questions. Both formats have their potential advantages and disadvantages, which have been well-studied in the education research literature.
\paragraph{}
The literature has highlighted the pragmatic benefits of Multiple Choice questions~\cite{simkin2005multiple}. With machine-grading, Multiple Choice exams can be graded very quickly, shortening the delay between students writing the exam and receiving their feedback. Automated grading of Multiple Choice problems also reduces the grading burden of instructors and teaching assistants, freeing up their time to (hopefully) concentrate on improving the quality of the course in other ways. Multiple Choice exams also have the advantage of providing an objective scoring system during grading, removing all grader subjectivity from the process. This can increase the perceived fairness of the exam. From the perspective of students, Multiple Choice exams have the advantage of inducing less anxiety. Students perceive Multiple Choice exams as being easier and quicker to prepare for, and are comforted by having potential answers available to them and the option they have to guess at a solution~\cite{zeidner1987essay}.
\paragraph{}
Despite the advantages above, Multiple Choice questions have many disadvantages over Constructed Response questions. It is difficult to assess critical thinking skills with Multiple Choice questions~\cite{martinez1999cognition}, and it is essentially impossible to assess creativity with Multiple Choice questions. Students seem to be aware of this on some level, expressing a perception that Multiple Choice exams assess a more shallow, knowledge-level understanding of the course material~\cite{scouller1998influence}. In line with this viewpoint, students predominantly favor shallow-learning strategies like memorization for exams consisting primarily of Multiple Choice problems. In contrast, students perceive Construhcted Response exams as testing deeper understanding, and use deeper learning strategies to study for Constructed Response exams~\cite{scouller1998influence}
\paragraph{}
Stanger-Hall hypothesized that Multiple Choice exams hinder the development of critical-thinking skills in her introductory biology class students~\cite{stanger2012multiple}. Traditionally these classes are evaluated using Multiple Choice only exams. To test this hypothesis, she changed the exams to feature Multiple Choice problems alongside several Constructed Response problems of the short answer variety. She predicted that this adaptation would result a change in how students studied for the exams, and their performance on critical-thinking questions.
\paragraph{}
The author defined critical-thinking skills as those skills associated with the middle levels of Bloom's taxonomy: application, analysis, evaluation, and synthesis. The experiment was performed using two sections of introductory biology given by the same instructor in the same semester to large classes of about $200$ students. The sections were entirely indentical in course material (lectures, assignments, readings) with the exception of the final exam. One section received a Multiple Choice only final, and the other received a Multiple Choice plus short answer Constructed Response final. Through a series of four surveys, Stanger-Hall evaluated student approaches to studying and perception of the importance of critical-thinking skills in introductory biology. The Constructed Response group reported significantly higher cognitively active learning behaviours during weeks outside of the exam study period, and this was reflected in their significantly higher test scores, in particular on questions associated with critical-thinking levels of Bloom's taxonomy~\cite{stanger2012multiple}.
\paragraph{}
A major weakness of Multiple Choice exams is their tendency to over-estimate student learning achievement~\cite{funk2011multiple}. The authors conjectured that grades from Multiple Choice exams were artificially inflated due to the relative ease of recognizing a correct answer versus generating it oneself, and students employing educated guessing strategies on exams. They conducted a study wherein $50$ students in a personality psychology course were randomly assigned to take a $10$-time short answer Constructed Response pretest or protest on a $50$ question Multiple Choice exam. To control for content and grading subjectivity, they used the same question prompts on the short answer and Multiple Choice problems.
\paragraph{}
The results were very strong. The mean score on short answer pretests was $34.24$ percent, followed by $77.1$ percent on the Multiple Choice exam. For the group where short answer followed the Multiple Choice exam, the student group achieved a mean score of $78.64$ percent on Multiple Choice and $59.84$ percent on the Constructed Response. These results show it is a significantly more difficult task to have a working knowledge of the correct solutions, than it is to recognize them on sight in a Multiple Choice environment. When the short answer problems followed the Multiple Choice problems, students were able to use some of their recognition of correct answers to solve more short answer problems than the pretest group, but were unable to recover their Multiple Choice test scores. The questions in this test were aimed at the remember and understand levels of Bloom's taxonomy, indicating that even for the most basic forms of learning, Multiple Choice questions due a poor job of measuring student knowledge retention.
\subsection{Reference Material}

%\subsection{Frequency}
%can include takehome

\section{Oral Exams and Collaboration}\label{sec:communication}
\subsection{Oral Exams}\label{subsec:oral}
\paragraph{Implementation}
\paragraph{Results}
\paragraph{Benefits}
\paragraph{Drawbacks}
\paragraph{Data}

\subsection{Collaborative Exams}\label{subsec:collab}
\paragraph{Group}
\paragraph{Two-Stage}


\section{Recommendations}\label{sec:recommendations}
content
\subsection{Designing Exams to Encourage Good Learning Habits}
\subsection{Assessing a Variety of Cognitive Processes}
\subsection{Encouraging Genuine Engagement}
\nocite{*}
\bibliographystyle{apacite}
\bibliography{references.bib}

\end{document}