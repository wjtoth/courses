\documentclass[12pt]{article}

\usepackage{hyperref}
\usepackage{apacite}
\usepackage{setspace}
\usepackage[margin=1in]{geometry}
\doublespace
\title{Impact of Exam Format on Student Learning Outcomes}
\author{W. Justin Toth\\Combinatorics and Optimization\\20411831}
\begin{document}
\maketitle
\begin{abstract}
We study of question of how exam formats impact student study behaviours, learning outcomes, and perception of the courses they take. We particularly focus on Mathematics and Science courses, but the findings in this survey are general enough to be widely applicable. We study multiple choice versus constructed response, how much reference material to allow in the exam room, oral versus written exams, and the Two Stage method for injecting collaborative learning into exams.
\end{abstract}
\newpage
\tableofcontents
\newpage
\section{Introduction}

\paragraph{}
A well-known truism in human social interaction is that how you say something often has as much, if not more, importance that the specific content of your speech. Yet when instructors are designing exams for their courses, it is all too common that the focus is squarely on what questions to ask to cover the course content, rather than how the problems should be presented to the students. In this survey we seek to synthesize the academic literature on the ``how'', rather than ``what'', of assessment methods used for exams and provide guidance for instructors on the impact that exam format can have on student learning outcomes and their ability to measure the attainment thereof.
\paragraph{}
In Section \ref{sec:written} we study the particulars of written finals. We concentrate on exams in the context of STEM courses, but many of the findings are broadly applicable. For written finals we will concentrate on two areas of focus in the literature. The first is on the use of multiple choice versus constructed response questions, and the second is on allowing students to use reference material such as the course textbook or handwritten notes during the exam. In Section \ref{sec:communication} we study the use of oral communication during exams. This breaks down in to two subsections. In Subsection \ref{subsec:oral} we survey the literature on implementing interview-style oral exams  between student and instructor in the classroom, and in Subsection \ref{subsec:collab} we discuss student-to-student collaboration in exams with a particular focus on the innovative two-stage exam format. We conclude by with Section \ref{sec:recommendations} where present recommendations for instructors on choosing exam formats for their courses based on the previously surveyed literature.
\section{Individual Written Finals}\label{sec:written}
Two common questions when it comes to Written Exams are what sort of question format should be used, and should students be allowed to have access to the textbook during the exam? In this Section we draw from the education research literature to find answers to these questions.
\subsection{Multiple Choice}
\paragraph{}
When preparing a written final exam, a very basic choice appears early on in the design of the test. That is the choice between having students select their response to a question or generate their response. This is the decision between using a Multiple Choice question format or using a Constructed Response question format. In a Multiple Choice question, students are given the question prompt and a finite list, usually of length between two and five, of potential responses they are to select from to answer the question prompt. In a Constructed Response question, the students are given the question prompt, but instead of a list of responses they are simply given a blank space in which to write their response. Constructed Response questions can vary more significantly in format than Multiple Choice, encompassing for example fill-in-the-bank, short answer, and essay questions. Both formats have their potential advantages and disadvantages, which have been well-studied in the education research literature.
\paragraph{}
The literature has highlighted the pragmatic benefits of Multiple Choice questions~\cite{simkin2005multiple}. With machine-grading, Multiple Choice exams can be graded very quickly, shortening the delay between students writing the exam and receiving their feedback. Automated grading of Multiple Choice problems also reduces the grading burden of instructors and teaching assistants, freeing up their time to (hopefully) concentrate on improving the quality of the course in other ways. Multiple Choice exams also have the advantage of providing an objective scoring system during grading, removing all grader subjectivity from the process. This can increase the perceived fairness of the exam. From the perspective of students, Multiple Choice exams have the advantage of inducing less anxiety. Students perceive Multiple Choice exams as being easier and quicker to prepare for, and are comforted by having potential answers available to them and the option they have to guess at a solution~\cite{zeidner1987essay}.
\paragraph{}
Despite the advantages above, Multiple Choice questions have many disadvantages over Constructed Response questions. It is difficult to assess critical thinking skills with Multiple Choice questions~\cite{martinez1999cognition}, and it is essentially impossible to assess creativity with Multiple Choice questions. Students seem to be aware of this on some level, expressing a perception that Multiple Choice exams assess a more shallow, knowledge-level understanding of the course material~\cite{scouller1998influence}. In line with this viewpoint, students predominantly favor shallow-learning strategies like memorization for exams consisting primarily of Multiple Choice problems. In contrast, students perceive Constructed Response exams as testing deeper understanding, and use deeper learning strategies to study for Constructed Response exams~\cite{scouller1998influence}
\paragraph{}
Stanger-Hall hypothesized that Multiple Choice exams hinder the development of critical-thinking skills in her introductory biology class students~\cite{stanger2012multiple}. Traditionally these classes are evaluated using Multiple Choice only exams. To test this hypothesis, she changed the exams to feature Multiple Choice problems alongside several Constructed Response problems of the short answer variety. She predicted that this adaptation would result a change in how students studied for the exams, and their performance on critical-thinking questions.
\paragraph{}
The author defined critical-thinking skills as those skills associated with the middle and high levels of Bloom's taxonomy: application, analysis, evaluation, and synthesis. The experiment was performed using two sections of introductory biology given by the same instructor in the same semester to large classes of about $200$ students. The sections were entirely indentical in course material (lectures, assignments, readings) with the exception of the final exam. One section received a Multiple Choice only final, and the other received a Multiple Choice plus short answer Constructed Response final. Through a series of four surveys, Stanger-Hall evaluated student approaches to studying and perception of the importance of critical-thinking skills in introductory biology. The Constructed Response group reported significantly higher cognitively active learning behaviours during weeks outside of the exam study period, and this was reflected in their significantly higher test scores, in particular on questions associated with critical-thinking levels of Bloom's taxonomy~\cite{stanger2012multiple}.
\paragraph{}
A major weakness of Multiple Choice exams is their tendency to over-estimate student learning achievement~\cite{funk2011multiple}. The authors conjectured that grades from Multiple Choice exams were artificially inflated due to the relative ease of recognizing a correct answer versus generating it oneself, and students employing educated guessing strategies on exams. They conducted a study wherein $50$ students in a personality psychology course were randomly assigned to take a $10$-time short answer Constructed Response pretest or protest on a $50$ question Multiple Choice exam. To control for content and grading subjectivity, they used the same question prompts on the short answer and Multiple Choice problems.
\paragraph{}
The results were very strong. The mean score on short answer pretests was $34.24$ percent, followed by $77.1$ percent on the Multiple Choice exam. For the group where short answer followed the Multiple Choice exam, the student group achieved a mean score of $78.64$ percent on Multiple Choice and $59.84$ percent on the Constructed Response. These results show it is a significantly more difficult task to have a working knowledge of the correct solutions, than it is to recognize them on sight in a Multiple Choice environment. When the short answer problems followed the Multiple Choice problems, students were able to use some of their recognition of correct answers to solve more short answer problems than the pretest group, but were unable to recover their Multiple Choice test scores. The questions in this test were aimed at the remember and understand levels of Bloom's taxonomy, indicating that even for the most basic forms of learning, Multiple Choice questions due a poor job of measuring student knowledge retention. 

\subsection{Reference Material}
\paragraph{}
Another fundamental decision an instructor needs to make when designing an exam is whether or not they will allow students to make use of reference material during the exam. Perhaps the most common decision taken is to not allow reference material of any sort during an exam, the so-called Closed Book exam. On the other end of the reference material permissiveness spectrum is the Open Book exam. In an Open Book exam, students are permitted to use the primary reference material for the course, usually the recommended course textbook but alternatively all course notes may be permitted. For the purpose of our discussion an Open Book exam will be defined by the unmitigated allowance of a reference text where students can expect to find the answer to any reasonable basic fact about the subject they may need for an exam. Another option exists in the middle between Closed and Open Book exams. This option is what we will call Cheatsheet exams. In a Cheatsheet exam students are allowed to prepare a restricted set of notes to bring to the exam. Usually these notes are allowed to have any content the student may want on them, but with space and formatting restrictions which allow the notes they carry in to have significantly more compact information than an Open Book exam. Comparing the overall efficacy, in terms of encouraging postive study habits and long term knowledge retention, of Open versus Closed Book exams in the literature has led to mixed results.
\paragraph{}
On the positive side for Open Book exams, it has been shown that they lead to significantly lower test anxiety and stress among students leading up to the exam~\cite{broyles2005open}. This reduction in tension is hypothesized to lead to better study habits, such as focusing on understanding over rote memorization. It was found by~\cite{broyles2005open} that there are slight gains in student performance on Open Book exams over Closed Book exams. They also corroborate the findings of~\cite{broyles2005open} that student test anxiety is reduced in an Open Book exam setting. 
\paragraph{}
A study of the empirical literature on Open Book exams conducted by~\cite{theophilides2000study} supports the assertions that Open Book exams lead to lower test anxiety, and less focus on memorization over understanding. But their study also shows that Open Book examinations do not lead to higher test scores, in particular where critical-thinking questions are concerned. The findings of~\cite{agarwal2008examining} are even more lukewarm. They find that the effect of testing on student retention is equal between Open Book and Closed Book exams. Separate studies by~\cite{ioannidou1997testing} and~\cite{boniface1985candidates} found that student reliance on their permitted notes during an Open Book exam correlated with lower test scores. The findings of Ioannidou further indicate that critical-thinking questions are not effected with any statistical significance by the variable of whether an exam is Open or Closed Book. \cite{moore2007open} found that Open Book exams diminish long term retention of material among Biology students, and promote shallow-learning study strategies. The results of~\cite{heijne2011directing} support these findings, indicating that Closed Book exams do a better job of promoting deep-learning strategies among students than Open Book Exams.
\paragraph{}
The effects of Open, Closed, and Cheatsheet exams on retention were measured by~\cite{gharib2012cheat} in an Introductory Psychology class. Across sections, students were given either an Open Book, Closed Book, or Cheatsheet exam. There were $99$ students participating in the study, and they were divided evenly into the three exam formats. Two weeks after writing a midterm with their designated exam format, the students participating in the study were given a surprise quiz to measure retention of the material from the exam. They found that there were no significant differences between retention across exam formats, despite scores on Open Book and Cheatsheet exams being higher than Closed Book exams.
\paragraph{}
In an Introduction to Statistics course~\cite{block2012discussion} sought to compare the use of Closed Book, Open Book, and Cheatsheet exams. Over the course of four different semesters, Block varied the exam format and measured student performance and perceptions of the course. In Fall $2006$, the exam format was a traditional Closed Book exam. In Spring $2007$, it was an Open Book exam, and again in Fall $2007$. The difference being, in Fall $2007$, the instructor emphasized to the students that the Open Book exam would be more challenging than a corresponding Closed Book exam and they shoud prepare accordingly using by trying for a deeper understanding of the material. Finally, in Spring $2008$, the Cheatsheet format was tried. Students were allowed to put as much information as they could fit on a typically index card and bring that into the exam with them. Their results showed higher performance on the Closed Book and Cheatsheet exams, and lower performance on the Open Book exams. The students preferred Open Book exams, except when they were told that they would be more difficult exams. The Cheatsheet exam reigned supreme, having comparable student enjoyment to the first semester of Open Book exams, and strong student preparation for the exams in line with what was seen for Closed Book exams. The cheatsheet itself encourages good study habits among students, forcing them to summarize and prioritize content, deciding what is truly important to write down.

\section{Oral Exams and Collaboration}\label{sec:communication}
Getting students to speak up in the classroom is a challenge in its own right. In this Section we consider exam formats which involve students communicating verbally with the instructor and each other. We will find that these approaches break from the norm in North American institutions, but provide many benefits to the students and instructors who can overcome the new challenges they bring.
\subsection{Oral Exams}\label{subsec:oral}
In the discipline of Mathematics, like many post-secondary level subjects, Written Exams seem to be the predominant form of assessment at North American Universities~\cite{gold1999assessment}. This stands in contrast to European institutions where Oral Exams are more common.  We loosely define an Oral Exam as a dialogue between Student(s) and Instructor(s) used for evaluation purposes. In this sense Oral Exams can take on many forms. They could be in the style of a presentation followed by questioning, akin to a Doctoral thesis defence, or they could be more conversational, with the student and instructor having a back-and-forth discussion about a problem. The questions for an Oral Exam may be given to the students beforehand, or appear spontaneously at the time of the exam.
\paragraph{}
In~\cite{videnovic2017oral} the author interviews seven mathematics professors of different culture backgrounds about the their perspectives on Written versus Oral Exams. One common point of agreement found among the subjects were type of skills each exam format was better suited for. Mathematical skills can broadly be categorized into one of two types: Procedural or Conceptual. Procedural skills reflect technical competency with the instruments of Mathematics. An example of a question testing Procedural knowledge would be ``Compute the integral $\int\sec x dx.$". Conceptual skills reflect understanding of the ways in which individual units of mathematical fact interrelate and support each other. For example, ``What does a matrix being invertible tell you about its determinant, and consequently its eigenvalues?". In Videnovic's survey, it was widely agreed that Written Exams are more proficient in testing Procedural skills, and Oral Exams more proficient in testing Conceptual skills.
\paragraph{}
Ten years of experience giving Oral Exams, consisting of one-on-one discussions between professor and student, in Upper Year Mathematics courses at the United States Air Force Academy were reported on in~\cite{boedigheimer2015individual}. The author identifies three major benefits to using oral exams: depth of study, real-time personal adjustment, and two-way student-instructor feedback. Students in courses with Oral Exams reported a desire to not want to embarass themselves in a discussion with their instructor, and as a consequence they studied more thoroughly and emphasized conceptual understanding. Since the exams and evaluation of student performance therein are occuring simultaneously, the instructor can quickly acertain student ability and adjust the exam accordingly. A struggling student may receive hints from the instructor helping steering them back on course, or a high performing student may receive more challenging questions probing their understanding at a deeper level than an average exam. The dialogue making up the exam is an excellent two-way feedback mechanism, making immediately apparent to both student and instructor where conceptual holes are in the students understanding allowing both to make corrections right after the exam without having to wait for the grading process to complete for feedback to be received.
\paragraph{}
In a first course on Abstract Algebra~\cite{capaldi2014non} reports on her experience using ``Non-Traditional" teaching methods, one of which was Oral Exams. She identifies many of the same advantages as Boedigheimer, but includes an additional valuable insight. In her implementation, each student gave two oral exams, one in the first half of the semester, and one in the second half. Capaldi observed that many students were visibly anxious during the first exam, but by the second exam they had developed a dramatic growth in confidence in presenting mathematical proof. Many students in Capaldi's course reported that the Oral Exams made a huge difference in their understanding of their own mathematical abilities.
\paragraph{}
The anxiety Capaldi noted is a common problem. Boedigheimer also observed that same phenomenon. Another issue with Oral Exams is the instructor workload. In a class of $35$ to $40$ students, Oral Exams can easily span a couple of full days. Nevermind a first year course with $200$ plus students where the workload can become entirely infeasible without considerable instructional support. This difficulty can spill over into missed class time, with the instructor often needing to block off classes during the semester just for Oral Exams. 
\paragraph{}
Oral Exams provide an intersting opportunity for student collaboration in the exam environment. \cite{odafe2006oral} gave an Oral Exam to students in a first Algebra course where students were tasked with solving a math problem in small groups. Odafe noted unique advantages and disadvantages to the group collaboration element. The obvious advantage is that students gain practice working on solutions as part of a team. This skill would be critical if they hoped to pursue mathematics as a career in the future, and is one that is not trained nearly enough in most students mathematical education. Teams that worked well together would have each student in the group contributing actively, and they were able to build off each others ideas learning and solving more in the exam than any one student would normally be able to. The disadvantages come with groups that did not work well together. If one member of a group was unable or unwilling to contribute, the students in that group were severely hampered by their teammate. Groups with poor communication or teamwork reported high dissatisfaction with the Oral Exam component of the course in the course evaluation surveys. Most students did not have that experience though, and overall student evaluations of the Oral Exam format were highly positive.
\paragraph{}
Over $10$ years giving interview style Oral Exams in Mathematics, \cite{boedigheimer2015individual} collected data on the correlation between Oral Exam performance and performance on other, more traditional, forms of assessment occuring in the classroom. Interestingly, Oral Exam and Written Exam performance were only weakly correlated (as opposed to say the strong correlation between assignment and Written Exam performance). This is in line with ~\cite{ahmed1999assessing} who assessed correlation between Oral and Written Exam performance at Cambridge, and finding a correation coefficient of only $0.53$. This seems to suggest that Oral and Written Exams test different abilities. This supports the beliefs of the professors in Videnovic's survey that Written and Oral Exam test Procedural and Conceptual skills respectively. But more importantly it highlights that when we favour only one exam format, students whose aptitudes line up with the unused format may fall through the cracks.
\subsection{Two-Stage Exams}\label{subsec:collab}
\paragraph{}
In the previous Subsection (Subsection \ref{subsec:oral}) we briefly discussed student collaboration in the context of an Oral Exam, highlighting the unique challenges and opportunities associated with that exam format. We would now like to discuss an innovative exam format which merges peer-to-peer collaborative discussions with a traditional Written Exam. This is the so-called Two Stage Exam format.
\paragraph{}
In advance of a Two Stage Exam, students agree on (or are assigned) a team of three to four students who they will collaborate with during the exam period. This already has the advantage of giving students a pre-made study group for exam preparation. The actual exam period is divided into two stages. During the first stage (approximately $2/3$ to $3/4$ of the exam period), students individually write a traditional Written Exam. At the end of the first stage, their exam papers are collected as normal. For the second stage (the remaining time of the exam period), students join up in their pre-arranged groups and each group is given a single copy of the same test they wrote during the first stage. The students are then tasked with coming to a consensus submission for the second stage exam paper. Notice that since the problems are repeats of the first stage problems, they take considerably less time to write their solutions the second time around.
\paragraph{}
At UBC \cite{rieger2014examinations} tested this Two Stage Exam format in their introductory physics class of $178$ students. The high pressure environment of the first stage of the exam primes students and heavily invests them in finding the answers to the problems. This carries over to the second stage, where students are well prepared to discuss their solution with the group. These discussions yield immediate feedback on student work from their peers. Struggling students receive clarification on problems they did not understand, and stronger students cement their understanding by explaining solutions to their peers. The main reason that Two Stage Exams are great way to get these benefits over regular in class collaborative learning is that they have a much higher engagement rate, nearly $100$ percent according to Rieger and Heiner. They surveyed student opinions of their implementation of Two Stage Exams and found that $76$ percent of students had a generally positive opinion of the format. In particular, the majority of students expressed surprise at how effective the exam method was for them learning from their mistakes.
\paragraph{}
Students did express some concerns with the Two Stage exam format of~\cite{rieger2014examinations}. Some found it emotionally distressing to immediately be made aware of their exam mistakes. Others expressed concern about weak students unfairly gaining marks from their strong group-mates during the second stage. Finally some students did not appreciate weak or unprepared teammates being unhelpful during the second stage of the exam.


\section{Recommendations}\label{sec:recommendations}
\paragraph{}
In this paper we discussed some basic decisions instructors need to make about their exam formats and how they can impact the learning outcomes of students in their courses. We have shown that exam format is tremendously impactful on students' experience of the course.
\paragraph{}
We saw that while Multiple Choice exams are imminently pragmatic, they encourage shallow learning of course material, and are detrimental to long term retention when compared to Constructed Response exam problems. We recommend that instructors relegate Multiple Choice to assessing the lowest levels of Bloom's taxonomy, and even then to only use them in very large classes where the practical benefits outweigh the potential drawbacks. If an instructor is in the situation where Multiple Choice exams are being used, and they still desire to test higher-order learning objectives there are some best-practices avaiable in the literature. See for example in the context of teaching Biology~\cite{crowe2008biology}, or for Economics~\cite{buckles2006using}.
\paragraph{}
We also considered the question of allowing students to have access to Reference Material during an exam. We found that Open Book exams do not promote appreciably higher retention of material over Closed Book exams, and can even lead to poor studying practices, like under-preparing for the exam or not employing deep-learning study strategies. The effect of Open Book exams on anxiety seems to be generally positive, reducing test anxiety among students compared to Closed Book exams. But that effect can be lost if you tell students their Open Book exam will be harder than the analogous Closed Book exam. If test anxiety is a major concern for you, then Open Book exams can help to solve that problem. The exam format which seems to capture the best of both worlds is the Cheatsheet exam format. It encourages students to employ similar deep-learning strategies to a Closed Book exam. In fact, it naturally pushes students to prioritize material and assess what is important since they have limited space to record their notes. It also has comparable test anxiety reducing properties to Open Book exams. We highly recommend instructors consider this format for their Written exams. When implementing Cheatsheet exams, ensure to have a standard format for the cheatsheet. Consider limiting both note size and font size, as some students will attempt to squish the entire course on the note somewhat defeating the purpose.
\paragraph{}
In Section \ref{sec:communication} we studied the potential for Oral Exam formats and Collaboration. We found that Oral Exams provide many impressive benefits, in particular for Mathematics classes. They increase student confidence, allow for immediate corrective feedback and clarification of student understanding, and even allow the instructor to tailor the exam to each individual student based on their level of capability. Perhaps most importantly of all though, there appears to be evidence that they test a different set of aptitudes than Written Exams. Both the skills which are amenable to Oral Exams, and those amenable to Written Exams are desirable skills which should be rewarded and encouraged in the evaluation process.
\paragraph{}
We strongly recommend that, when practical, instructors incorporate an Oral Exam component into their course. This ``when practical" caveat exists solely because it can be very time-intensive to implement Oral Exams, espcially if your class is large relative to your Teaching Assistant support. The only other reason to avoid Oral Exams may be that your course content is almost entirely mechanical in nature. Courses with little emphasis on conceptual understanding will not see much benefit from Oral Exams. But if those caveats do not exist for your course, or you are not dissuaded by them then Oral Exams can greatly benefit your students. There are a couple tips gathered from the articles we surveyed on Oral Exams which may be of help to instructors. If you have multiple graders for Oral Exams, have some ``practice'' Oral Exams all your Teaching Assistants grade at the start of the term, and discuss the grades as a group to make sure you are all callibrated and equally critical in your assessments. Do not schedule all the Oral Exams on the same day. Spread them out as appointments over a few days, or a week, otherwise the exercise will be mentally fatiguing for the instructor and the quality of exams occuring later in the day will suffer. Some students may freeze out of nervousness, especially if it's their first time giving an Oral Exam. If you know this is a new experience for your class, have some leniency and consider allowing students who break down in this way to reschedule another attempt. Finally, have a rubric which you are grading by present and write on it during the Oral Exam. Do not try to remember and fill it out after the exam. 
\paragraph{}
Lastly, in Subsection \ref{subsec:collab} we presented an innovative approach to injecting collaborative learning into Written Exams: the Two Stage Exam format. For instructors who have collaborative learning as a high priority goal, this is a relatively easy to implement change to your exam format which can help achieve that goal. It has been found that Two Stage Exams encourage dramatically higher student engagement in the collaborative activity that in-class collaborations. These exams go a long way to to encouraging students to seek feedback on their solutions, identifying missing elements in their understanding, and learn how to correct their mistakes. As with any group work task, poor teamwork skills can lead to drawbacks. To mitigate these, consider a policy where student's group stage mark will not be lower than their individual first stage grade. Similarly, try to make the group work portion just valuable enough that students have an incentive to give an earnest effort, but no higher than that. 
\paragraph{}
Whether we like it or not, grades are central to the University student experience and are a major motivator for student behaviour. Thus it is not surprising that the way we design our assessments has an impact on how students study, the skills they develop, and ultimately the learning outcomes they achieve in our courses. Hence we should be cognizant of the link between exam format and learning outcomes, and adjust accordingly. In this way we can use student grade motivation to our advantage, designing our assessment formats to funnel students into the positive learning behaviours we all hope they will adopt.
%\subsection{Designing Exams to Encourage Good Learning Habits}
%\subsection{Assessing a Variety of Cognitive Processes}
%\subsection{Encouraging Genuine Engagement}
\newpage
\bibliographystyle{apacite}
\bibliography{references.bib}

\end{document}