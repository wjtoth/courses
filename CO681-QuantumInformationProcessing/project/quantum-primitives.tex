\section{Quantum Primitives}\label{sec:quantum-primitives}
\paragraph{}
In this section we present important quantum primitives to be used in our optimization algorithms in later sections. The centerpiece of these algorithms is Grover's search, first presented in \cite{grover1996fast} and rigorously analyzed in \cite{boyer1996tight}, whose presentation we will follow.
\subsection{Grover's Search}
\paragraph{}
Let $T$ be  a table with with $N$ elements, where the $i$-th element is denoted $T[i]$. In a general {\it search problem} we are given $T$ and some $x$ in the universe from which the elements of $T$ are drawn, and are to find some $i \in \{0, \dots, N-1\}$ such that $T[i] = x$. Let $t= |\{ i \in \{0,\dots, N-1\} : T[i] = x\}$. Classically, it is easy to see that when $T$ is unsorted any algorithm which solves this problem with success probability at least $1/2$ will need to make at least $N/2t$ queries to $T$. In contrast, Grover's Quantum Search algorithm will need only $O(\sqrt{N/t})$ queries to have $1/2$ success probability. This is within a constant factor of optimal for a quantum algorithm 
\paragraph{}
Let $A = \{ i : T[i] = x\}$ and let $B = \{ i : T[i] \neq x \}$. Let $k, \ell \in \R$ satisfying $tk^2 + (N-t)\ell^2 = 1$.  We define the following general quantum register state
$$\ket{\Phi(k,\ell)} = \sum_{i \in A} k \ket{i} + \sum{i \in B} \ell \ket{i}.$$
Grover's Search begins with the state $$\ket{\Phi_0} := \sum_i \frac{1}{\sqrt{N}} \ket{i}$$
and each iteration applies a transformation sending some $\ket{\Phi(k,\ell)}$ to
$$\ket{\Phi(\frac{N-2t}{N}k + \frac{2(N-t)}{N}\ell, \frac{N-2t}{N}\ell - \frac{2t}{N} k)}.$$
The transformation is specified as follows (we assume $N$ is a power of $2$ for simplicity, full details appear in the source material). Define for any $K \subseteq \{0, \dots, N-1\}$ the ``conditional phase shift transform" satisfying
$$S_K\ket{i} = \begin{cases}
-\ket{i}, &\text{if $i \in K$} \\
\ket{i}, &\text{otherwise}.
\end{cases}$$
Let $H$ be the Hadamard transform satisfying
$$H\ket{j} = \frac{1}{\sqrt{N}} \sum_i (-1)^{i\cdot j} \ket{i}.$$
Then we observe that the transformation
$$-HS_{\{0\}}HS_A$$
efficiently implements a Grover Search iteration using only $1$ query to $T$.
\paragraph{}
The search begins by fixing some $\lambda \in (1,4/3)$ and initializing $m=1$. Then repeating the following procedure:
\begin{enumerate}
\item Choose $j$ uniformly at random from $\{1, \dots, m\}$.
\item Apply $j$ Grover iterations, as described above, starting from $\ket{\Phi_0}$.
\item Measure and let $i$ be the outcome.
\item If $T[i] = x$ return $i$; otherwise set $m$ to $\min(\lambda m, \sqrt{N})$.
\end{enumerate}
\begin{theorem}\label{th:grovers}
Grover's Search Algorithm above finds a solution in expected time $O(\sqrt{N/t})$. (Alternatively, the algorithm has small failure probability if terminated after $\Omega(\sqrt{N/t})$ iterations)
\end{theorem}
\begin{proof}
The case where $t=0$ can be treated with an appropriate timeout, with arbitrarily small failure probability. The case $t > \frac{3}{4}N$ can be dismissed via classical sampling techniques in constant expected time. So we may assume $1\leq t\leq \frac{3}{4}N$.
\paragraph{}
Let choose $\theta$ so that $\sin^2\theta = \frac{t}{N}$. Since $t\leq \frac{3}{4}N$, if we let $m_0 = \frac{1}{sin(2\theta)}$ then
$$m_0 = \frac{N}{2\sqrt{(N-t)t}} < \sqrt{\frac{N}{t}}.$$
Let $c = \ceil{\log_\lambda m_0}$. Observe that the algorithm sets $m =\lambda^{s-1}$ in iteration $s \leq \frac{3}{4}N$ of the main loop, and since $j$ is chosen randomly, the algorithm uses at most $\frac{1}{2} \lambda^{s-1}$ Grover iterations in expectation at iteration $s$. 
\paragraph{}
To reach iteration $c$ the algorithm would need
$$\frac{1}{2}\sum_{s=1}^{\ceil{\log_\lambda m_0}} \lambda^{s-1} <\frac{1}{2}\frac{\lambda}{\lambda - 1} m_0 = 3m_0$$
Grover iterations in expectation. Since $m_0 = O(\sqrt{\frac{N}{t}})$ if the algorithm terminates before iteration $c$ then we have the claimed running time bound. Notice that clearly the number of Grover iterations dominates the complexity of the running time.
\paragraph{}
Thus we may assume that algorithm uses at least $c$ iterations. The probability of success for $j$ Grover iterations from $\ket{\Phi_0}$ is $tk_j^2= \sin^2((2j+1)\theta)$. Since $j$ is chosen randomly the average success probability of step $2$ is
$$\sum_{j=0}^{m-1}\frac{1}{m}\sin^2((2j+1)\theta) = \frac{1}{2m}\sum_{j=0}^{m-1}1 - \cos((2j+1)\theta) = \frac{1}{2} - \frac{\sin(4m\theta)}{4m\sin(2\theta)}$$
recalling (learning?) that the last equality follows from a trigonometric identity. Since $m \geq \frac{1}{\sin(2\theta)}$ after the algorithm passes $c$ iterations, the above probability is at least $\frac{1}{4}$. Therefore the expected number of Grover iterations after the critical stage is
$$ \frac{1}{2}\sum_{s=0}^\infty \frac{3^s}{4^{s+1}}\lambda^{s+c} < \frac{\lambda}{8-6\lambda}m_0 = \frac{3}{2} m_0. $$
So the algorithm uses at most $3m_0 + \frac{3}{2}m_0 = O(\sqrt{\frac{N}{t}})$ Grover iterations in expectation.
\end{proof}
\begin{note}\label{note:log}
Many of our algorithms in the sections to follow. Particularly in Section \ref{sec:nf} and Section \ref{sec:matching} will use a polynomial number of applications of Grover's search, say $n^d$. If we want all of our quantum subroutines to simultaneously output a correct answer with probability at least $1-\frac{1}{n}$, then we need the probability of failure for a given subroutine to be at most $\frac{1}{n^{d+1}}$. Repeating each subroutine a $\log n$ number of times will ensure this probability. Consequentially, $\log $ factors will appear in our runtimes. We will omit them from the analysis, and cite this note as explanation of their appearance.
\end{note}
\subsection{Minima Finding}
\paragraph{}
Consider the following {\it minima finding } problem, wherein we try to find $d$ smallest values of different types. We are give two functions: an objective $f: \{1, \dots, N\} \rightarrow \N \cup \{\infty\}$, and a type $g: \{1, \dots, N\} \rightarrow \N$. Let $|g| := | \{ g(i): i \in \{1,\dots, N\}\}|$. We are also given an integer $d \in \{, \dots, \min\{|g|, N\}\}$, and are to find $I \subseteq \{1, \dots, N\}$ such that:
\begin{itemize}
\item $|I| = d$,
\item $g(i) \neq g(j)$ for all $i,j \in I$ with $i\neq j$,
\item and for all $j \in \{1,\dots, N\}\backslash I$ and $i \in I$, if $f(j) < f(i)$ then there exists some $i' \in I$ such that $g(i') = g(j)$ and $f(i') \leq f(j)$.
\end{itemize}
\paragraph{}
We define a notion of the sort of indices we want to add to $I$, split into two subtypes. For any $j \in \{1, \dots, N\}$, we call $j$ {\it good for} $I$ if there exists some $i \in I$ such that $f(j) < f(i)$ and either
\begin{itemize}
\item {\it(Known Type)} $g(j) = g(i)$, or 
\item {\it(Unknown Type)} $g(j) \not\in g(I)$.
\end{itemize}
We present the following algorithm for minima finding:
\begin{enumerate}
\item Initialize $I$ as a set of $d$ dummy indices of unique type with respect to $g$ and unique maximal value with respect to $f$
\item Repeat until no good elements for $I$
	\begin{enumerate}
	\item Use Grover's Search to find a good element $j$ for $I$.
	\item If $j$ is of Known type, replace $i \in I$ such that $f(j) < f(i)$ and $g(j) = g(i)$ with $j$
	\item Otherwise (i.e. if $j$ is of Unknown type) let $i \in I$ be element of largest $f$-value. Replace $i$ with $j$.
	\end{enumerate}
\end{enumerate}
\begin{lemma}\label{lemma:minima}Let $I \subseteq \{1,\dots, N\}$ such that $|I| = d$ and for all $i,j \in I$, $g(i) \neq g(j)$ with $t>0$ good elements for $I$. Then after $O(d)$ iterations of step $2$, in expectation, there are at most $\frac{3}{4}t$ good elements for $I$.
\end{lemma}
\begin{proof}
See Lemma $4$ of D{\"u}rr et al. \cite{durr2004quantum}.
\end{proof}
\begin{theorem}\label{th:minima}
The algorithm above solves the minima finding problem in expected time $O(\sqrt{dN})$.
\end{theorem}
\begin{proof}
Let $t$ denote the number of good elements remaining for $I$ at a given iteration of the algorithm. From Lemma \ref{lemma:minima} and Theorem \ref{th:grovers}, we have that, if $t > 2d$ then there are at most $\frac{t}{2}$ good elements for $I$ after $O(d\sqrt{\frac{N}{t}})$ queries of $f$ in expectation. Hence for $t>2d$ the expected number of queries of $f$ until $t\leq 2d$ is
$$O\big(d\big(\sqrt{\frac{N}{d}} + \sqrt{\frac{N}{2d}} + \sqrt{\frac{N}{4d}} + \dots\big)\big) = O(\sqrt{dN}).$$
Then once the algorithm has $t \leq 2d$, the number of queries of $f$ needed in expectation to reach $t=0$ is
$$O\big(\sum_{j=1}^{2d}\sqrt{\frac{N}{j}}\big) = O(\sqrt{dN})$$
as desired.
\end{proof}
\paragraph{}
Two special case of minima finding are the problems of Finding $d$ Smallest Values of a Function, and Find $d$ Elements of Different Type. By a reduction from distinguishing matrices to these special subproblems, they show in \cite{durr2004quantum} that Minima Finding requires $\Omega(\sqrt{dN})$ queries of $f$ and hence the algorithm we present is optimal. 