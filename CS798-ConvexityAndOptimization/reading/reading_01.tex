\documentclass[letterpaper,12pt,oneside,onecolumn]{article}
\usepackage[margin=1in, bottom=1in, top=1in]{geometry} %1 inch margins
\usepackage{amsmath, amssymb, amstext}
\usepackage{fancyhdr}
\usepackage{mathtools}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{theorem}
\usepackage{tikz}
\usepackage{tkz-berge}

%Macros
\newcommand{\A}{\mathbb{A}} \newcommand{\C}{\mathbb{C}}
\newcommand{\D}{\mathbb{D}} \newcommand{\F}{\mathbb{F}}
\newcommand{\N}{\mathbb{N}} \newcommand{\R}{\mathbb{R}}
\newcommand{\T}{\mathbb{T}} \newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
 
 
\newcommand{\cA}{\mathcal{A}} \newcommand{\cB}{\mathcal{B}}
\newcommand{\cC}{\mathcal{C}} \newcommand{\cD}{\mathcal{D}}
\newcommand{\cE}{\mathcal{E}} \newcommand{\cF}{\mathcal{F}}
\newcommand{\cG}{\mathcal{G}} \newcommand{\cH}{\mathcal{H}}
\newcommand{\cI}{\mathcal{I}} \newcommand{\cJ}{\mathcal{J}}
\newcommand{\cK}{\mathcal{K}} \newcommand{\cL}{\mathcal{L}}
\newcommand{\cM}{\mathcal{M}} \newcommand{\cN}{\mathcal{N}}
\newcommand{\cO}{\mathcal{O}} \newcommand{\cP}{\mathcal{P}}
\newcommand{\cQ}{\mathcal{Q}} \newcommand{\cR}{\mathcal{R}}
\newcommand{\cS}{\mathcal{S}} \newcommand{\cT}{\mathcal{T}}
\newcommand{\cU}{\mathcal{U}} \newcommand{\cV}{\mathcal{V}}
\newcommand{\cW}{\mathcal{W}} \newcommand{\cX}{\mathcal{X}}
\newcommand{\cY}{\mathcal{Y}} \newcommand{\cZ}{\mathcal{Z}}

\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}


\newenvironment{proof}{{\bf Proof:  }}{\hfill\rule{2mm}{2mm}}
\newenvironment{proofof}[1]{{\bf Proof of #1:  }}{\hfill\rule{2mm}{2mm}}
\newenvironment{proofofnobox}[1]{{\bf#1:  }}{}\newenvironment{example}{{\bf Example:  }}{\hfill\rule{2mm}{2mm}}

%\renewcommand{\thesection}{\lecnum.\arabic{section}}
%\renewcommand{\theequation}{\thesection.\arabic{equation}}
%\renewcommand{\thefigure}{\thesection.\arabic{figure}}

\newtheorem{fact}{Fact}[section]
\newtheorem{lemma}[fact]{Lemma}
\newtheorem{theorem}[fact]{Theorem}
\newtheorem{definition}[fact]{Definition}
\newtheorem{corollary}[fact]{Corollary}
\newtheorem{proposition}[fact]{Proposition}
\newtheorem{claim}[fact]{Claim}
\newtheorem{exercise}[fact]{Exercise}
\newtheorem{note}[fact]{Note}
\newtheorem{conjecture}[fact]{Conjecture}

\newcommand{\size}[1]{\ensuremath{\left|#1\right|}}
\newcommand{\ceil}[1]{\ensuremath{\left\lceil#1\right\rceil}}
\newcommand{\floor}[1]{\ensuremath{\left\lfloor#1\right\rfloor}}

%END MACROS
%Page style
\pagestyle{fancy}

\listfiles

\raggedbottom

\lhead{2017-11-01}
\rhead{William Justin Toth CS798-Convexity and Optimization Reading 1} %CHANGE n to ASSIGNMENT NUMBER ijk TO COURSE CODE
\renewcommand{\headrulewidth}{1pt} %heading underlined
%\renewcommand{\baselinestretch}{1.2} % 1.2 line spacing for legibility (optional)

\begin{document}
\section{Mathematical Background}
This section covers Appendix A  of Convex Optimization by Boyd and Vandenberghe. It is not a complete transcription but notes on particular things that I didn't already feel I understood strongly.
\subsection{Norms}
\paragraph{}
The {\it Cauchy-Schwartz inequality} states that $x^Ty \leq ||x||_2||y||_2$ for any $x,y,\in \R^n$. Such an inequality is useful for proving that the Euclidean norms satisfies the triangle inequality:
$$ ||x + y||_2^2 = (x+y)^T(x+y) = ||x||_2^2 + ||y||_2^2 + 2x^Ty \leq ||x||_2^2 + ||y||_2^2 + 2||x||_2||y||_2 = (||x||_2 + ||y||_2)^2$$
where the inequality follows from Cauchy-Schwartz.
\paragraph{}
The {\it standard inner product} for matrices, $\langle X,Y\rangle$, is defined as
$$\langle X, Y \rangle = trace(X^TY).$$
\paragraph{}
Suppose that $||\cdot ||_a$ and $||\cdot  ||_b$ are norms on $\R^n$. A fundamental result in analysis shows that there exists positive constants $\alpha$ and $\beta$ such that, for all $x \in \R^n$,
$$\alpha ||x||_a \leq ||x||_b \leq \beta ||x||_a.$$
This result implies that norms are {\it equivalent}, in an analytical sense, since they (for instance) define the same set of open subsets, the same set of convergent sequences, etc. An important caveat: this result works on finite dimensional vector spaces only. We can strengthen this result using convex analysis: If $||\cdot ||$ is any norm on $\R^n$, then there exists a quadratic norm $|| \cdot ||_P$ for which
$$||x||_P \leq ||x|| \leq \sqrt{n} ||x||_P,$$
for all $x$.
\subsection{Derivatives}
\paragraph{}
Let $f: \R^n \rightarrow \R^m$ and $x \in \textbf{int dom } f$. The function $f$ is differentiable at $x$ if there exists a matrix $Df(x) \in \R^{m \times n}$ that satisfies
$$ \lim_{z \in \textbf{dom} f, z\neq x, z\rightarrow x} \frac{||f(z) - f(x) - Df(x)(z-x)||_2}{||z-x||_2} = 0.$$
In such a case we refer to $Df(x)$ as the {\it derivative} (aka {\it Jacobian}) of $f$ at $x$. There can be at most one such matrix. The function $f$ is {\it differentiable } if $\textbf{dom} f$ is open, and it is differentiable at every point in its domain. The affine function of $z$ given by
$$f(x) + Df(x)(z-x)$$
is called the {\it first-order approximation} of $f$ at (or near) $x$.  The derivative can be found by deriving the first-order approximation of the function $f$ at $x$ or from partial derivatives as:
$$Df(x)_{ij} = \frac{\partial f_i(x)}{\partial x_j}, \quad i=1,\dots,m,\ \ \ j=1,\dots,n.$$
\paragraph{}
When $f$ is real-valued, the derivative $Df(x)$ is a$1\times n$ matrix. Its transpose in $\R^n$ is called the {\it gradient} of the function:
$$\nabla f(x) = Df(x)^T.$$
Its components are the partial derivatives of $f$.
\paragraph{}
Suppose $f: \R^n \rightarrow \R^m$ is differentiable at $x \in \textbf{int dom} f$ and $g: \R^m \rightarrow \R^p$ is differentiable at $f(x) \in \textbf{int dom}g$. Let $h(z) = g(f(z))$ define the composition $h: \R^n \rightarrow \R^p$. Then $h$ is differentiable at $x$, with derivative
$$Dh(x) = Dg(f(x))(Df(x)).$$
\paragraph{}
Let $f: \R^n \rightarrow \R$ be a real-valued function. The second derivative or {\it Hessian matrix} of $f$ at $x$ denoted $\nabla^2 f(x) \in \R^{n \times n}$ is given by,
$$\nabla^2f(x)_{ij} = \frac{\partial^2f(x)}{\partial x_i \partial x_j},$$
provided $f$ is twice differentiable at $x$. The {\it second-order approximation} of $f$, at or near $x$, is the quadratic function of $z$ defined by
$$\hat{f}(z) = f(x) + \nabla f(x)^T(z-x) + \frac{1}{2}(z-x)^T\nabla^2 f(x)(z-x).$$
This can be interpreted as the derivative of the gradient mapping.
\section{Operations that Preserve Convexity}
\paragraph{}
The following is a list of operations which preserve convexity:
\begin{enumerate}
\item The intersection of convex sets is a convex set.
\item The image (and inverse image) of a convex set under an affine map is a convex set. For instance, scaling, translations, and projection.
\item The image of a convex set under a {\it linear fractional } map (to be defined below) is a convex set.
\end{enumerate}
\subsection{Linear Fractional and Perspective Functions}
\paragraph{}
The {\it perspective function} $P: \R^{n+1} \rightarrow \R^n$ with domain $\textbf{dom} P = \R^n \times \{x\in\R : x > 0 \}$, is defined as
$$P(z,t) = z/t.$$
The perspective functions scales vectors so the last component is one then drops the last component. If $C \subseteq \textbf{dom} P$ is convex then $P(C)$ is convex. We will show that $P$ maps line segments to line segments, and the result will follow. Let $x = (\tilde{x}, x_{n+1}), y = (\tilde{y}, y_{n+1}) \in \R^{n+1}$ with $x_{n+1} > 0$. Then for $0 \leq \theta \leq 1$,
$$P(\theta x + (1-\theta)y) = \frac{\theta \tilde{x} + (1-\theta)\tilde{y}}{\theta x_{n+1} + (1-\theta)y_{n+1}} = \mu P(x) + (1-\mu)P(y),$$
where
$$\mu = \frac{\theta x_{n+1}}{\theta x_{n+1} + (1-\theta)y_{n+1}} \in [0,1].$$
\paragraph{}
A {\it linear-fractional function} is formed by composing the perspective function with an affine function. Suppose $g: \R^n \rightarrow \R^{m+1}$ is affine, i.e.,
$$g(x) = \begin{bmatrix}
A \\ c^T
\end{bmatrix} x + \begin{bmatrix}
b \\ d \end{bmatrix},$$
the function $f : \R^n \rightarrow \R^m$ given by $f = P \circ g$, i.e.,
$$fg(x) = \frac{Ax + b}{c^Tx + d}$$
is called a {\it linear-fractional} (or projective) function. If $c = 0$ and $d>0$ then $f$ is an affine function. The fact that linear-fractional functions preserve convexity follows since affine functions and perspective functions preserve convexity.
\end{document}
