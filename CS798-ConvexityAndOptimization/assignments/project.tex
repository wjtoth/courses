\documentclass[letterpaper,12pt,oneside,onecolumn]{article}
\usepackage[margin=1in, bottom=1in, top=1in]{geometry} %1 inch margins
\usepackage{amsmath, amssymb, amstext}
\usepackage{fancyhdr}
\usepackage{mathtools}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{theorem}
\usepackage{tikz}
\usepackage{tkz-berge}

%Macros
\newcommand{\A}{\mathbb{A}} \newcommand{\C}{\mathbb{C}}
\newcommand{\D}{\mathbb{D}} \newcommand{\F}{\mathbb{F}}
\newcommand{\N}{\mathbb{N}} \newcommand{\R}{\mathbb{R}}
\newcommand{\T}{\mathbb{T}} \newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
 
 
\newcommand{\cA}{\mathcal{A}} \newcommand{\cB}{\mathcal{B}}
\newcommand{\cC}{\mathcal{C}} \newcommand{\cD}{\mathcal{D}}
\newcommand{\cE}{\mathcal{E}} \newcommand{\cF}{\mathcal{F}}
\newcommand{\cG}{\mathcal{G}} \newcommand{\cH}{\mathcal{H}}
\newcommand{\cI}{\mathcal{I}} \newcommand{\cJ}{\mathcal{J}}
\newcommand{\cK}{\mathcal{K}} \newcommand{\cL}{\mathcal{L}}
\newcommand{\cM}{\mathcal{M}} \newcommand{\cN}{\mathcal{N}}
\newcommand{\cO}{\mathcal{O}} \newcommand{\cP}{\mathcal{P}}
\newcommand{\cQ}{\mathcal{Q}} \newcommand{\cR}{\mathcal{R}}
\newcommand{\cS}{\mathcal{S}} \newcommand{\cT}{\mathcal{T}}
\newcommand{\cU}{\mathcal{U}} \newcommand{\cV}{\mathcal{V}}
\newcommand{\cW}{\mathcal{W}} \newcommand{\cX}{\mathcal{X}}
\newcommand{\cY}{\mathcal{Y}} \newcommand{\cZ}{\mathcal{Z}}

\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}


\newenvironment{proof}{{\bf Proof:  }}{\hfill\rule{2mm}{2mm}}
\newenvironment{proofof}[1]{{\bf Proof of #1:  }}{\hfill\rule{2mm}{2mm}}
\newenvironment{proofofnobox}[1]{{\bf#1:  }}{}\newenvironment{example}{{\bf Example:  }}{\hfill\rule{2mm}{2mm}}

%\renewcommand{\thesection}{\lecnum.\arabic{section}}
%\renewcommand{\theequation}{\thesection.\arabic{equation}}
%\renewcommand{\thefigure}{\thesection.\arabic{figure}}

\newtheorem{fact}{Fact}[section]
\newtheorem{lemma}[fact]{Lemma}
\newtheorem{theorem}[fact]{Theorem}
\newtheorem{definition}[fact]{Definition}
\newtheorem{corollary}[fact]{Corollary}
\newtheorem{proposition}[fact]{Proposition}
\newtheorem{claim}[fact]{Claim}
\newtheorem{exercise}[fact]{Exercise}
\newtheorem{note}[fact]{Note}
\newtheorem{conjecture}[fact]{Conjecture}

\newcommand{\size}[1]{\ensuremath{\left|#1\right|}}
\newcommand{\ceil}[1]{\ensuremath{\left\lceil#1\right\rceil}}
\newcommand{\floor}[1]{\ensuremath{\left\lfloor#1\right\rfloor}}

%END MACROS
%Page style
\pagestyle{fancy}

\listfiles

\raggedbottom

\lhead{\today}
\rhead{William Justin Toth CS798-Convexity and Optimization Project} %CHANGE n to ASSIGNMENT NUMBER ijk TO COURSE CODE
\renewcommand{\headrulewidth}{1pt} %heading underlined
%\renewcommand{\baselinestretch}{1.2} % 1.2 line spacing for legibility (optional)

\begin{document}
\paragraph{}
The multiplicative weights update method studied in Lecture $9$ is a powerful and ubiquitous tool in algorithm design \cite{arora2012multiplicative}. This method is the inspiration behind the critical step in the primal-dual paradigm for designing competitive online algorithms \cite{buchbinder2009design}. One major area of application for such algorithms is the design of auctions for selling ad space in search query results.
\paragraph{}
We will begin by introducing the framework in which competitive online algorithms are studied. General packing/covering problems with be defined, and a we will give an online algorithm using the primal-dual paradigm for such problems. We will observe how a multiplicative weights update procedure is used to update the primal variables in each iteration. 
\paragraph{}
Afterwards we will see how ideas from this general approach can be applied to applications in implementing ad auctions. We will start with the simple single-unit case, then move on to the more complex multi-slot case. In the multi-slot case strong duality of linear programs will be of great utility.
\section{Competitive Online Algorithms via Primal-Dual}
\subsection{Online Packing and Covering}
\paragraph{}
We will start by discussing packing/covering problems in the standard, offline setting then move from there to the online model. The $\textit{covering}$ problem is specified by a linear program $(P)$ of the form:
\begin{align*}
\min &\sum_{i=1}^n c_i x_i \\
\text{s.t.} \sum_{i \in S(j)} x_i &\geq 1 &\forall j \in [m]\\
x_i &\geq 0 &\forall  i \in [n].
\end{align*}
There is some notation here to unpack. First for any $k \in \N$ we have $[k] := \{1, \dots, k\}$. The vector $x \in \R^n$ is the variable set. The vector $c \in \R^n$ is the objective function, which is assumed to be non-negative. The sets $S(1), \dots, S(m)$ are subsets of $[n]$. We note here that this model is not the most general form of covering problem imaginable, but it is somewhat simpler while still being general enough to capture the main ideas we are striving for. 
\paragraph{}
We are discussing $\textit{fractional}$ covering here, but if we were to restrict our attention to $0-1$ integral solutions we would have nice a combinatorial interpretations for this problem. In this context, covering asks you to find a minimum cost subset of $[n]$ whose intersection with each $S(j)$ is non-empty.
\paragraph{}
The $\textit{packing}$ problem $(D)$ is defined as the dual to a corresponding covering problem:
\begin{align*}
\max &\sum_{j=1}^m y_j \\
\text{s.t.} \sum_{j : i \in S(j)} y_j &\leq c_i &\forall i \in [n] \\
y_j &\geq 0 &\forall j \in [m].
\end{align*}
\paragraph{}
In the $\textit{online covering problem}$ we are solving the covering problem, but we do not have all the information in advance. The cost function $c$ is known to us, as well as the size of the variable space. The entire space of constraints is not known. The constraints are given in some sequence unknown to the algorithm. At each point in the sequence the algorithm receives knowledge of one constraint, and must decide on a feasible solution to the covering problem with the constraints at hand. Any variables the algorithm decides to increase to maintain feasibility of the current sub-instance may not be decreased at future points in the input sequence.
\paragraph{}
One can observe that at any point in the operation of the algorithm in the online model, the constraints revealed so far form a sub-instance of the covering problem which is itself a covering problem. We define the online packing problem in such a way that its sub-instances are the duals to the covering problem sub-instances. That is, we reveal the variables one-at-a-time in the online packing problem, as opposed to the constraints as we did for online covering.
\paragraph{}
In the $\textit{online packing problem}$ we are solving the packing problem, but again we do not have all the information in advance. The values of $c$ are all known to us in advance, however the number of variables and the precise way they must be packed is not known. At each point in the sequence for which the input is given, a new variable can be introduced to the algorithm. If $y_j$ is the variable introduced, the algorithm is at this point made aware of precisely which $i \in [n]$ satisfy $i \in S(j)$. So the constraints are revealed over time. The algorithm must decide on the value of $y_j$ at the point it is introduced and may not change it in subsequent iterations. Under this definition, for any online covering problem, there is a corresponding online packing problem that, if run simultaneously, have their sub-instances at each point in the sequence the input is given acting as primal-dual pairs.
\subsection{General Algorithm}
\paragraph{}
Consider the following algorithm for simultaneously solving a corresponding pair of online packing/covering problems of the form $(P)$ and $(D)$ from the previous subsection. By scaling, we may assume that each $c_i \geq 1$. We denote the algorithm by $\cA$:
\begin{enumerate}
\item When a new constraint for $(P)$ of the form $\sum_{i \in S(j)} x_i \geq 1$ arrives with corresponding dual variable $y_j$ do:
\item While $\sum_{i \in S(j)} x_i < 1$:
	\begin{enumerate}
	\item For each $i \in S(j)$: $x_i \leftarrow x_i(1+\frac{1}{c_i}) + 1(|S(j)|\cdot c_i)$.
	\item $y_j \leftarrow y_j +1$.
	\end{enumerate}
\end{enumerate}
\paragraph{}
We now proceed to analyze this algorithm. The critical notion we will use here is that of a $\textit{competitive online algorithm}$. An online algorithm $\cA$ (or its solution) is said to be $c$-competitive if for every instance of the minimization problem $\cA$ solves, the cost of the solution produced by $\cA$ is at most $c\cdot OPT + \alpha$ where $OPT$ is the optimal value of solving the ``offline" version of the problem, and $\alpha$ is an additive factor independent of the sequence in which the input is presented to $\cA$. If $\cA$ instead solves a maximization problem, to be $c$-competitive the cost of the solution produced instead needs to be at least $\frac{1}{c} OPT -\alpha$.
\begin{theorem}\label{th:1}
Let $d = \max_{j \in [m]} |S_(j)|$. Then $\cA$ produces two things:
\begin{enumerate}
\item An $O(\log d)$-competitive fractional covering solution,
\item and a $2$-competitive integral packing solution which violates each packing constraint by at most $O(\log d)$.
\end{enumerate}
Note: We can rescale our packing solution by $O(\log d)$ to obtain a feasible fractional packing if desired.
\end{theorem}
\begin{proof}
The dual variables start at $0$ and increase by $1$ whenever they are changed. Thus the packing solution returned is integral. By an iteration of $\cA$ we mean a complete execution of step $2$. Let $x$ be the solution for $(P)$ returned by $\cA$. Let $y$ be the solution for $(D)$ returned by $\cA$. We want to show three things:
\begin{enumerate}
\item $x$ is feasible for $(P)$.
\item In each iteration of $\cA$ the increase in $c^Tx$ is at most twice the increase in $\sum_{j=1}^m y_j$.
\item for all $i \in [n]$, $\sum_{j : i \in S(j)} y_j \leq c_i \cdot O(\log d)$.
\end{enumerate}
\paragraph{}
Part $1.$ is quite clear. Whenever a primal constraint is introduced, the While loop of Step $2$ does not terminate until the constraint is satisfied. Subsequent iterations do not decrease the variables of $x$, so the constraint remains satisfied.
\paragraph{}
To demonstrate $2.$ we will study a single iteration of the While loop. In an iteration of Step $2$ the change in $\sum_{j=1}^m y_j$ is precisely $1$. The change in the primal objective value $c^Tx$ is (from the update in Step $2a$):
$$\sum_{i \in S(j)} c_i (\frac{x_i}{c_i} + \frac{1}{|S(j)| c_i}) = \sum_{i \in S(j)}(x_i + \frac{1}{|S(j)|}) \leq 2$$
since $\sum_{i \in S(j)} x_i \leq 1$ at the time Step $2a$. Therefore $2.$ holds.
\paragraph{}
Let $i \in [n]$. We consider the dual constraint corresponding to $i$. Whenever we increase a dual variable $y_j$ such that $i \in S(j)$ by $1$ in Step $2b$ we had just increased $x_i$ in Step $2a$. We claim that 
$$x_i \geq \frac{1}{d}((1+\frac{1}{c_i})^{\sum_{j : i \in S(j)} y_j} - 1).$$
We will proceed by induction on the iterations of $\cA$. In the base case $x_i = 0$, and $\sum_{j : i \in S(j)} y_j = 0$ so the claim holds. Now consider an iteration in which some dual variable $y_k$ with $k \in S(j)$ is increased. Let $x_i'$ denote the value of $x_i$ after this iteration (and $x_i$ denote the value before).
\begin{align*}
x'_i &= x_i(1 + \frac{1}{c_i}) + \frac{1}{|S(j)|c_i} \\
&\geq x_i(1+\frac{1}{c_i}) + \frac{1}{d\cdot c_i} \\
&\geq \frac{1}{d}((1+\frac{1}{c_i})^{\sum_{j : i \in S(j)\backslash\{k\}}y_j} - 1)(1 + \frac{1}{c_i})^y_k + \frac{1}{d\cdot c_i} &\text{by the inductive hypothesis on $x_i$} \\
&= \frac{1}{d}((1+\frac{1}{c_i})^{\sum_{j :i \in S(j)} y_j} -1).
\end{align*}
So the claim holds by induction. Now we compute an upper bound on $x'_i$. By the condition of the While loop, $x_i <1$. We also have $c_i \geq 1$ and $d\geq 1$, so
$$x'_i =  x_i(1 + \frac{1}{c_i}) + \frac{1}{|S(j)|c_i} \leq 1(1+1) + 1 = 3.$$
Combining inequalities we have
$$ \frac{1}{d}((1+\frac{1}{c_i})^{\sum_{j :i \in S(j)} y_j} -1) \leq 3.$$
Using once more that $c_i \geq 1$ and rearranging for $\sum_{j : in \in S(j)} y_j$ we obtain
$$\sum_{j : i \in S(j)} y_j \leq c_i \log(3d+1) = c_i \cdot O(\log d)$$
as desired.
\paragraph{}
It remains to use these claims to verify the competitive factors. This is done via duality of linear programs. Let $p^*$ be the optimal value of $(P)$. Let $f^*$ be the optimal value of $(D)$. By claim $2$ we have
$$c^Tx \leq 2 \sum_{j} y_j.$$
Since $x$ is primal feasible, $c^Tx \geq p^*$. So 
$$2\sum_j y_j \geq c^Tx \geq p^* = f^*.$$
Thus $y$ is $2$-competitive. By claim $3$, $y' \in \R^m$ where $y'_j = y_j/O(\log d)$ is feasible for $(D)$.
So similarly,
$$c^Tx \leq 2 O(\log d) \sum_j y_j \leq O(\log d) f^* = O(\log d) p^*.$$
Therefore $x$ is $O(\log d)$-competitive.
\end{proof}
\paragraph{Connection to Multiplicative Weights}
If we looks at Step $2a$ of $\cA$, we observe a multiplicative update rule very similar to those used in the study of the Multiplicative Weights Update Method of Lecture $9$:
$$x_i \leftarrow x_i(1+\frac{1}{c_i}) + 1(|S(j)|\cdot c_i).$$
So each $x_i$ involved in constraint $j$ is increased multiplicatively, while $y_j$ increases additively during the operation of the While loop in Step $2$. Notice that during the proof for the competitive factor we used an induction obtaining that $x_i$ is approximately an exponential function of the $y_j$'s:
$$x_i = \frac{1}{d}((1+\frac{1}{c_i})^{\sum_{j : i \in S(j)} y_j} -1),$$
similar to how we observe an exponential relationship between the potential function (in the proof for the performance guarantee in multiplicative weights update) and the variables of the multiplicative weights update procedure. In both scenarios these exponentials that arise from the respective multiplicative update rules allow us to derive logarithmic performance factors. The difference being that factor is additive in multiplicative update method, and is multiplicative in primal-dual.
\section{Ad Auctions}
\paragraph{}
The business of displaying ads in search engine results is a natural and important application for online algorithms. Search provides like Google and Microsoft will sell ad space to advertisers in the search results corresponding with some users given keyword query. The way they sell the space is through auctions to buyers interested in displaying ads related to the given query.

\subsection{Single Slot}
\paragraph{}
In the $\textit{ad-auctions problem}$ we are given a set $I$ of $n$ buyers, each with a daily budget of $B(i)$. We are also given a set $M$ of items with $|M| = m$. The items arrive in an online manner. Upon the arrival of some item, say the $j$-th item for $j \in [m]$, each buyer will submit a bid for said item, denoted $b(i,j)$. The algorithm can decide to allocate the item to any of the buyer upon its arrival. The online constraint is that they cannot reallocate items once they have decided who they are to be allocated to.
\paragraph{}
Allocations can be either $\textit{fractional}$ or $\textit{integral}$. In integral allocation, each item is allocated to a single buyer. In fractional allocations the items can be subdivided and allocated to multiple buyers. The algorithm we present will give an integral allocation, but the linear programming formulation we base it on will be fractional. Clearly, integral allocations suffice as fractional allocations, so we can naturally come our integral solutions to the optimal values of the fractional linear programs.
\paragraph{}
The objective is to maximize the total revenue of the seller. The revenue received from each buyer is the minimum of the sum total of bids corresponding to items allocated to that buyer and their budget. It is defined as such since we cannot overcharge buyers.
\paragraph{}
We will denote by $(D)$ the $\textit{fractional ad-allocation}$ linear program, defined as follows:
\begin{align*}
\max \sum_{j =1}^m \sum_{i=1}^n &b(i,j) y(i,j) \\
\text{s.t. } \sum_{i=1}^n y(i,j) &\leq 1 &\forall j \in [m] \\
\sum_{j=1}^m b(i,j)y(i,j) &\leq B(i) &\forall i \in [n] \\
y(i,j)&\geq 0 &\forall i \in [n], \forall j \in [m]. 
\end{align*}
The variables of $(D)$ are the values $y(i,j)$. The meaning of $y(i,j)$ is the fraction of item $j$ that is allocated to buyer $i$. The objective function maximizes our revenue, assuming that we do not overcharge any buyer. The second collection of constraints ensures just that: that we do not charge any buyer more than their budget $B(i)$. The first set of constraints ensures that each item is not over-allocated. That is, you can allocated at most one of item $j$. We observe that $(D)$ is a packing problem, similar to the simplified packing problem studied in the previous section.
\paragraph{}
Since $(D)$ is a packing problem, its dual is a covering linear program which we will denote $(P)$. The linear program $(P)$ is as follows:
\begin{align*}
\min \sum_{i=1}^nB(i)x(i) &+ \sum_{j=1}^m z(j) \\
\text{s.t. } b(i,j)x(i) + z(j) &\geq b(i,j) &\forall i \in [n], \forall j \in [m] \\
x(i) &\geq 0 &\forall i \in [n] \\
z(j) &\geq 0 &\forall j \in [m].
\end{align*}
We describe the following primal-dual algorithm, $\cA$, for ad-allocation (initially all variables are set to $0$):
\begin{enumerate}
\item When a new item $j$ is given, allocate $j$ to buyer $i$ which maximizes $b(i,j)(1-x(i))$.
\item If $x(i) < 1$ then do
	\begin{enumerate}
	\item Charge buyer $i$ the minimum of $b(i,j)$ and the remainder of $B(i)$ not yet charged. Set $y(i,j) = 1$
	\item Set $z(j) = b(i,j)(1-x(i))$.
	\item Set $x(i) = x(i)\frac{1+b(i,j)}{B(i)} + \frac{b(i,j)}{(c-1)B(i)}$.
	\end{enumerate} 
\end{enumerate}
The value of $c$ above is taken to be $(1+R_{max})^\frac{1}{R_{max}}$ where $R_{max} = \max_{i \in I, j\in M} \frac{b(i,j)}{B(i)}$.
\begin{theorem}
The algorithm $\cA$ is $(1-\frac{1}{c})(1-R_{max})$-competitive for the ad-allocation problem. Note that when $R_{max} \rightarrow 0$, $\cA$ tends to being $(1-\frac{1}{e})$-competitive.
\end{theorem}
\begin{proof}
As in the proof of theorem \ref{th:1} we prove three claims about feasibility and the relative change in objective value for $(P)$ and $(D)$ in each iteration:
\begin{enumerate}
\item The solution returned by $\cA$ for $(P)$ is feasible.
\item The increase in the solution for $(P)$ in each iteration is at most $(1+\frac{1}{c-1})$ times in the increase in the solution for $(D)$ during that iteration.
\item The solution returned for $(D)$ is ``nearly feasible". We will derive the precise bounds when we reach this step of the proof. 
\end{enumerate}
\paragraph{}
Consider a constraint of $(P$) for buyer $i$ and item $j$:
$$b(i,j)x(i) + z(j) \geq b(i,j).$$
Suppose $x$ with $z$ is the solution for $(P)$ returned by $\cA$. If $x(i) \geq 1$ then the constraint is satisfied immediately. Otherwise let $i'$ be the buyer $\cA$ allocates $j$ to during operation. The buyer $i'$ maximizes $b(i',j)(1-x(i'))$ by Step $1$. In Step $2b$ $z(j)$ is set to $b(i',j)(1-x(i')).$ It is never changed in later iterations. By our choice of $i'$, 
$$z(j) \geq b(i,j)(1-x(i))$$
and hence the constraint for $i$ and $j$ is satisfied by $x$ and $z$. Therefore we have shown claim $1$.
\paragraph{}
In each iteration of $\cA$, the increase in value for $(D)$ is $b(i,j)$ where $i$ is the buyer the item $j$ corresponding to the given iteration is allocated to. This follows since we always set $y(i,j)$ to $1$. By step $2b$ the increase in $z$ is 
$$b(i,j) (1-x(i)).$$
By step $2c$ the increase in $x$ is
$$(\frac{1+b(i,j)}{B(i)}-1)x(i) + \frac{b(i,j)}{(c-1)B(i)}.$$
So the increase in the objective value for $(P)$ is
\begin{align*}
&B(i)[(\frac{1+b(i,j)}{B(i)}-1)x(i) + \frac{b(i,j)}{(c-1)B(i)}] + b(i,j)(1-x(i)) \\&= x(i) +b(i,j)x(i) - B(i)x(i) +\frac{b(i,j)}{(c-1)} + b(i,j) -b(i,j)x(i)\\
&\leq b(i,j)(1+\frac{1}{c-1}).
\end{align*}
Therefore claim $2$ holds.
\paragraph{}
We want to show that there is at most one iteration per buyer $i$ that we charge them in a manner which exceeds their budget. In particular during the last iteration where we charge $i$, this can happen. To prove this we observe that $\cA$ never increases $y(i,j)$ when $j$ has been allocated to $i$ yet $x(i) \geq 1$. So we want to show that 
$$\sum_j b(i,j)y(i,j) \geq B(i)$$
implies that $x(i) \geq 1$. It suffices to show that 
\begin{equation}\label{eq:1}
x(i) \geq \frac{1}{c-1}(c^{\frac{\sum_j b(i,j)y(i,j)}{B(i)}} -1)
\end{equation}
and when $\sum_j b(i,j)y(i,j) \geq B(i)$ this implies $x(i) \geq 1$. We proceed by induction on the iterations in which an item is allocated to buyer $i$. Before any item has been allocated to buyer $i$, the right hand side of (\ref{eq:1}) is $0$ and so the claim holds. Now consider the inductive case: suppose we are in some iteration where item $k$ is allocated to buyer $i$. Let $x'$ be the value of $x$ after this iteration and $x$ will denote the prior value. Then
\begin{align*}
x'(i) &= x(i)(1+\frac{b(i,k)}{B(i)}) + \frac{b(i,k)}{(c-1)B(i)} \\
&\geq \frac{1}{c-1}(c^{\frac{\sum_{j\neq k} b(i,j)y(i,j)}{B(i)}} -1)(1+\frac{b(i,k)}{B(i)}) + \frac{b(i,k)}{(c-1)B(i)}  &\text{by inductive hypothesis}\\
&= \frac{1}{c-1}(c^{\frac{\sum_{j\neq k} b(i,j)y(i,j)}{B(i)}}(1+\frac{b(i,k)}{B(i)}) - 1) \\
&\geq \frac{1}{c-1}(c^{\frac{\sum_{j\neq k} b(i,j)y(i,j)}{B(i)}}c^{\frac{b(i,k)}{B(i)}} -1) \\
&=\frac{1}{c-1}(c^{\frac{\sum_{j} b(i,j)y(i,j)}{B(i)}} -1).
\end{align*}
When $\frac{b(i,k)}{B(i)} = R_{max}$, the second inequality holds with equality. From this we derived our choice of $c$.
\paragraph{}
Now we have shown that once the sum of charges for items allocated to any buyer exceeds their budget, we no longer charge the buyer. So their charge can exceed their budget by at most the cost of one bid. Hence for each buyer $i$:
$$\sum_j b(i,j)y(i,j) \leq B(i) + \max_j b(i,j).$$
This is precisely how much $(D)$ is violated by in the solution $y$ returned by $\cA$. So we have claim $3$.
\paragraph{}
Now the revenue we receive from buyer $i$ is at least
$$(\sum_j b(i,j)y(i,j)) \frac{B(i)}{B(i) + \max_j b(i,j)} \geq (\sum_j b(i,j)y(i,j)) (1-R_{max}).$$
From claim $2$ the value of $y$ for $(D)$ present above is at least $1-\frac{1}{c}$ times the value of $(P)$. Since $x$ and $z$ is feasible by claim $1$, the value of $(P)$ is an upper bound on the optimal value of $(D)$. Together this demonstrates the competitive ratio.
\end{proof}
\subsection{Multiple Slots}
\paragraph{}
Now we extend to a more general problem. As before we will have $n$ buyers and $m$ items, but now each item will have $\ell$ ``slots". Think of slots as different positions for an ad on a given page of search results corresponding to some keyword. Naturally, different positions may have different value to advertisers, so we allow different bids for different slots. Thus each buyer $i \in [n]$ submits a bid for each slot $k \in[\ell]$ of item $j \in [m]$: $b(i,j,k)$. We fractionally allocate slots just as we did before for items. Thus our general form for $(D)$ is
\begin{align*}
\max \sum_{j =1}^m \sum_{i=1}^n \sum_{k=1}^\ell&b(i,j,k) y(i,j,k) \\
\text{s.t. } \sum_{i=1}^n y(i,j,k) &\leq 1 &\forall j \in [m], \forall k \in [\ell]. \\
\sum_{j=1}^m \sum_{k=1}^\ell b(i,j,k)y(i,j,k) &\leq B(i) &\forall i \in [n] \\
y(i,j,k)&\geq 0 &\forall i \in [n], \forall j \in [m], \forall k \in [\ell]. 
\end{align*}
The dual to $(D)$, denoted $(P)$ is now
\begin{align*}
\min \sum_{i=1}^nB(i)x(i) &+ \sum_{j=1}^m\sum_{k=1}^\ell z(j,k) + \sum_{i=1}^n\sum_{j=1}^m s(i,j) \\
\text{s.t. } b(i,j,k)x(i) + z(j,k) + s(i,j) &\geq b(i,j,k) &\forall i \in [n], \forall j \in [m] , \forall k \in [\ell]\\
x(i) &\geq 0 &\forall i \in [n] \\
z(j,k) &\geq 0 &\forall j \in [m], \forall k \in [\ell] \\
s(i,j) &\geq 0 &\forall i \in [n], \forall j \in [m].
\end{align*}
The generalized algorithm, $\cA$, is as follows:
\begin{enumerate}
\item When a new item $j$ is given, do:
	\begin{enumerate}
	\item Let $H$ be a bipartite graph with vertex set $[n] \cup [\ell]$. Each edge of $H$ is of the form $ik$ with $i \in [n]$ and $k \in [\ell]$.  The weight of edge $ik$ is $b(i,j,k)(1-x(i))$.
	\item Find a maximum weight integral matching in $H$. Such a matching forms an assignment on the variables $y(i,j,k)$.
	\item Charge buyer $i$ the minimum of $\sum_{k=1}^\ell b(i,j,k)y(i,j,k)$ and the remainder of $B(i)$ not yet charged.
	\item For each $i \in [n]$, for each $k \in [\ell]$ with $y(i,j,k) > 0$ update  $$x(i) = x(i)(1+ \frac{b(i,j,k)y(i,j,k)}{B(i)}) + \frac{b(i,j,k)y(i,j,k)}{(c-1)B(i)}.$$
	\end{enumerate} 
\end{enumerate}
One can observe that this algorithm is indeed a proper generalization of the previous algorithm. In the case where $\ell = 1$ (ie. each item has one slot), the multiple slot algorithm computes a maximum weight matching in a very simple graph: where there is one vertex in the item partition, and all the buyers in the other. It amounts to picking the max weight bidder as we did in the previous algorithm. Thus the operation of the multiple slot algorithm reduces to the operation of the single slot algorithm in the case where $\ell = 1$.
\paragraph{}
We will need the linear programs for maximum weight matching, and its dual in our analysis of $\cA$. Fix some item $j$ and some previously set $x \in \R^n$. Then the maximum weight matching linear program for $H$, denoted $(M)$ is
\begin{align*}
\max \sum_{i=1}^n \sum_{k=1}^\ell b(i,j,k)(1-x(i))&y(i,j,k) \\
\text{s.t.} \sum_{i=1}^ny(i,j,k) &\leq 1 &\forall j \in [\ell] \\
\sum_{k=1}^\ell y(i,j,k) &\leq 1 &\forall i \in [n] \\
y(i,j,k) &\geq 0 &\forall i \in [n], \forall k \in [\ell].
\end{align*}
The dual of $(M)$ is a vertex cover problem $(C)$ of the form
\begin{align*}
\min \sum_{i=1}^n s(i,j) &+ \sum_{k=1]}^\ell z(j,k) \\
\text{s.t.} s(i,j) + z(j,k) &\geq b(i,j,k)(1-x(i)) &\forall i \in [n], \forall k \in [\ell] \\
z(j,k) &\geq 0 &\forall k \in [\ell] \\
s(i,j) &\geq 0 &\forall i \in [n].
\end{align*}
It is a well known theorem that $(M)$ always has integral optimal solutions, and thus by strong duality the integral matching found along with dual vertex cover in $H$  satisfies:
\begin{equation}\label{eq:sd}
\sum_{i=1}^n \sum_{k=1}^\ell b(i,j,k)(1-x(i))y(i,j,k) = \sum_{i=1}^n s(i,j) + \sum_{k=1]}^\ell z(j,k)
\end{equation}
The theorem will use this connection critically in its analysis, but otherwise follows the same structure as the proof in the single slot case.
\begin{theorem}
The algorithm $\cA$ is $(1-\frac{1}{c})(1-R_{max})$-competitive for the online ad allocation problem with multiple slots.
\end{theorem}
\begin{proof}
As in the single slot case we will show the following three claims:
\begin{enumerate}
\item The solution returned by $\cA$ for $(P)$ is feasible.
\item The increase in the solution for $(P)$ in each iteration is at most $(1+\frac{1}{c-1})$ times in the increase in the solution for $(D)$ during that iteration.
\item The solution returned for $(D)$ is ``nearly feasible". We will derive the precise bounds when we reach this step of the proof. 
\end{enumerate}
Then, just before we can achieve the desired competitive ratio using weak duality.  The proof for claims $1.$ and $3.$ are directly analogous to their proofs in the single slot case. So we shall focus our attention on the proof of claim $2.$ where strong duality is used.
\paragraph{}
Consider the iteration for item $j$. The increase in $x(i)$ during this iteration is 
$$ \sum_{k=1}^\ell (x(i)\frac{b(i,j,k)y(i,j,k)}{B(i)} + \frac{b(i,j,k)y(i,j,k)}{(c-1)B(i)})$$
for each $i \in [n]$. So the increase in $(P)$ is
$$\sum_{i=1}^n z(j,i) + \sum_{k=1}^\ell s(i,j) +\sum_{i=1}^n\sum_{k=1}^\ell B(i) (x(i)\frac{b(i,j,k)y(i,j,k)}{B(i)} + \frac{b(i,j,k)y(i,j,k)}{(c-1)B(i)})$$
Using strong duality equation (\ref{eq:sd}) this is equal to
$$\sum_{i=1}^n \sum_{k=1}^\ell b(i,j,k)(1-x(i))y(i,j,k) + \sum_{i=1}^n \sum_{k=1}^\ell (b(i,j,k)x(i)y(i,j,k) + \frac{b(i,j,k)y(i,j,k)}{c-1}) $$
Simplifying this gives
$$\sum_{i=1}^n \sum_{k=1}^\ell b(i,j,k)y(i,j,k)(1+\frac{1}{c-1}).$$
Now by step $1c$ the increase in $(D)$ is precisely
$$\sum_{i=1}^n \sum_{k=1}^\ell b(i,j,k)y(i,j,k)$$
and so claim $2.$ holds.
\end{proof}
\section{Conclusions}
\paragraph{}
Putting things together, we can synthesize a general approach to using the primal-dual method for designing competitive online algorithms. First one formulates the offline version of their problem as a primal-dual pair of linear programs such that the introduction of a new part of the input in the online model corresponds to the introduction of some new constraints in the primal and some new variables to the dual. Then design an algorithm which additively decides the value of the new dual variables, and multiplicatively updates the primal variables in the new primal constraints until they are satisfied. This multiplicative update will secure a high quality competitive factor if chosen properly.
\paragraph{}
The analysis of such general algorithms above proceeds via three claims. First that the primal solution is feasible. Second that the increase in the primal in each iteration can be bounded by some factor of the increase in the dual. The tighter you get this bound, the better your competitive ratio. Third that the dual solution violates its constraints by some small factor. That factor will appear in your competitive ratio and thus should be as tight as possible. The three claims together can be filtered through weak duality to provide a bound on the competitive ratio with factors coming from claim $2$ and $3$.
\paragraph{}
For the ad-auction problems we gave algorithms for both the single-slot and multi-slot versions with a competitive ratio that tends to $(1-\frac{1}{e})$. These were first studied in \cite{buchbinder2007online}. In \cite{mehta2007adwords} they demonstrate that the ratios we give are in fact tight, showing that $(1-\frac{1}{e})$ is a lower bound for these problems. 
\paragraph{}
Some work has been done on going beyond the lower bounds by instituting additional assumptions for ad-auctions that are reasonable in real world applications. These are detailed in \cite{buchbinder2009design}. One possibility is to assume that each bidder is likely to spend some fraction of their budget, say $g_i$. Such an assumption can be verified by observing past behaviour of the bidder in the real world. In this case, known as the stochastic information case, one can use primal-dual with some modifications to achieve a competitive ratio of $$1-\frac{1-g}{e^{1-g}}$$
where $g = \min_{i} g_i$. Another assumption that allows one to get below the lower bound is the bounded degree assumption. Here we assume that the buyer is interested in at most $d$ of the items. Usually $d$ is much smaller than $m$. In this case a ratio of 
$$1-\frac{d-1}{d(1+\frac{1}{d-1})^{d-1}}$$
can be achieved.
\bibliography{references}
\bibliographystyle{plain}
\end{document}
