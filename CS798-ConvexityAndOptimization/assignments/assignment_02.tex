\documentclass[letterpaper,12pt,oneside,onecolumn]{article}
\usepackage[margin=1in, bottom=1in, top=1in]{geometry} %1 inch margins
\usepackage{amsmath, amssymb, amstext}
\usepackage{fancyhdr}
\usepackage{mathtools}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{theorem}
\usepackage{tikz}
\usepackage{tkz-berge}

%Macros
\newcommand{\A}{\mathbb{A}} \newcommand{\C}{\mathbb{C}}
\newcommand{\D}{\mathbb{D}} \newcommand{\F}{\mathbb{F}}
\newcommand{\N}{\mathbb{N}} \newcommand{\R}{\mathbb{R}}
\newcommand{\T}{\mathbb{T}} \newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
 
 
\newcommand{\cA}{\mathcal{A}} \newcommand{\cB}{\mathcal{B}}
\newcommand{\cC}{\mathcal{C}} \newcommand{\cD}{\mathcal{D}}
\newcommand{\cE}{\mathcal{E}} \newcommand{\cF}{\mathcal{F}}
\newcommand{\cG}{\mathcal{G}} \newcommand{\cH}{\mathcal{H}}
\newcommand{\cI}{\mathcal{I}} \newcommand{\cJ}{\mathcal{J}}
\newcommand{\cK}{\mathcal{K}} \newcommand{\cL}{\mathcal{L}}
\newcommand{\cM}{\mathcal{M}} \newcommand{\cN}{\mathcal{N}}
\newcommand{\cO}{\mathcal{O}} \newcommand{\cP}{\mathcal{P}}
\newcommand{\cQ}{\mathcal{Q}} \newcommand{\cR}{\mathcal{R}}
\newcommand{\cS}{\mathcal{S}} \newcommand{\cT}{\mathcal{T}}
\newcommand{\cU}{\mathcal{U}} \newcommand{\cV}{\mathcal{V}}
\newcommand{\cW}{\mathcal{W}} \newcommand{\cX}{\mathcal{X}}
\newcommand{\cY}{\mathcal{Y}} \newcommand{\cZ}{\mathcal{Z}}

\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}


\newenvironment{proof}{{\bf Proof:  }}{\hfill\rule{2mm}{2mm}}
\newenvironment{proofof}[1]{{\bf Proof of #1:  }}{\hfill\rule{2mm}{2mm}}
\newenvironment{proofofnobox}[1]{{\bf#1:  }}{}\newenvironment{example}{{\bf Example:  }}{\hfill\rule{2mm}{2mm}}

%\renewcommand{\thesection}{\lecnum.\arabic{section}}
%\renewcommand{\theequation}{\thesection.\arabic{equation}}
%\renewcommand{\thefigure}{\thesection.\arabic{figure}}

\newtheorem{fact}{Fact}[section]
\newtheorem{lemma}[fact]{Lemma}
\newtheorem{theorem}[fact]{Theorem}
\newtheorem{definition}[fact]{Definition}
\newtheorem{corollary}[fact]{Corollary}
\newtheorem{proposition}[fact]{Proposition}
\newtheorem{claim}[fact]{Claim}
\newtheorem{exercise}[fact]{Exercise}
\newtheorem{note}[fact]{Note}
\newtheorem{conjecture}[fact]{Conjecture}

\newcommand{\size}[1]{\ensuremath{\left|#1\right|}}
\newcommand{\ceil}[1]{\ensuremath{\left\lceil#1\right\rceil}}
\newcommand{\floor}[1]{\ensuremath{\left\lfloor#1\right\rfloor}}

%END MACROS
%Page style
\pagestyle{fancy}

\listfiles

\raggedbottom

\lhead{2017-03-29}
\rhead{William Justin Toth CS798-Convexity and Optimization Assignment 2} %CHANGE n to ASSIGNMENT NUMBER ijk TO COURSE CODE
\renewcommand{\headrulewidth}{1pt} %heading underlined
%\renewcommand{\baselinestretch}{1.2} % 1.2 line spacing for legibility (optional)

\begin{document}

%Question 1
\section{}
\paragraph{}
Let $A \in \{0,1\}^{m\times n}$ be a matrix. Let $\mathbf{1}$ denote the all-ones vector where dimension is clear from context. Consider the following $(LP)$:
\begin{align*}
\min\ &\mathbf{1}^T x \\
\text{s.t. } Ax &\geq \mathbf{1} \\
x &\geq 0.
\end{align*}
\paragraph{}
We intend to use the multiplicative weight update method to solve this linear program. So we will need an oracle function. Thinking ahead to our intention to use multiplicative weights within a binary search framework we will design our oracle with a parameter representing the intended upper bound on our linear program. Let $\mu \in [0,n]$ and define the oracle $\cO(\mu)$ as follows:
\begin{enumerate}
\item Suppose $p\in \R^m_+$ represents the probability distribution passed to the oracle.
\item Let $a_j$ denote the $j$-th column of $A$. Compute
$$j^* = \text{argmax}_{j \in [n]} \{p^Ta_j\}.$$
\item If $p^Ta_{j^*} = 0$ return infeasible.
\item Let $r = \frac{1}{p^Ta_{j^*}}$. If $r  > \mu$ return infeasible.
\item Set $x_{j^*} = r$ and set $x_j = 0$ for all $j \neq j^*$.
\item Return $x$.
\end{enumerate}
\begin{lemma}\label{lemma:oracle}
Let $(LP)(p)$ be the linear program
$$\min\{\mathbf{1}^Tx : p^TAx \geq 1 \text{ and } x \geq 0\}.$$
If $\cO(\mu)$ returns infeasible then there is no feasible solution to $(LP)(p)$ of cost at most $\mu$. Otherwise $\cO(\mu)$ returns an optimal solution to $(LP)(p)$. The width of $\cO(\mu)$ is at most $n$.
\end{lemma}
\begin{proof}
If $\cO(\mu)$ returns infeasible in step $3$ then $p^TA = 0$ and hence $(LP)(p)$ is infeasible. We will show that $\cO(\mu)$ returns an optimal solution to $(LP)(p)$ of cost $r$, and thus if $\cO(\mu)$ returns infeasible in step $4$ then by optimality there is no feasible solution to $(LP)(p)$ of cost  at most $\mu$.
\paragraph{}
To see the optimality of $x$ observe the dual of $(LP)(p)$:
$$\max\{y : yp^Ta_j \leq 1 \forall j \in [n] \text{ and } y \geq 0\}.$$
By our choice of $j^*$, setting $y = r = \frac{1}{p^Ta_{j^*}}$ satisfies all constraints of the dual to $(LP)(p)$ and thus by strong duality, $x$ is an optimal solution to $(LP)(p)$.
\paragraph{}
It remains to verify the width of $\cO(\mu)$. Consider of row $A_i$ of $A$. If $A_{ij^*} = 0$ then $|A_i x - 1 | = 1 \leq n$. If $A_{ij^*} = 1$ then
$$|A_i x - 1| = |r - 1| \leq \mu -1 \leq n.$$
Thus the width of $\cO(\mu)$ is at most $n$.
\end{proof}
We will use a version the procedure given in Lecture $9$ for solving linear programs via multiplicative weight update. Let $\cM(\mu)$ denote the procedure:
\begin{enumerate}
\item Let $p^{(1)} = \frac{1}{m} \mathbf{1}.$
\item For $t = 1, \dots, T$
	\begin{enumerate}
	\item Use $\cO(\mu)$ with $p^{(t)}$ to find $x^{(t)}$ such that $(p^{(t)})^TA x^{(t)} \geq 1.$
	\item For all $i$ set $m_i^{(t)} = \frac{A_ix^{(t)} - b_i}{n}.$
	\item Use the multiplicative weight update method with $m_i^{(t)}$ to compute $p^(t+1)$.
	\end{enumerate}
\item Return $x = \frac{1}{T}\sum_{t=1}^T x^{(t)}.$
\end{enumerate}
We have the following Theorem from class, which by Lemma \ref{lemma:oracle} we can modify slightly with respect to upper bound $\mu$.
\begin{theorem}\label{th:mwu}
Let $\epsilon > 0$ be a given error parameter. Then $\cM(\mu)$ either finds a solution $x$ such that $A_ix\geq b_i - \epsilon$ for all $i$, or correctly concludes that the system has no feasible solution of cost at most $\mu$. The algorithm $\cM(\mu)$ is called at most $O(n^2\log (m) / \epsilon^2)$ times with $O(m)$ additional processing per call.
\end{theorem}
The theorem as stated in Lecture $9$ said that the oracle correctly concludes the system is infeasible, but by lemma \ref{lemma:oracle} this needs to be changed to infeasibility for $x$ of cost at most $\mu$. We note that feasibility is not affected by ignoring constraints $x\geq 0$ since our oracle always gives non-negative solutions and thus there average will be non-negative.
\paragraph{}
Now we want to use $\cM(\cdot)$ in a bineary search procedure to find a solution to $(LP)$ with additive error $\epsilon>0$. Consider the following algorithm denoted $\cA$:
\begin{enumerate}
\item Fix $\ell = 0$ and $u = n$. Set $x = null$ (denoting infeasible)
\item While $\ell \neq u$ do:
	\begin{enumerate}
	\item Compute $\mu = \floor{\frac{\ell + u}{2}}$
	\item Run $\cM(\mu)$ with error parameter $\epsilon$.
	\item If $\cM(\mu)$ returns infeasible set $\ell = \mu$.
	\item Otherwise let $x$ be the solution returned by $\cM(\mu)$ and set $u = \mu$.
	\end{enumerate}
\item Return $x$.
\end{enumerate}
\paragraph{}
The binary search procedure $\cA$ will find the optimal $\mu$ for which $\cM(\mu)$ returns a feasible solution. Therefore $\cA$ returns an optimal solution to $(LP)$ and by Theorem \ref{th:mwu} the solution returned is infeasible by at most an additive error of $\epsilon$. The running time of binary search is $O(\log(n))$. Our oracle uses an additional $O(m)$ processing to find $j^*$. Therefore, along with the running time of $\cM(\mu)$, our algorithm runs in time
$$O(\frac{m^2n^2\log(m)\log(n)}{\epsilon^2}).$$
%Question 2
\section{}
\paragraph{}
Let $A \in \R^{m\times n}$ and let $i$-th row of $A$ be denoted by $a_i$. Let $b \in \R^m$. We assume that each entry of $A$ and $b$ has absolute value at most $c$. Let $a_0$ be a row of zeroes and let $b_0$ be zero. To solve the system of linear inequalities we can reformulate as
$$\min_{x \in \R^n} f(x)$$
where $f(x) :=\max_{0 \leq i \leq m} (b_i - a_ix)$. We added $b_0 - a_0 x = 0$ to avoid the situation where the polytope described by $Ax \geq b$ is unbounded. In such a situation attempting this optimization would lead to something unbounded, which is undesirable. Now $f(x) \geq 0$, and for any feasible point $x^*$, $f(x^*) = 0$. Thus any feasible point is optimal for this choice of function $f$. We attempt to approximate $f(x)$ with the $0$-convex soft-max function (for some parameter $K$):
$$g(x) := \frac{1}{K}\log (1 + \sum_{i=1}^m \exp (K(b_i - a_i x))).$$
Using the inequalities for log-sum-exp we saw in lecture $2$ we see this function has the property that
$$f(x) \leq g(x) \leq f(x) + \frac{\log(m + 1)}{k}.$$
We let $\cA(\epsilon, \beta)$ denote the algorithm of running standard gradient descent on $g(x)$ with accuracy $\epsilon$ and step size $\eta = \frac{1}{\beta}$. We will use the theorem from class lecture $7$ on the convergence rate of gradient descent on $\beta$-smooth functions:
\begin{theorem}\label{th:gd}
If $f$ is $\beta$-smooth then $\cA(\epsilon, \beta)$ returns a point $x$ such that $f(x) -f(x^*) \leq \epsilon$, where $x^*$ minimizes $f$,  in running time $O(\frac{\beta}{\epsilon})$.
\end{theorem}
\paragraph{}
We need to determine two things: first what $K$ and $\epsilon$ to choose so that optimizing $g$ gives us the desired accuracy with respect to $f$, and second what is an upper bound on $\beta$.
\paragraph{}
Let $\epsilon >0$ be our desired accuracy for approximating $Ax \geq b$. That is we want $x$ such that $Ax \geq b - \epsilon \textbf{1}$. Equivalently we want $x$ such that $f(x) - f(x^*) \leq \epsilon$ where $x^*$ minimizes $f$ (that is, $x^*$ is feasible for $Ax \geq b$). Suppose we run $\cA(\frac{\epsilon}{2}, \beta)$ on $g$ with some choice of $K$ and $\beta$ obtaining a point $x$. Say $\tilde{x}$ minimizes $g$. Then is we set $K = \frac{2\log(m+1)}{\epsilon}$ we obtain:
$$f(x) -f(x^*) \leq g(x) -g(x^*) + \frac{\log(m+1)}{K} \leq g(x) - g(\tilde{x}) + \frac{\log(m+1)}{K} \leq \frac{\epsilon}{2} + \frac{\log(m+1)}{\frac{2\log(m+1)}{\epsilon}} = \epsilon.$$
\paragraph{}
Thus if we run $\cA(\frac{\epsilon}{2}, \beta)$, the $x$ returned will achieve our desired accuracy for the system of linear inequalities. It remains to bound $\beta$ so we can calculate the running time. We compute a generic entry of the gradient of $g$ (and then later an entry of the hessian). For ease of notation we let $z(x) \in \R^m$ we defined by $z_i(x) = \exp(b_i-a_ix)$ and let $S(x) = \textbf{1}^Tz(x) +1$.
$$\nabla g(x)_j = \frac{\partial}{\partial x_j} g(x) = \frac{1}{S(x)} \sum_{i=1}^m z_i(x)a_{ij}.$$
From this we compute a generic entry of the hessian:
$$\nabla^2g(x)_{jk} = \frac{\partial^2}{\partial x_k \partial x_j} g(x) = \frac{K}{S(x)^2} [(\sum_{i=1}^m z_i(x)a_{ik})(z_i(x)a_{ij}) - S(x)\sum_{i=1}^m z_i(x)a_{ij}a_{ik}].$$ 
We want to obtain a bound on the largest eigenvalue, $\lambda_1$, of $\nabla^2g(x)$. To do this we use the loose bound
$$\lambda_1 \leq n \max_{j,k} |\nabla^2g(x)_{jk}|.$$
Observe that
\begin{align*}
|\nabla^2g(x)_{jk}| &= |\frac{K}{S(x)^2} [(\sum_{i=1}^m z_i(x)a_{ik})(z_i(x)a_{ij}) - S(x)\sum_{i=1}^m z_i(x)a_{ij}a_{ik}]| \\
&\leq \frac{K}{S(x)^2}[ |\sum_{i=1}^m z_i(x)a_{ik}| \cdot |\sum_{i=1}^m z_i(x)a_{ij}| + S(x) |\sum_{i=1}^m z_i(x)a_{ij}a_{ik}| ]&\text{note that $S(x) \geq 0$} \\
&\leq \frac{K}{S(x)^2}[ c|\sum_{i=1}^m z_i(x)| \cdot c|\sum_{i=1}^m z_i(x)| + S(x) c^2|\sum_{i=1}^m z_i(x)| ] &\text{since $|a_{ij}| \leq c$}\\
&\leq \frac{Kc^2}{S(x)^2}[2S(x)^2 ] \\
&= 4c^2\log(m) &\text{by our choice of $K$}.
\end{align*}
Therefore $\lambda_1 \leq 4c^2 n\log(m)$. Hence we can run our algorithm with $\beta = 4c^2n\log(m)$. So the final running time is
$$O(\frac{4c^2n\log(m)}{\frac{\epsilon}{2}}) = O(\frac{c^2n\log(m)}{\epsilon}).$$
%TODO compute the Hessian and determine for which \beta is g \beta-smooth

%TODO figure out which values of K and epilson' are needed so that we can find a point x with approximates f(x)-f(x^*) \leq \epsilon

%TODO propose algorithm: run gradient descent on g with K and \epsilon'

%TODO verify correctness and running time using theorem about gradient descent on \beta-smooth functions

%Question 3
\section{}
\paragraph{}
I collaborated with Sharat on this problem.
\paragraph{}
Let $\cA$ denote our black-box algorithm. Let $f$ be a $\beta$-smooth function. Given an initial point $x_0$ and desired accuracy $\epsilon > 0$ our algorithm does:
\begin{enumerate}
\item If $||x_0 - x^*||_2^2 = 0$ return $x_0$.
\item Let $\epsilon' = \epsilon/2$ and set $\alpha = \frac{\epsilon}{||x_0 - x^*||_2^2}$.
\item Let $g$ be the function given by $g(x) := f(x) + \frac{\alpha}{2}||x - x_0||_2^2$.
\item Run $\cA$ on $g$ with accuracy $\epsilon'$ and starting point $x_0$ returning a point $x'$.
\item Return solution $x'$.
\end{enumerate} 
Our algorithm assumes that the value of $||x_0 - x^*||_2$ is known in advance. In the trivial case where we know $x^*$ the algorithm returns it in step $1$. So we may assume that $x_0 \neq x^*$.
\begin{lemma}\label{lemma:convex-smooth}
The function $g$ is $\alpha$-strongly convex and $(\beta + \alpha)$-smooth.
\end{lemma}
\begin{proof}
The Hessian of $g$ is:
$$\nabla^2 g = \nabla^2 f + \alpha I.$$
Since we know $f$ is $\beta$-smooth we have
$$0I \preccurlyeq \nabla^2 f \preccurlyeq \beta I.$$
Since adding scalar multiples of $I$ only shifts eigenvalues we observe
$$\alpha I \preccurlyeq \nabla^2 g \preccurlyeq (\beta + \alpha) I.$$
Hence the lemma holds as desired. 
\end{proof}
\paragraph{}
By lemma \ref{lemma:convex-smooth} we know that $\cA$ can process our function $g$. It remains to verify the accuracy of the point returned and to bound the running time.
\begin{lemma}\label{lemma:q3-accuracy}
The point $x'$ returned by our algorithm satisfies
$$f(x') - f(x^*) \leq \epsilon.$$
\end{lemma}
\begin{proof}
Let $\tilde{x}$ be the optimal solution for $g$, By our guarantees on $\cA$:
$$g(x') - g(\tilde{x}) \leq \epsilon'.$$
Now observe that
\begin{align*}
f(x') - f(x^*) &= g(x') - \frac{\alpha}{2}||x'-x_0||^2_2 - g(x^*) +\frac{\alpha}{2}||x^*-x_0||_2^2 \\
&\leq g(x') - g(x^*) + \frac{\alpha}{2}||x^*-x_0||_2^2 &\text{since $\frac{\alpha}{2}||x'-x_0||^2_2\geq 0$} \\
&= g(x') - g(x^*) + \frac{\epsilon}{2} &\text{by our choice of $\alpha$} \\
&\leq g(\tilde{x}) + \epsilon' -g(x^*) + \frac{\epsilon}{2} \\
&\leq \epsilon' +\frac{\epsilon}{2} &\text{by optimality of $\tilde{x}$} \\
&= \epsilon &\text{by our choice of $\epsilon'$}.
\end{align*}
\end{proof}
\paragraph{}
Therefore our algorithm operates properly and returns a point within the desired accuracy. It remains to verify the running time of our algorithm. This running time is entirely bounded by the operation of $\cA$ on $g$ with $\epsilon'$. By the guarantee on $\cA$ this gives a bound of:
\begin{align*}
O(\sqrt{\frac{\beta}{\alpha} + 1}\log (\frac{g(x_0)-g(\tilde{x})}{\epsilon'})) &= O(\sqrt{\frac{\beta}{\alpha}}\log (\frac{g(x_0)-g(\tilde{x})}{\epsilon})) &\text{dropping constants}\\
&= O(\sqrt{\frac{\beta}{\alpha}}\log (\frac{f(x_0)-f(\tilde{x}) - \frac{\alpha}{2}||x_0-x^*||^2_2}{\epsilon})) &\text{expanding $g$}\\
&= O(\sqrt{\frac{\beta}{\alpha}}\log (\frac{f(x_0)-f(\tilde{x})}{\epsilon})) &\text{dropping non-positive term} \\
&= O(\sqrt{\frac{\beta}{\alpha}}\log (\frac{f(x_0)-f(x^*)}{\epsilon})) &\text{by optimality of $x^*$} \\
&= O(\sqrt{\frac{\beta}{\epsilon}} ||x_0-x^*||_2\log (\frac{f(x_0)-f(x^*)}{\epsilon}))  &\text{expanding $\alpha$}.
\end{align*}
Thus we obtain the desired running time. Together with lemma \ref{lemma:q3-accuracy} this completes the problem.
\end{document}
