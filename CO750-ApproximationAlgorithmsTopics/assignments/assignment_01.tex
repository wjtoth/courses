\documentclass[letterpaper,12pt,oneside,onecolumn]{article}
\usepackage[margin=1in, bottom=1in, top=1in]{geometry} %1 inch margins
\usepackage{amsmath, amssymb, amstext}
\usepackage{fancyhdr}
\usepackage{mathtools}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{theorem}
\usepackage{tikz}
\usepackage{tkz-berge}

%Macros
\newcommand{\A}{\mathbb{A}} \newcommand{\C}{\mathbb{C}}
\newcommand{\D}{\mathbb{D}} \newcommand{\F}{\mathbb{F}}
\newcommand{\N}{\mathbb{N}} \newcommand{\R}{\mathbb{R}}
\newcommand{\T}{\mathbb{T}} \newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
 
 
\newcommand{\cA}{\mathcal{A}} \newcommand{\cB}{\mathcal{B}}
\newcommand{\cC}{\mathcal{C}} \newcommand{\cD}{\mathcal{D}}
\newcommand{\cE}{\mathcal{E}} \newcommand{\cF}{\mathcal{F}}
\newcommand{\cG}{\mathcal{G}} \newcommand{\cH}{\mathcal{H}}
\newcommand{\cI}{\mathcal{I}} \newcommand{\cJ}{\mathcal{J}}
\newcommand{\cK}{\mathcal{K}} \newcommand{\cL}{\mathcal{L}}
\newcommand{\cM}{\mathcal{M}} \newcommand{\cN}{\mathcal{N}}
\newcommand{\cO}{\mathcal{O}} \newcommand{\cP}{\mathcal{P}}
\newcommand{\cQ}{\mathcal{Q}} \newcommand{\cR}{\mathcal{R}}
\newcommand{\cS}{\mathcal{S}} \newcommand{\cT}{\mathcal{T}}
\newcommand{\cU}{\mathcal{U}} \newcommand{\cV}{\mathcal{V}}
\newcommand{\cW}{\mathcal{W}} \newcommand{\cX}{\mathcal{X}}
\newcommand{\cY}{\mathcal{Y}} \newcommand{\cZ}{\mathcal{Z}}

\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}


\newenvironment{proof}{{\bf Proof:  }}{\hfill\rule{2mm}{2mm}}
\newenvironment{proofof}[1]{{\bf Proof of #1:  }}{\hfill\rule{2mm}{2mm}}
\newenvironment{proofofnobox}[1]{{\bf#1:  }}{}\newenvironment{example}{{\bf Example:  }}{\hfill\rule{2mm}{2mm}}

%\renewcommand{\thesection}{\lecnum.\arabic{section}}
%\renewcommand{\theequation}{\thesection.\arabic{equation}}
%\renewcommand{\thefigure}{\thesection.\arabic{figure}}

\newtheorem{fact}{Fact}[section]
\newtheorem{lemma}[fact]{Lemma}
\newtheorem{theorem}[fact]{Theorem}
\newtheorem{definition}[fact]{Definition}
\newtheorem{corollary}[fact]{Corollary}
\newtheorem{proposition}[fact]{Proposition}
\newtheorem{claim}[fact]{Claim}
\newtheorem{exercise}[fact]{Exercise}
\newtheorem{note}[fact]{Note}
\newtheorem{conjecture}[fact]{Conjecture}

\newcommand{\size}[1]{\ensuremath{\left|#1\right|}}
\newcommand{\ceil}[1]{\ensuremath{\left\lceil#1\right\rceil}}
\newcommand{\floor}[1]{\ensuremath{\left\lfloor#1\right\rfloor}}

%END MACROS
%Page style
\pagestyle{fancy}

\listfiles

\raggedbottom

\lhead{2017-02-09}
\rhead{William Justin Toth CO750-Approximation Algorithms Assignment 1} %CHANGE n to ASSIGNMENT NUMBER ijk TO COURSE CODE
\renewcommand{\headrulewidth}{1pt} %heading underlined
%\renewcommand{\baselinestretch}{1.2} % 1.2 line spacing for legibility (optional)

\begin{document}
%Q1
\section{}
\paragraph{}
Let $E$ be the edge set of a graph $G$. Let $\cS$ be the collection of all sets $S \subseteq E$ that are matchings of $G$. Let $w: E \rightarrow \R_+$ be a weight function on the edges. For a subset $\bar{E} \subseteq E$ we denote by $G[\bar{E}]$ the subgraph of $G$ induced by the edges in $\bar{E}$.
	%Pa
\subsection{a}
\paragraph{}
Let $\bar{E} \subseteq E$. Suppose that $M \subseteq \bar{E}$ is an inclusion-wise maximal matching of $G[\bar{E}]$. Let $\bar{M}$ be a matching of $G[\bar{E}]$. Let $f : M \rightarrow \{ \{e_1,e_2\} | e_1, e_2 \in \bar{M}\}$ be defined by
$$ f(uv) = \{e_1, e_2\}$$
where $e_1 \in \bar{M}$ covers $u$ and $e_2 \in \bar{M}$ covers $v$, for each $uv \in M$.
\paragraph{}
We claim that for all $uv \in \bar{M}$, there exists $P \in Im(f)$ (where $Im(f)$ denotes the image of $f$) such that $uv \in P$. To see this suppose for a contradiction that there exists some $uv \in \bar{M}$ where each $P \in Im(f)$ does not contain $uv$. Then by the definition of $f$, neither $u$ nor $v$ are covered by $M$. But this violates that $M$ is inclusion-wise maximal since $uv \not\in M$ and $M \cup \{uv\}$ is a matching. Hence the claim holds.
\paragraph{}
From the previous claim we observe $|\bar{M}| \leq | \cup_{P \in Im(f)} P|$. But this yields a string of inequalities which complete the proof:
\begin{align*}
|\bar{M}| &\leq |\cup_{P \in Im(f)} P| \\
&\leq \sum_{P \in Im(f)} |P| \\
&\leq \sum_{P \in Im(f)} 2 \\
&\leq 2|Im(f)| \\
&\leq 2|M|. 
\end{align*}
With the last inequality following since $M$ is the domain of $f$. Hence we have $|M| \geq \frac{1}{2}|\bar{M}|$ as desired. $\blacksquare$
	%Pb
\subsection{b}
(This argument follows the exact same lines as the argument for problem $2$)
\paragraph{}
We can decided in polynomial time if $M\cup \{e\}$ is a matching in $\cS$ by iterating over $V$, counting the number of edges in $M\cup\{e\}$ which cover each vertex in $V$, and returning ``No" is the count is greater than $1$ for any vertex and ``Yes" otherwise. Thus the algorithm clearly runs in polynomial time and returns a feasible set.
\paragraph{}
Let $S$ be the set of edges returned by the greedy algorithm. Order the edges as $S = \{e_1, \dots, e_n\}$ in the order they were taken during the operation of $\cA$. Let $F_i := E\backslash \tilde{E}$ be the edges removed from $E$ at the edge of iteration $i$ of $\cA$ (the iteration where $e_i$ is added to $S$). Let $S^*$ be the optimal solution and partition $S^* = S^*_1 \cup \dots \cup S^*_n$ where $S^*_i = S^* \cap F_i$.
\paragraph{}
We want that $|S^*_i| \leq 2$ to hold, but we may need to redistribute some edges to ensure that this happens (and we will do this without effecting our ability to bound the optimal solution). Run the following algorithm, denoted $\cB$, to redistribute the edges in our partition of $S^*$:
\begin{enumerate}
\item For $i = 1, \dots, n$
\begin{enumerate}
\item While $|S^*_i| > 2$: take an edge $e$ from $S^*_i$ and redistribute $e$ to some $S^*_j$ for $j<i$ with $|S^*_j| < 2$.
\end{enumerate}
\end{enumerate}
\paragraph{Claim 1}
After the operation of $\cB$, for all $i = 1, \dots, n$, $|S^*_i| \leq 2$ and $w_{e_i} \geq w_e$ for all $e \in S^*_i$.
\paragraph{Proof of Claim 1}
We proceed by induction. For $e_1$ $\{e_1\}$ is inclusion-wise maximal on $\cS[F_1]$ (by definition of $F_1$), and $S_1^*$ is feasible on $\cS[F_1]$ since $S^*_1 \subseteq F_1$ and $S^* \in \cS$. Thus we have $|\{e_1\}| \geq \frac{1}{2} |S_1^*|$, and hence
$$|S^*_1| \leq \alpha.$$
Further by our greedy choice of $e_1$, $w_{e_1} \geq w_e$ for all $e \in F_1$ (and hence for all $e \in S^*_1$).
\paragraph{}
Suppose that the claim holds for $S^*_1 ,\dots, S^*_i$. Now by our greedy choice, and induction, $w_{e_{i+1}} \geq w_{e_j} \geq w_e$ for all $e \in S^*_j$ for all $j = 1,\dots, i$. So when $\cB$ redistributes edges of $S^*_{i+1}$ the cost property on the earlier sets is not violated. Also by our greedy choice, any edges that were not moved from $S^*_{i+1}$ by $\cB$ have cost at most $w_{e_{i+1}}$. To finish the proof of the claim, we demonstrate that while $|S_{i+1}^*| > \alpha$ there exists some $S^*_j$ with $j\leq i$ and $|S^*_j| < 2$. Observe that $\{e_1, \dots, e_{i+1}\}$ is inclusion-wise maximal on $\cS[F_1 \cup \dots \cup F_{i+1}]$ by definition of $F_1 \cup \dots \cup F_{i+1}$. Also $\bigcup_{j=1}^{i+1} S^*_j \in \cS[F_1 \cup \dots \cup F_{i+1}]$ so we have that
$$|\{e_1, \dots, e_{i+1}\}| \geq \frac{1}{2} |\bigcup_{j=1}^{i+1} S^*_j | = \frac{1}{2} \sum_{j=1}^{i+1} |S^*_j|.$$
Thus we see that
$$ \frac{1}{i+1}\sum_{j=1}^{i+1} |S^*_j| \leq 2.$$
Now since the desired bound holds on average, if $|S^*_{i+1}| > 2$ there exists some $S^*_j$ with $j \leq i$ and $|S^*_j| < 2$ (otherwise the bound does not hold). Thus we have proven Claim $1$. $\blacksquare$
\paragraph{}
With Claim $1$ in hand we are prepared to compute the desired approximation factor:
\begin{align*}
w(S^*) &= \sum_{i=1}^n c(S^*_i) \\
&\leq \sum_{i=1}^n w_{e_i} |S^*_i| \\
&\leq \sum_{i=1}^n w_{e_i} 2 \\
&= 2 w(S).
\end{align*}
Thus $w(S) \geq \frac{1}{2} w(S^*)$ and hence the approximation factor holds as desired. $\blacksquare$
	%Pc
	\subsection{c}
	\paragraph{}
	Suppose for a contradiction that the Greedy Algorithm is in fact a $(\frac{1}{2} + \epsilon)$-approximation algorithm for some $\epsilon \in (0, \frac{1}{2}]$. Then consider the following example. Let $G$ be the path graph with vertex set $V(G) = \{a,b,c,d\}$ and edge set $E(G) = \{ab, bc,cd\}$. Define the weight function $w : E(G) \rightarrow \R_+$ by
	$$w_{ab} = w_{cd} = 1 \quad\text{and}\quad w_{bc} = 1+\epsilon.$$
	Then the Greedy algorithm would choose the edge $bc$ as its solution. This solution has cost $1+\epsilon$. The optimal solution is to take edges $ab$ and $cd$ with cost $2$. The ratio of these solutions is
	$$ \frac{1+\epsilon}{2} = \frac{1}{2} + \frac{\epsilon}{2} < \frac{1}{2} + \epsilon$$
	Contradicting that the Greedy Algorithm is a $(\frac{1}{2} + \epsilon)$-approximation algorithm. $\blacksquare$
%Q2
\section{}
\paragraph{}
Let $E$ be a set of $m$ elements and $\cS$ a collection of sets $S \subseteq E$. Let $w: E\rightarrow \R_+$ be a weight function. Let $\cS[\bar{E}] := \{ S \in \cS : S \subseteq \bar{E}\}$. Suppose that $\cS$ satisfies the following for a given $\alpha \geq 1$:
\begin{enumerate}
\item $S \in \cS$ implies $S' \in \cS$ for any $S' \subseteq S$
\item For all $\bar{E} \subseteq E$, if $S$ is an inclusion-wise maximal set in $\cS[\bar{E}]$ then $|S| \geq \frac{1}{\alpha} |\bar{S}|$ for all $\bar{S} \in \cS[\bar{E}]$.
\end{enumerate}
We will show the Greedy Algorithm, denoted $\cA$, is a $\frac{1}{\alpha}$-approximation algorithm for the problem of finding a maximum weight $S \in \cS$.
\paragraph{}
Since we have a polynomial time oracle for membership in $\cS$ the algorithm clearly runs in polynomial time and returns a feasible set.
\paragraph{}
Let $S$ be the set of edges returned by the greedy algorithm. Order the edges as $S = \{e_1, \dots, e_n\}$ in the order they were taken during the operation of $\cA$. Let $F_i := E\backslash \tilde{E}$ be the edges removed from $E$ at the edge of iteration $i$ of $\cA$ (the iteration where $e_i$ is added to $S$). Let $S^*$ be the optimal solution and partition $S^* = S^*_1 \cup \dots \cup S^*_n$ where $S^*_i = S^* \cap F_i$.
\paragraph{}
We want that $|S^*_i| \leq \alpha$ to hold, but we may need to redistribute some edges to ensure that this happens (and we will do this without effecting our ability to bound the optimal solution). Run the following algorithm, denoted $\cB$, to redistribute the edges in our partition of $S^*$:
\begin{enumerate}
\item For $i = 1, \dots, n$
\begin{enumerate}
\item While $|S^*_i| > \alpha$: take an edge $e$ from $S^*_i$ and redistribute $e$ to some $S^*_j$ for $j<i$ with $|S^*_j| < \alpha$.
\end{enumerate}
\end{enumerate}
\paragraph{Claim 1}
After the operation of $\cB$, for all $i = 1, \dots, n$, $|S^*_i| \leq \alpha$ and $w_{e_i} \geq w_e$ for all $e \in S^*_i$.
\paragraph{Proof of Claim 1}
We proceed by induction. For $e_1$ $\{e_1\}$ is inclusion-wise maximal on $\cS[F_1]$ (by definition of $F_1$), and $S_1^*$ is feasible on $\cS[F_1]$ since $S^*_1 \subseteq F_1$ and $S^* \in \cS$. Thus we have $|\{e_1\}| \geq \frac{1}{\alpha} |S_1^*|$, and hence
$$|S^*_1| \leq \alpha.$$
Further by our greedy choice of $e_1$, $w_{e_1} \geq w_e$ for all $e \in F_1$ (and hence for all $e \in S^*_1$).
\paragraph{}
Suppose that the claim holds for $S^*_1 ,\dots, S^*_i$. Now by our greedy choice, and induction, $w_{e_{i+1}} \geq w_{e_j} \geq w_e$ for all $e \in S^*_j$ for all $j = 1,\dots, i$. So when $\cB$ redistributes edges of $S^*_{i+1}$ the cost property on the earlier sets is not violated. Also by our greedy choice, any edges that were not moved from $S^*_{i+1}$ by $\cB$ have cost at most $w_{e_{i+1}}$. To finish the proof of the claim, we demonstrate that while $|S_{i+1}^*| > \alpha$ there exists some $S^*_j$ with $j\leq i$ and $|S^*_j| < \alpha$. Observe that $\{e_1, \dots, e_{i+1}\}$ is inclusion-wise maximal on $\cS[F_1 \cup \dots \cup F_{i+1}]$ by definition of $F_1 \cup \dots \cup F_{i+1}$. Also $\bigcup_{j=1}^{i+1} S^*_j \in \cS[F_1 \cup \dots \cup F_{i+1}]$ so we have that
$$|\{e_1, \dots, e_{i+1}\}| \geq \frac{1}{\alpha} |\bigcup_{j=1}^{i+1} S^*_j | = \frac{1}{\alpha} \sum_{j=1}^{i+1} |S^*_j|.$$
Thus we see that
$$ \frac{1}{i+1}\sum_{j=1}^{i+1} |S^*_j| \leq \alpha.$$
Now since the desired bound holds on average, if $|S^*_{i+1}| > \alpha$ there exists some $S^*_j$ with $j \leq i$ and $|S^*_j| < \alpha$ (otherwise the bound does not hold). Thus we have proven Claim $1$. $\blacksquare$
\paragraph{}
With Claim $1$ in hand we are prepared to compute the desired approximation factor:
\begin{align*}
w(S^*) &= \sum_{i=1}^n c(S^*_i) \\
&\leq \sum_{i=1}^n w_{e_i} |S^*_i| \\
&\leq \sum_{i=1}^n w_{e_i} \alpha \\
&= \alpha w(S).
\end{align*}
Thus $w(S) \geq \frac{1}{\alpha} w(S^*)$ and hence the approximation factor holds as desired. $\blacksquare$
%Q3
\section{}
\paragraph{}
Let $G = (V,E)$ be a graph. For any $S, T \subseteq V$ we denote by $E(S,T)$ the set of edges between $S$ and $T$. Formally $E(S,T) = \{\{s,t\} \in E: s \in S, t \in T\}$.
%Pa
\subsection{a}
\paragraph{}
The fact that the Greedy Algorithm runs in polynomial time is obvious. $|E(v,S)|$ can be computed in $O(|E|)$ time and the while loop runs for $O(|V|)$ iterations. Further it clearly returns a cut. It remains to verify the approximation factor of $\frac{1}{2}$ holds.
\paragraph{}
We show that the approximation factor is maintained throughout operation of the algorithm on the graph $G[V\backslash \bar{V}]$ induced by vertices considered so far. After the first iteration only one vertex has been considered and the approximation factor holds trivially on the graph induced by that one vertex. Now for induction let $V' = V \backslash \bar{V}$ be the set of vertices considered so far by the Greedy Algorithm, and at the start of the next iteration the vertex $v \in V$ is being considered. Let $S \subseteq V$ be the cut set considered so far, and let $S^*$ be the optimal cut set on $G[V']$. Let $\bar{S} = V'\backslash S$ and $\bar{S^*} = V' \backslash S^*$. We may assume without loss of generality that the Greedy Algorithm puts $v \in S$ and the optimal solution puts $v \in S^*$ after this iteration (if this does not hold simply relabel $S$ with $\bar{S}$ below, or $S^*$ with $\bar{S^*}$ with respect to which assumption does not hold). 
\paragraph{}
So after this iteration the optimal solution has value
$$|E(S^* \cup \{v\}, \bar{S^*})| =  \sum_{u \in S^* \cup \{v\}} |E(u, \bar{S^*})| = |E(v,\bar{S^*})| + \sum_{u \in S^*} |E(u, \bar{S^*})| \leq |E(v,\bar{S^*})| + 2\sum_{u \in S} |E(u, \bar{S})|$$
with the inequality following by induction. Hence the approximation factor will hold as desired provided that
$$|E(v,\bar{S^*})| \leq 2|E(v,\bar{S})|.$$
Suppose for a contradiction that
$$|E(v,\bar{S^*})| > 2|E(v,\bar{S})|.$$
We observe that
$$|E(v, \bar{S^*})| = |E(v,\bar{S^*}\backslash S)| + |E(v, \bar{S^*}\cap S)| \leq  |E(v, \bar{S})| + |E(v, S)|.$$
The inequality follows since $\bar{S^*}\backslash S \subseteq V' \backslash S = \bar{S}$, and $\bar{S^*} \cap S \subseteq S$. Combining this inequality with the contradiction assumption we see
$$2|E(v,\bar{S})| <|E(v, \bar{S})| + |E(v, S)|$$
and subtracting $|E(v, \bar{S})|$ from both sides yields
$$ |E(v,\bar{S})| < |E(v,S)|.$$
This contradicts our Greedy choice $v \in S$ since such choice implies
$$ |E(v,\bar{S})| \geq |E(v,S)|.$$
Hence we have
$$|E(v,\bar{S^*})| \leq 2|E(v,\bar{S})|.$$
Thus after this iteration the optimal solution has value:
$$|E(S^* \cup \{v\}, \bar{S^*})|  \leq |E(v,\bar{S^*})| + 2\sum_{u \in S} |E(u, \bar{S})| \leq 2|E(v,\bar{S})| + 2\sum_{u \in S} |E(u, \bar{S})| = 2 |E(S \cup \{v\}, \bar{S})|.$$
Therefore $|E(S \cup \{v\}, \bar{S})| \geq \frac{1}{2} |E(S^* \cup \{v\}, \bar{S^*})|$ as desired. 
\paragraph{}
Now we observe that, from the invariant we just demonstrated, upon termination of the algorithm the approximation factor holds for the greedy solution versus the optimal solution on $G[V \backslash \emptyset] = G$, and hence the Greedy Algorithm is $\frac{1}{2}$-approximation algorithm.$\blacksquare$
%Pb
\subsection{b}
\paragraph{}
Our goal is to find $k$ cuts of $G$ $V_1, \dots, V_k \subseteq V$ maximizing:
$$|\bigcup_{i=1}^k \delta(V_i)|.$$
Let $\cB$ denote the $2$-approximation algorithm for max cut given in problem $3a$. Consider the following algorithm, which we will denote by $\cA$:
\begin{enumerate}
\item Set $V_1 = \dots = V_k = \emptyset$. Set $G_1 = G$
\item For $i = 1, \dots, k$
\begin{enumerate}
\item Let $S$ be the cut returned by $\cB$ run on $G_i$.
\item Set $V_i = S$. Set $G_{i+1} = G_i \backslash \delta(S).$
\end{enumerate}
\item Return $V_1, \dots, V_k$.
\end{enumerate}
The idea behind the operation of $\cA$ is to find an optimal cut, then remove the edges of the cut and iterate.
\begin{lemma}\label{lemma:3b1}
Let $\cO = \{V^*_1, \dots, V^*_k\}$ denote the optimal solution. For any $i=1,\dots, k$ let $\cS^i = \{V_1, \dots, V_{i}\}$ be the set of partitions chosen so far at the end of iteration $i$ by $\cA$. Then 
$$2k(c(\cS^i) - c(\cS^{i-1})) \geq c(\cO) - c(\cS^{i-1}).$$
With $S^0 = \emptyset$.
\end{lemma}
\begin{proof}
For $i = 1,\dots, k$ let $S_i^* = V_i^k \backslash \bigcup_{S\in \cS^{i-1}} S$. Then
$$c(\cO) - c(\cS^{i-1}) = c(\{S^*_1,\dots, S^*_l\}) = |\bigcup_{i=1}^k \delta(S^*_i)|.$$
Let $$S^* = \text{arg max}_{S^*_i : i=1,\dots, k} |\delta(S^*_i)|.$$
Then 
$$c(\cO) - c(\cS^{i-1}) \leq k|\delta(S^*)|.$$
Now observe that for all $i = 1,\dots, k$, $\delta(S^*_i) \subseteq E(G_i)$, hence in particular $\delta(S^*) \subseteq E(G_i)$. Thus $|\delta(S^*)|$ has size at most that of an optimal cut in $G_i$. Now by our choice of $V_i$, using $2$-approximation algorithm $\cB$, $2|\delta(V_i)|$ is at the size of an optimal cut in $G_i$. Hence we have
$$|\delta(S^*)| \leq 2|\delta(V_i)|.$$
Thus, combining inequalities, we observe
$$c(\cO) - c(\cS^{i-1}) \leq 2k |\delta(V_i)|.$$
But $|\delta(V_i)| = c(\cS^i) - c(\cS^{i-1})$ and thus
$$c(\cO) - c(\cS^{i-1}) \leq 2k (c(\cS^i) - c(\cS^{i-1}))$$
as desired.
\end{proof}
\paragraph{}
Using the previous lemma, we can demonstrate that $\cA$ is a $\alpha := 1-(1-\frac{1}{2k})^k$-approximation algorithm for the max $k$-cut problem.
\paragraph{Main Proof}
Since $\cB$ can be run in polynomial time, and $k$ is at most $|V|$, it is clear that step $2$ of $\cA$, and hence $\cA$, runs in polynomial time. At termination the algorithm returns $k$ subsets of $V$, so the algorithm returns a feasible solution.
\paragraph{} 
Now to see the approximation factor holds let $\cS^i$ denote the solution maintained by $\cA$ at the end of iteration $i$. Then the returned solution is $\cS^{k}$. Let $\cO$ be an optimal solution. Observe from Lemma \ref{lemma:3b1} that $c(\cS^{i}) \geq \frac{1}{2k} c(\cO) + (1- \frac{1}{2k})c(S^{i-1})$. We will apply this inequality inductively to achieve the bound:
\begin{align*}
c(\cS^{k}) &\geq \frac{1}{2k} c(\cO) + (1- \frac{1}{2k})c(S^{k-1}) \\
		&\geq \frac{1}{2k} c(\cO) + (1- \frac{1}{2k})(\frac{1}{2k} c(\cO) + (1- \frac{1}{2k})c(S^{k-2})) \\
		&\geq \frac{1}{2k} c(\cO) + (1- \frac{1}{2k})(1 + (1-\frac{1}{2k}) + (1-\frac{1}{2k})^2 + \dots + (1-\frac{1}{2k})^{k-1}) \\
		&= \frac{c(\cO)}{2k} \cdot \frac{1-(1-\frac{1}{2k})^k}{1-(1-\frac{1}{2k})} \\
		&=\frac{c(\cO)}{2k} \cdot \frac{1-(1-\frac{1}{2k})^k}{\frac{1}{2k}}\\
		&= (1-(1-\frac{1}{2k})^k)\cdot c(\cO).
\end{align*}
Thus the approximation factor holds as desired. $\blacksquare$
%Q4
\section{}
\paragraph{}
Let $G = (V,E)$ be a graph with edge costs $c: E \rightarrow \R_+$. Let $r \in V$ be a root vertex. Let $R \subseteq V \backslash \{r\}$ be a set of terminals. Let $d_v \in \{2^0, 2^1, \dots, 2^\delta\}$ denote the demand of $v$ for each $v \in R$. We assume we have an $O(1)$-approximation algorithm for the Steiner Tree problem. Then each iteration of step $2$ of the given algorithm runs in polynomial time using the assumed approximation algorithm. There are $\delta+1$ iterations of step $2$ which is $O(\log \max_v d_v)$ and hence polynomial in the number of bits needed to specify the input to the problem. Thus the entirety of step $2$ runs in polynomial time and hence the given algorithm runs in polynomial time. It is clear the tree returned spans $R \cup \{r\}$ since the partial solution at each iteration $i$ spans $R_\delta \cup \dots \cup R_i \cup \{r\}$. Further at each iteration $i$ the paths in the final solution connecting $v \in R_i$ to $r$ are assigned costs $2^i$ on their edges and hence a feasible flow of value $d_v$ from $r$ to $v$ is possible. Thus the algorithm returns a feasible solution in polynomial time.
\paragraph{}
It remains to verify the approximation factor of $O(1)$. We do so by induction on the number of iterations the algorithm needs to run to return a solution. In the base case, that is when $\delta = 0$, the algorithm returns an $O(1)$-approximate Steiner Tree connecting $R \cup \{r\}$ and installs capacity of $1$ on each edge. By observing that the optimal solution in this case is a Steiner Tree connecting $R \cup \{r\}$ with capacity $1$ on each edge we see that the approximation factor holds in this case.
\paragraph{}
Suppose that $\delta > 0$ and for any problem instance with maximum demand $2^{\alpha}$ with $0 \leq \alpha < \delta$ the algorithm returns an $O(1)$ approximate solution. We will describe solutions by an ordered pair $(T, x)$ where $T$ is a tree and $x : T \rightarrow \R_+$ maps edges of $T$ with their corresponding capacity. Then the objective function, which we denote by $f$, in our problem maps feasible solutions $(T,x)$ as follows:
$$f(T,x) = \sum_{e \in T} c_e x_e.$$
\paragraph{}
Let $(T,x)$ be the solution returned by running our algorithm, and let $(T^*, x^*)$ be the optimal solution. Let $T_{\delta}$ be the $O(1)$-approximate Steiner Tree connecting $R_\delta \cup \{r\}$ in the first iteration (iteration $\delta$) of our algorithm. Let $(\bar{T}, x)$ denote the solution returned by our algorithm running on the residual problem with input graph $G_{\delta-1}$ (the resulting graph after contracting $T_{\delta}$ into $r$). Then $T = T_\delta \cup \bar{T}$ and 
$$f(T,x) = f(T_\delta, x) + f(\bar{T},x).$$
If we let $(\bar{T}^*, \bar{x}^*)$ denote the optimal solution for the residual problem with graph $G_{\delta - 1}$ then by our inductive hypothesis
$$f(T,x) \leq f(T_\delta,x) + O(1)f(\bar{T}^*, \bar{x}^*).$$
Now we observe that $T^*$ contains a Steiner Tree $T'$ with terminals $R_\delta \cup \{r\}$. Since $T_\delta$ is an $O(1)$-approximate optimal Steiner tree with terminals $R_\delta \cup \{r\}$ we have $c(T_\delta) \leq O(1) c(T')$. Since $T'$ necessarily installs capacities $2^\delta$ on all of its edges, $x_e^* = 2^\delta$ for all $e \in T'$. To see that a capacity of $2^\delta$ is necessary, suppose some edge $e$ in $T'$ has $x^*_e < 2^\delta$. Since $T'$ is a Steiner Tree, some terminal $v \in R_\delta$ uses $e$ on its unique path to $r$. But then an $rv$-flow along this path has value at most $x^*_e < 2^\delta$ contradicting feasibility of $(T^*, x^*)$. Thus $x_e^* = 2^\delta$ for all $e \in T'$. Hence we have
$$f(T_\delta, x) = \sum_{e \in T_\delta} c_e x_e = 2^\delta c(T_\delta) \leq O(1)2^\delta c(T') = O(1)f(T', x^*) \leq O(1) f(T^*, x^*).$$
Thus we see that
$$f(T,x) \leq O(1) f(T^*,x^*) + O(1) f(\bar{T}^*, \bar{x}^*).$$
Now consider the graph that results from contracting all edges of $T_\delta \cap T^*$ into $r$ in the tree $T^*$. If any cycles form then do the following: while there still exists a cycle $C$ take the edge $e = \text{argmin}_{e\in C} x^*_e$ and remove it. Call the resulting graph $S^*$. 
\paragraph{}
We claim that $(S^*, x^*)$ is feasible for the residual problem on $G_{\delta-1}$. Indeed $S^*$ is a tree since we trimmed one edge from all cycles. Since trimming cycles preserves connectivity, and we contracted edges into $r$, all $rv$-paths in $T^*$ are preserved in $S^*$. Hence for all $v \in R\backslash R_\delta$ there exists an $rv$-path in $S^*$. Now we claim that the capacities on any $rv$-path still admit feasible flows after converting from $T^*$ to $S^*$. Observe that contraction will not decrease the minimum capacity. Further observe that deleted edges had minimum capacity on a cycle, so any flow that used a deleted edge $uv$ could send the same flow through the $uv$-path remaining from the cycle $uv$ was deleted from. Hence the flows on $S^*$ under capacities $x^*$ can still meet all demands in $R\backslash R_\delta$. Therefore $(S^*,x^*)$ (with $x^*$ restricted to $E(G_{\delta-1})$) is feasible for $G_{\delta -1}$. Hence, since $(\bar{T}^*, \bar{x}^*)$ is optimal for $G_{\delta -1}$:
$$f(\bar{T}^*, \bar{x}^*) \leq f(S^*, x^*) \leq f(T^*, x^*).$$
Therefore
\begin{align*}
f(T,x) &\leq O(1)f(T^*,x^*) + O(1)f(\bar{T}^*,\bar{x}^*) \\
&\leq O(1)f(T^*,x^*) + O(1)f(T^*,x^*) \\
&= 2O(1)f(T^*,x^*)\\
&= O(1)f(T^*,x^*).
\end{align*}
So the approximation factor $O(1)$ holds in the inductive case. Thus the algorithm given is a $O(1)$-approximation algorithm.$\blacksquare$

%Q5
\section{}
\paragraph{}
Let $G=(V,E)$ be a metric graph with edge lengths $d: E \rightarrow \N_+$ and let $f : 2^V \rightarrow \N_+$  be a a function satisfying:
\begin{enumerate}
\item (Symmetric) $f(S) = f(V\backslash S)$ for all $S \subseteq V$
\item (Subadditive) $f(A \cup B) = f(A) + f(B)$ for all $A,B \subseteq V$.
\end{enumerate}
Consider the following integer program, to be labelled $IP$:
\begin{align*}
\text{min} \sum_{e \in E} d(e) x_e\\
\text{s.t.} \sum_{e\in \delta(S)} x_e &\geq f(S) &\forall S \subseteq V \\
x &\geq 0, \text{ integer}.
\end{align*}
\paragraph{Solving $IP$ of Trees}
We first observe that this $IP$ is easy to solve on trees. Indeed let $T$ be a tree, and for every $uv \in E(T)$ let $S^{uv}_u$ denote the set of vertices in connected component of $T-uv$ containing $u$, and define $S^{uv}_v$ similarly. By definition, $\delta(S^{uv}_u) =  \{e\} = \delta(S^{uv}_v)$. We define the following algorithm $\cA$ to solve $IP$ on a tree $T$:
\begin{enumerate}
\item For all $uv \in E(T)$ set $x_e = f(S^{uv}_u)$.
\item Return $x$.
\end{enumerate}
To see that $\cA$ solves $IP$ on trees, we will show that $(1)$ the solution returned is feasible, and $(2)$ that every feasible $x$ has necessarily $x_{uv} \geq f(S^{uv}_u)$, implying that the $x$ returned by $\cA$ is optimal.  Let $x$ be the solution returned by $\cA$. 
\paragraph{}
To see $(1)$, it is immediate from the definition of $f$ that $x \geq 0$. It remains to verify the cut inequalities. Let $S \subseteq V$. Enumerate $\delta(S)$ as $\delta(S) = \{e_1, \dots, e_k \}$, where each $e_i := u_iv_i$ and $u_i \in S$. Let $S_1, \dots, S_\ell$ be the partition of $S$ induced by the connected components of $G-\delta(S)$ contained in $S$.  Now let $D(S_i) = \{ e_j \in \delta(S) : u_j \in S_i \}$ be the set of edges in $\delta(S)$ incident with $S_i$. Notice that the set of $D(S_i)$ partition $\delta(S)$. 
\paragraph{}
We claim that $\bigcap_{e_j \in D(S_i)} S^{e_j}_{u_j} = S_i.$ If $v \in S_i$ then for all $e_j \in D(S_i)$ there exists a $vu_j$-path that does not use $e_j$, since $v$ and $u_j$ are in the same connected component $S_i$. Thus $v \in \bigcap_{e_j \in D(S_i)}S^{e_j}_{u_j}$. If $v \not\in S_i$ then either $v \in S_{k}$ for $k \neq i$ or $v \in V\backslash S$. In either case, $v$ is connected to any vertex in $S_i$ via a path using some $e_j \in D(S_i)$. But then $v \in S^{e_j}_{v_j} = V \backslash S^{e_j}_{u_j}$. Therefore $v \not\in \bigcap_{e_j \in D(S_i)}S^{e_j}_{u_j}$. Thus the claim holds.
\paragraph{}
So we have $$S = \bigcup_{i=1}^\ell S_i = \bigcup_{i=1}^\ell \bigcap_{e_j \in D(S_i)}S^{e_j}_{u_j}.$$ Now observe that
\begin{align*}
f(S) &\leq \sum_{i=1}^\ell f(\bigcap_{e_j \in D(S_i)}S^{e_j}_{u_j}) &\text{by subadditivity} \\
&= \sum_{i=1}^\ell f(V\backslash \bigcap_{e_j \in D(S_i)} S^{e_j}_{u_j}) &\text{by symmetry} \\
&= \sum_{i=1}^\ell f(\bigcup_{e_j\in D(S_i)} V\backslash S^{e_j}_{u_j}). \\
&\leq \sum_{i=1}^\ell \sum_{e_j \in D(S_i)} f(V\backslash S^{e_j}_{u_j}) &\text{by subadditivity} \\ t
&= \sum_{i=1}^\ell \sum_{e_j \in D(S_i)} f(S^{e_j}_{u_j}) &\text{by symmetry} \\
&= \sum_{e_j \in \delta(S)} f(S^{e_j}_{u_j}) \\
&= \sum_{e_j \i \delta(S)} x_{e_j}.
\end{align*}
Hence $x$ satisfies the cut constraint for $S$. Therefore $x$ is feasible and $(1)$ holds.
\paragraph{}
Now to demonstrate $(2)$, let $uv \in E(T)$. Consider the cut $S^{uv}_u$. We have $\delta(S^{uv}_u) = \{uv\}$ and hence by the cut constraint for $S^{uv}_u$, all feasible $x$ satisfy
$$x_{uv} \geq f(S^{uv}_u).$$
Now the $x$ we construct is tight for all the above, and since $d \geq 0$ we can do no better without violating feasibility. Thus $(2)$ holds and our $x$ is optimal. So we conclude that we can solve $IP$ on trees easily. Further this proof shows that $x$ is optimal for the linear programming relaxation of $IP$ on $T$, and since $x$ is integral this implies the equivalence of the linear programming relaxation and the integer program itself on $T$.
\paragraph{Integrality Gap for $IP$}
We now give an algorithm that produces an integral solution to $IP$ for a general metric graph $G$ which approximates the optimal $LP$-relaxation of $IP$ within a factor of $O(\log n)$. Our algorithm will use the following theorem from class:
\begin{theorem} \label{th:metric-tree}
Let $G=(V,E)$ be a graph. Let $d: E \rightarrow \R_+$ be a metric function. Let $\lambda^*_{uv} \geq 0$ be coefficients for all $(u,v) \in V\times V$. There exists a polynomial time algorithm which constructs a weighted tree $T$ on $V$ such that $d^T(u,v) \geq d(u,v)$ for all $u,v\in V$ and $$\sum_{u,v} \lambda^*_{uv}d^T(u,v) \leq O(\log n) \sum_{u.v} \lambda^*_{uv} d(u,v).$$
\end{theorem}
Consider the algorithm $\cB$:
\begin{enumerate}
\item Let $x^*$ be the optimal solution to the linear programming relaxation.
\item Use Theorem \ref{th:metric-tree} with $\lambda^* = x^*$ to obtain a weighted tree $T$.
\item Solve $IP$ (equivalently its $LP$ relaxation) on $T$ as discussed above and return the solution $\bar{x}$.
\end{enumerate}
\paragraph{}
We claim that the solution $\bar{x}$ returned by $\cB$ is feasible for $IP$. To see this, observe that completing $T$ to the graph $G$ does not remove any edges across any cut set $S$, hence the bound $\sum_{e \in \delta(S)} \bar{x}_e \geq f(S)$ still holds, since additional edges contribute non-negative amounts to $\sum_{e \in \delta(S)}$. This implies that $\bar{x}$ is feasible for $IP$ on $G$.
\paragraph{}
Now we analyze the cost. We see that
$$\sum_{u,v \in V} d(u,v) \bar{x}_{uv} \leq \sum_{u,v \in V} d^T(u,v) \bar{x}_{uv}$$
from Theorem \ref{th:metric-tree}. Now we convert $x^*$, the optimal solution for the linear programming relaxation of $IP$ on $G$, to a feasible solution on $T$. We will call such a solution $x'$. Our transformation will  preserve that $\sum_{u,v \in V} d^T(u,v) x^*_{uv} = \sum_{u,v \in V} d^T(u,v) x'_{uv}$. 
\begin{enumerate}
\item Begin with $x'_{uv} = x^*_{uv}$ for all $uv \in E(T)$.
\item Now for all $u,v \in V$ such that $uv \not\in E(T)$ do:
\begin{enumerate}
\item Let $P$ be the $uv$-path in $T$.
\item For each $e \in P$ add $x^*_{uv}$ to the current value of $x'_e$.
\end{enumerate}
\end{enumerate}
First we show that $x'$ is feasible on $T$. Let $\delta_G(S)$ denote $\delta(S)$ with respect to $G$ and similarly define $\delta_T(S)$ as $\delta(S)$ with respect to $T$. Let $P_{uv}$ denote the $uv$-path in $T$. Then for any cut set $S$ we have
$$f(S) \leq \sum_{e \in \delta_G(S)} x^*_e = \sum_{e\in \delta_T(S)} x^*_e + \sum_{e \in \delta_G(S)\backslash \delta_T(S)} x^*_e \leq \sum_{e \in \delta_T(S)} x'_e.$$
The second inequality follows from the observation that for every edge $e \in \delta_G(S) \backslash \delta_T(S)$ there exists some $e' \in P_{e} \cap \delta_T(S)$ and $$x'_{e'} = x^*_e + \sum_{uv \in E(G) \backslash E(T) : e' \in P_{uv}} x^*_{uv}.$$
The fact that $\sum_{u,v \in V} d^T(u,v) x^*_{uv} = \sum_{u,v \in V} d^T(u,v) x'_{uv}$ is immediate from the definitions:
\begin{align*}
\sum_{u,v \in V} d^T(u,v) x^*_{uv} &= \sum_{uv \in E(T)} d^T(u,v)x^*_{uv} + \sum_{e \in E(G) \backslash E(T)} \sum_{uv \in P_{e}} d^T(u,v) x^*_{e}\\
&= \sum_{uv \in E(T)} d^T(u,v)( x^*_{uv} + \sum_{e \in E(G)\backslash E(T): uv \in P_{e}} x^*{e}) \\
&= \sum_{uv \in E(T)} d^T(u,v) x'_{uv} \\
&= \sum_{u,v \in V} d^T(u,v) x'_{uv}.
\end{align*}
More intuitively, the distance between two vertices $u,v$ in $T$ is the sum of the distances on the edges in the path that joins them. Thus if we transfer the allocation $x^*_{u,v}$ to each edge in $P_{uv}$ we lose nothing in the objection function as $d^T(u,v) x^*_{u,v} = \sum_{ab \in P_{uv}} d^T(a,b) x^*_{uv}$.
\paragraph{}
Now we have that $x'$ is feasible on $T$, so by the optimality of $\bar{x}$ on $T$ we have
$$\sum_{u,v \in V} d^T(u,v) \bar{x}_{uv} \leq \sum_{u,v \in V} d^T(u,v) x'_{uv} = \sum_{u,v \in V} d^T(u,v) x^*_{uv}.$$
Thus combining inequalities we see that
$$\sum_{u,v \in V} d(u,v) \bar{x}_{uv} \leq \sum_{u,v \in V} d^T(u,v) x^*_{uv}.$$
Now by Theorem \ref{th:metric-tree} and our choice of $\lambda^* =x^*$ we see that 
$$\sum_{u,v \in V} d^T(u,v) x^*_{uv} \leq O(\log n) \sum_{u,v \in V} d(u,v) x^*_{uv}$$
and hence
$$\sum_{u,v \in V} d(u,v) \bar{x}_{uv} \leq O(\log n) \sum_{u,v \in V} d(u,v) x^*_{uv}.$$
Now since $\bar{x}$ is feasible for the $IP$ on $G$, this inequality tell us that the ratio of the optimal value of $IP$ to the the optimal value of its linear programming relaxation is bounded above by $O(\log n)$. That is, the integrality gap of $IP$ is $O(\log n)$. $\blacksquare$

%Q^6
\section{}
%Pa
\subsection{a}
\paragraph{}
We want to show the following theorem:
\begin{theorem}\label{th:tree-metric}
Let $G = (V,E)$ be a metric graph with edge lengths $d:E\rightarrow \R_+$ and $W \subset V$. One can randomly find a tree $T$ such that
\begin{enumerate}
\item $d(u,v) \leq d^T(u,v)$ for all $u,v\in W$
\item $E[d^T(u,v)] \leq O(\log n) d(u,v)$ for all $u,v\in V$.
\end{enumerate}
\end{theorem}
\begin{proof}
We may assume $\max_{u,v \in V} d(u,v) = 2^\delta$ for some $\delta \in \N$ and $d(u,v) > 1$ for all $u,v\in V$. Construct a tree $T$ by:
\begin{enumerate}
\item Choose randomly a permutation $\pi$ of $W$.
\item Choose $\beta \in [0,1]$ uniformly at random.
\item Set $D_\delta = \{V\}$.
\item For $i = \delta -1$ to $0$ do:
\begin{enumerate}
\item Assign every node to the first node according to $\pi$ that has distance at most $2^\beta \cdot 2^{i-1}$.
\item For each node such that its distance from each vertex in $W$ is greater that $2^\beta \cdot 2^{i-1}$ assign it its own cluster.
\item All nodes assigned to the same node that are currently in the same cluster will form a new cluster in $D_i$.
\item Add a node to $T$ for each new cluster in $D_i$ and add edges to $T$ from each such node to the parent node in $T$ corresponding to their common cluster in $D_{i+1}$.
\item For the clusters formed in Step $(b)$ assign distance $0$ to the new edge in $T$. For all others (ie those from $(a)$) assign distance $2^{i+1}$.
\end{enumerate}
\item Return tree $T$.
\end{enumerate}
Note that the clusters of $D_0$ are singletons because the bound on the maximum distance is $2^\beta 2^0 \leq 1$. Hence all nodes of $V$ are invariably separated at some level by the end of operation of the algorithm.
\paragraph{Proof of 1.}
Suppose $u,v \in W$ are in the same cluster at level $D_i$ and are separated at level $D_{i-1}$. Let $w$ be the node identified with $u$ and $v$'s cluster $S_w$ at level $D_i$. Then
$$d(u,v) \leq d(u,w) + d(w,v) \leq 2^\beta 2^{i-1} + 2^\beta 2^{i-1} \leq 2^{i+1}.$$ The first inequality follows from the triangle inequality. The second inequality follows since the distance from $u$ (similarly $v$) to the next node on the $uw$-path is $2^{i-1}$ and the distance of the rest of the path is at most $\sum_{k=0}^{i-2} 2^k \leq 2^{i-1}$. On the other hand $d^T(u, S_w) \geq 2^i$ and $d^T(S_w, v) \geq 2^i$ so $d^T(u,v) = d^T(u, S_w) + d^T(S_w, v) \geq 2^i + 2^i = 2^{i+1}$. This proves part $1.$ of the theorem.
\paragraph{Proof of 2.}
Observe that if $u,v$ are separated first at level $D_i$ then $d^T(u,v) \leq 2^{i+2}$. Let $S_i$ be the cluster in $D_i$ containing $u,v$. Let $S^u_{i-1}$ be descended cluster of $S_i$ from which $u$ descends and similarly define $S^v_{i-1}$. Then 
\begin{align*}
d^T(u,v) &= d^T(S_i, S^u_{i-1}) + d^T(S_, S^u_{i-1}) + d^T(S^u_{i-1}, u) + d^T(S^v_{i-1}, v)\\
&\leq \max\{2^i,0\} + \max\{2^i,0\} +  d^T(S^u_{i-1}, u) + d^T(S^v_{i-1}, v) \\
&\leq 2^i + 2^i + 2\sum_{j = 1}^{i-1} 2^j \\
&= 2^{i+1} + 2^{i+1} \\
&= 2^{i+2}.
\end{align*}
Now we say that node $c$ {\it settles} $(u,v)$ at level $D_i$ if $c$ is the first node with respect to $\pi$ to which one of $u$ or $v$ gets assigned at level $D_i$. We say that node $c$ {\it cuts} $(u,v)$ at level $D_i$ if exactly one of $u,v$ is at distance at most $2^\beta 2^i$ from $c$. If $u,v$ are separated at level $D_i$ then either there exists some $c \in W$ which both settles and cuts $u,v$ at level $D_i$ (in which case they were separated by Step $4(a)$), or every $c \in W$ does not settle $u,v$ (in which case they were separated by Step $4(b)$).
\paragraph{}
Note that in the first case the distance between $u$ and $v$ is at most $2^{i+2}$ by our previous observation. In the second case both $u$ and $v$ were assigned their own cluster in $D_i$ and given distance $0$ to their common ancestor. Hence $d^T(u,v) = 0$ in this second case. So we have the following bound on $E[d^T(u,v)]$:
 \begin{align*}E[d^T(u,v)] &\leq \sum_{i=0}^{\delta - 1} (\sum_{c \in W} Pr [ \text{$c$ settles $u,v$ and  $c$ cuts $u,v$ at $D_i$}] 2^{i+2} + Pr[\text{all $c \in W$ do not settle $u,v$}]\cdot 0) \\&= \sum_{i=0}^{\delta - 1} \sum_{c \in W} Pr [ \text{$c$ settles $u,v$ and  $c$ cuts $u,v$ at $D_i$}] 2^{i+2}.
 \end{align*}
 For $c \in W$, $u,v \in V$, and level $D_i$,  let $A$ be the event that $c$ settles $u,v$ and let $B$ be the event that $c$ cuts $u,v$ at $D_i$. We are interested in $Pr[A \cap B]$, or equivalently, $Pr[A \mid B] \cdot Pr[B]$.
 \paragraph{}
We first analyze $Pr[B]$. Without loss we may assume $d(c,u) \leq d(c,v)$. For $c$ to cut $u,v$ we need $d(c,u) \leq 2^\beta 2^{i-1} < d(c,v)$ by definition.  Since we sample $2^\beta$ uniformly at random, the probability of this happening is simply $\frac{1}{2^{i-1}}$ times the length of the interval intersecting $[2^{i-1},2^i]$ with $[d(c,u), d(c,v)]$. More formally:
$$Pr[B] = \frac{| [2^{i-1},2^i] \cap [d(c,u),d(c,v)]|}{|[2^{i-1},2^i]|} = \frac{| [2^{i-1},2^i] \cap [d(c,u),d(c,v)]|}{2^{i-1}}.$$
\paragraph{}
Now we need $Pr[A \mid B]$. Let us order the nodes of $W$ according to their distance to $u,v$. That is, $W = \{c_1, \dots, c_n\}$ where $\min\{d(c_i, v), d(c_i, u)\} \leq \min \{d(c_{i+1}, v), d(c_{i+1}, u)\}$ for all $i$. If $c= c_k$ then the probability that $c$ comes before $c_1, \dots, c_{k-1}$ in $\pi$ is at most $\frac{1}{k}$. In order for $c$ (given that $c$ cuts $u,v$) to settle $u,v$ at least that has to happen. Hence
$$Pr[A \mid B] \leq \frac{1}{k}.$$
\paragraph{}
Putting it all together we obtain the following expectation:
\begin{align*}
E[d^T(u,v)] &\leq \sum_{i=0}^{\delta -1} \sum_{k=1}^n Pr [A \mid B]Pr[B] 2^{i+2} \\
&\leq \sum_{k=1}^n \frac{1}{k} \sum_{i=0}^{\delta -1 }  2^{i+2}\frac{| [2^{i-1},2^i] \cap [d(c,u),d(c,v)]|}{2^{i-1}} \\
&= 8 \sum_{k=1}^n \frac{1}{k} \sum_{i=0}^{\delta -1 }  | [2^{i-1},2^i] \cap [d(c,u),d(c,v)]| \\
&\leq 8 d(u,v) \sum_{k=1}^n \frac{1}{k} \\
&= 8 H_n d(u,v) \\
&= O(log n) d(u,v).
\end{align*}
Thus the second claim and therefore the theorem holds.
\end{proof}

%Pb
\subsection{b}
\paragraph{}
Let $G = (V,E)$ be a metric graph with edge costs $c:E \rightarrow \R_+$ and requirements $r(u,v) \in \N$ for all $u,v \in V$. We wish to operate on a tree with the same vertex set at $G$. The following theorem will let us do so.
\begin{theorem}
Let $T$ be a tree obtained by Theorem \ref{th:tree-metric}. Then there exists a tree $T' = (V,F)$ such that $d^T(u,v) \leq d^{T'}(u,v) \leq 4 d^T(u,v).$
\end{theorem}
\begin{proof}
Pick $v \in V$ with a parent $w \not \in V$ and contract edge $vw$ in $T$. Repeat until all nodes of $T$ are in $V$. Multiply by $4$ the weights on all remaining edges. Call the resulting tree $T'$. Note that clearly $d^{T'}(u,v) \leq 4d^T(u,v)$ as contracting only shortens the distances. Furthermore, in the worst case $d^{T'}(u,v) = 2^i \cdot 4$ if contraction results in contracting $u$ and $v$ right up to the cluster where they were separated (that is, contracting preserves the largest edge distance on the $uv$-path in $T$). But 
$$2^i \cdot 4 = 2^i + 2^i + 2^i + 2^i \geq d(S^u_{i-1}, S_i) + d(S^v_{i-1}, S_i) +  d(S^u_{i-1}, u) + d(S^v_{i-1}, v) = d^T(u,v).$$
Hence the theorem holds.
\end{proof}
\paragraph{}
Combining the above with Theorem \ref{th:tree-metric} and hiding the factor of $4$ in the $O$-notation we have the important corollary
\begin{corollary}\label{cor:tree-metric}
Let $G = (V,E)$ be a metric graph with edge lengths $d:E\rightarrow \R_+$ and $W \subset V$. One can randomly find a tree $T$ with vertex set $V$ such that
\begin{enumerate}
\item $d(u,v) \leq d^T(u,v)$ for all $u,v\in W$
\item $E[d^T(u,v)] \leq O(\log n) d(u,v)$ for all $u,v\in V$.
\end{enumerate}
\end{corollary}
\paragraph{}
Consider the following algorithm, $\cA$, for GSN on $G$:
\begin{enumerate}
\item Let $W = \{v \in V: \exists u \in V, r(u,v) > 0\}$.
\item Use Corollary \ref{cor:tree-metric} to compute a tree metric $T$ which approximates $G$ with $W$.
\item Solve GSN on $T$ and let $x$ be the resulting solution.
\item Return $x$.
\end{enumerate}
\paragraph{}
We want to show that $\cA$ is an $O(log |W|)$-approximation algorithm for GSN on $G$. Thus we need to show a few things:
\begin{enumerate}
\item That we can solve GSN on trees in polynomial time. This implies that $\cA$ runs in polynomial time since the tree of Theorem \ref{th:tree-metric} can be found in polynomial time, and the contractions necessary to yield the tree in Corollary \ref{cor:tree-metric} can be done in polynomial time. 
\item The $x$ returned is feasible on $G$.
\item The approximation factor holds.
\end{enumerate}
\paragraph{Proof of $1$}

\paragraph{Proof of $2$}

\paragraph{Proof of $3$}

\end{document}
