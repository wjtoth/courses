\documentclass[letterpaper,12pt,oneside,onecolumn]{article}
\usepackage[margin=1in, bottom=1in, top=1in]{geometry} %1 inch margins
\usepackage{amsmath, amssymb, amstext}
\usepackage{fancyhdr}
\usepackage{mathtools}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{theorem}
\usepackage{tikz}
\usepackage{tkz-berge}

%Macros
\newcommand{\A}{\mathbb{A}} \newcommand{\C}{\mathbb{C}}
\newcommand{\D}{\mathbb{D}} \newcommand{\F}{\mathbb{F}}
\newcommand{\N}{\mathbb{N}} \newcommand{\R}{\mathbb{R}}
\newcommand{\T}{\mathbb{T}} \newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
 
 
\newcommand{\cA}{\mathcal{A}} \newcommand{\cB}{\mathcal{B}}
\newcommand{\cC}{\mathcal{C}} \newcommand{\cD}{\mathcal{D}}
\newcommand{\cE}{\mathcal{E}} \newcommand{\cF}{\mathcal{F}}
\newcommand{\cG}{\mathcal{G}} \newcommand{\cH}{\mathcal{H}}
\newcommand{\cI}{\mathcal{I}} \newcommand{\cJ}{\mathcal{J}}
\newcommand{\cK}{\mathcal{K}} \newcommand{\cL}{\mathcal{L}}
\newcommand{\cM}{\mathcal{M}} \newcommand{\cN}{\mathcal{N}}
\newcommand{\cO}{\mathcal{O}} \newcommand{\cP}{\mathcal{P}}
\newcommand{\cQ}{\mathcal{Q}} \newcommand{\cR}{\mathcal{R}}
\newcommand{\cS}{\mathcal{S}} \newcommand{\cT}{\mathcal{T}}
\newcommand{\cU}{\mathcal{U}} \newcommand{\cV}{\mathcal{V}}
\newcommand{\cW}{\mathcal{W}} \newcommand{\cX}{\mathcal{X}}
\newcommand{\cY}{\mathcal{Y}} \newcommand{\cZ}{\mathcal{Z}}

\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}


\newenvironment{proof}{{\bf Proof:  }}{\hfill\rule{2mm}{2mm}}
\newenvironment{proofof}[1]{{\bf Proof of #1:  }}{\hfill\rule{2mm}{2mm}}
\newenvironment{proofofnobox}[1]{{\bf#1:  }}{}\newenvironment{example}{{\bf Example:  }}{\hfill\rule{2mm}{2mm}}

%\renewcommand{\thesection}{\lecnum.\arabic{section}}
%\renewcommand{\theequation}{\thesection.\arabic{equation}}
%\renewcommand{\thefigure}{\thesection.\arabic{figure}}

\newtheorem{fact}{Fact}[section]
\newtheorem{lemma}[fact]{Lemma}
\newtheorem{theorem}[fact]{Theorem}
\newtheorem{definition}[fact]{Definition}
\newtheorem{corollary}[fact]{Corollary}
\newtheorem{proposition}[fact]{Proposition}
\newtheorem{claim}[fact]{Claim}
\newtheorem{exercise}[fact]{Exercise}
\newtheorem{note}[fact]{Note}
\newtheorem{conjecture}[fact]{Conjecture}

\newcommand{\size}[1]{\ensuremath{\left|#1\right|}}
\newcommand{\ceil}[1]{\ensuremath{\left\lceil#1\right\rceil}}
\newcommand{\floor}[1]{\ensuremath{\left\lfloor#1\right\rfloor}}

%END MACROS
%Page style
\pagestyle{fancy}

\listfiles

\raggedbottom

\lhead{2017-02-14}
\rhead{W. Justin Toth CO750-Approximation Algorithms Assignment 1} %CHANGE n to ASSIGNMENT NUMBER ijk TO COURSE CODE
\renewcommand{\headrulewidth}{1pt} %heading underlined
%\renewcommand{\baselinestretch}{1.2} % 1.2 line spacing for legibility (optional)

\begin{document}
%Q1
\section{}
\paragraph{}
Let $E$ be the edge set of a graph $G$. Let $\cS$ be the collection of all sets $S \subseteq E$ that are matchings of $G$. Let $w: E \rightarrow \R_+$ be a weight function on the edges. For a subset $\bar{E} \subseteq E$ we denote by $G[\bar{E}]$ the subgraph of $G$ induced by the edges in $\bar{E}$.
	%Pa
\subsection{a}
\paragraph{}
Let $\bar{E} \subseteq E$. Suppose that $M \subseteq \bar{E}$ is an inclusion-wise maximal matching of $G[\bar{E}]$. Let $\bar{M}$ be a matching of $G[\bar{E}]$. Let $f : M \rightarrow \{ \{e_1,e_2\} | e_1, e_2 \in \bar{M}\}$ be defined by
$$ f(uv) = \{e_1, e_2\}$$
where $e_1 \in \bar{M}$ covers $u$ and $e_2 \in \bar{M}$ covers $v$, for each $uv \in M$.
\paragraph{}
We claim that for all $uv \in \bar{M}$, there exists $P \in Im(f)$ (where $Im(f)$ denotes the image of $f$) such that $uv \in P$. To see this suppose for a contradiction that there exists some $uv \in \bar{M}$ where each $P \in Im(f)$ does not contain $uv$. Then by the definition of $f$, neither $u$ nor $v$ are covered by $M$. But this violates that $M$ is inclusion-wise maximal since $uv \not\in M$ and $M \cup \{uv\}$ is a matching. Hence the claim holds.
\paragraph{}
From the previous claim we observe $|\bar{M}| \leq | \cup_{P \in Im(f)} P|$. But this yields a string of inequalities which complete the proof:
\begin{align*}
|\bar{M}| &\leq |\cup_{P \in Im(f)} P| \\
&\leq \sum_{P \in Im(f)} |P| \\
&\leq \sum_{P \in Im(f)} 2 \\
&\leq 2|Im(f)| \\
&\leq 2|M|. 
\end{align*}
With the last inequality following since $M$ is the domain of $f$. Hence we have $|M| \geq \frac{1}{2}|\bar{M}|$ as desired. $\blacksquare$
	%Pb
\subsection{b}
(This argument follows the exact same lines as the argument for problem $2$)
\paragraph{}
We can decided in polynomial time if $M\cup \{e\}$ is a matching in $\cS$ by iterating over $V$, counting the number of edges in $M\cup\{e\}$ which cover each vertex in $V$, and returning ``No" is the count is greater than $1$ for any vertex and ``Yes" otherwise. Thus the algorithm clearly runs in polynomial time and returns a feasible set.
\paragraph{}
Let $S$ be the set of edges returned by the greedy algorithm. Order the edges as $S = \{e_1, \dots, e_n\}$ in the order they were taken during the operation of $\cA$. Let $F_i := E\backslash \tilde{E}$ be the edges removed from $E$ at the end of iteration $i$ of $\cA$ (the iteration where $e_i$ is added to $S$). Let $S^*$ be the optimal solution and partition $S^* = S^*_1 \cup \dots \cup S^*_n$ where $S^*_i = S^* \cap F_i$.
\paragraph{}
We want that $|S^*_i| \leq 2$ to hold, but we may need to redistribute some edges to ensure that this happens (and we will do this without effecting our ability to bound the optimal solution). Run the following algorithm, denoted $\cB$, to redistribute the edges in our partition of $S^*$:
\begin{enumerate}
\item For $i = 1, \dots, n$
\begin{enumerate}
\item While $|S^*_i| > 2$: take an edge $e$ from $S^*_i$ and redistribute $e$ to some $S^*_j$ for $j<i$ with $|S^*_j| < 2$.
\end{enumerate}
\end{enumerate}
\paragraph{Claim 1}
After the operation of $\cB$, for all $i = 1, \dots, n$, $|S^*_i| \leq 2$ and $w_{e_i} \geq w_e$ for all $e \in S^*_i$.
\paragraph{Proof of Claim 1}
We proceed by induction. For the base case $e_1$, $\{e_1\}$ is inclusion-wise maximal on $\cS[F_1]$ (by definition of $F_1$), and $S_1^*$ is feasible on $\cS[F_1]$ since $S^*_1 \subseteq F_1$ and $S^* \in \cS$. Thus we have $|\{e_1\}| \geq \frac{1}{2} |S_1^*|$, and hence
$$|S^*_1| \leq 2.$$
Further by our greedy choice of $e_1$, $w_{e_1} \geq w_e$ for all $e \in F_1$ (and hence for all $e \in S^*_1$).
\paragraph{}
Suppose that the claim holds for $S^*_1 ,\dots, S^*_i$. Now by our greedy choice, and induction, $w_{e_{i+1}} \geq w_{e_j} \geq w_e$ for all $e \in S^*_j$ for all $j = 1,\dots, i$. So when $\cB$ redistributes edges of $S^*_{i+1}$ the cost property on the earlier sets is not violated. Also by our greedy choice, any edges that were not moved from $S^*_{i+1}$ by $\cB$ have cost at most $w_{e_{i+1}}$. To finish the proof of the claim, we demonstrate that while $|S_{i+1}^*| > 2$ there exists some $S^*_j$ with $j\leq i$ and $|S^*_j| < 2$. Observe that $\{e_1, \dots, e_{i+1}\}$ is inclusion-wise maximal on $\cS[F_1 \cup \dots \cup F_{i+1}]$ by definition of $F_1 \cup \dots \cup F_{i+1}$. Also $\bigcup_{j=1}^{i+1} S^*_j \in \cS[F_1 \cup \dots \cup F_{i+1}]$ so we have that
$$|\{e_1, \dots, e_{i+1}\}| \geq \frac{1}{2} |\bigcup_{j=1}^{i+1} S^*_j | = \frac{1}{2} \sum_{j=1}^{i+1} |S^*_j|.$$
Thus we see that
$$ \frac{1}{i+1}\sum_{j=1}^{i+1} |S^*_j| \leq 2.$$
Now since the desired bound holds on average, if $|S^*_{i+1}| > 2$ there exists some $S^*_j$ with $j \leq i$ and $|S^*_j| < 2$ (otherwise the bound does not hold). Thus we have proven Claim $1$. $\blacksquare$
\paragraph{}
With Claim $1$ in hand we are prepared to compute the desired approximation factor:
\begin{align*}
w(S^*) &= \sum_{i=1}^n c(S^*_i) \\
&\leq \sum_{i=1}^n w_{e_i} |S^*_i| \\
&\leq \sum_{i=1}^n w_{e_i} 2 \\
&= 2 w(S).
\end{align*}
Thus $w(S) \geq \frac{1}{2} w(S^*)$ and hence the approximation factor holds as desired. $\blacksquare$
	%Pc
	\subsection{c}
	\paragraph{}
	Suppose for a contradiction that the Greedy Algorithm is in fact a $(\frac{1}{2} + \epsilon)$-approximation algorithm for some $\epsilon \in (0, \frac{1}{2}]$. Then consider the following example. Let $G$ be the path graph with vertex set $V(G) = \{a,b,c,d\}$ and edge set $E(G) = \{ab, bc,cd\}$. Define the weight function $w : E(G) \rightarrow \R_+$ by
	$$w_{ab} = w_{cd} = 1 \quad\text{and}\quad w_{bc} = 1+\epsilon.$$
	Then the Greedy algorithm would choose the edge $bc$ as its solution. This solution has cost $1+\epsilon$. The optimal solution is to take edges $ab$ and $cd$ with cost $2$. The ratio of these solutions is
	$$ \frac{1+\epsilon}{2} = \frac{1}{2} + \frac{\epsilon}{2} < \frac{1}{2} + \epsilon$$
	Contradicting that the Greedy Algorithm is a $(\frac{1}{2} + \epsilon)$-approximation algorithm. $\blacksquare$
%Q2
\section{}
\paragraph{}
Let $E$ be a set of $m$ elements and $\cS$ a collection of sets $S \subseteq E$. Let $w: E\rightarrow \R_+$ be a weight function. Let $\cS[\bar{E}] := \{ S \in \cS : S \subseteq \bar{E}\}$. Suppose that $\cS$ satisfies the following for a given $\alpha \geq 1$:
\begin{enumerate}
\item $S \in \cS$ implies $S' \in \cS$ for any $S' \subseteq S$
\item For all $\bar{E} \subseteq E$, if $S$ is an inclusion-wise maximal set in $\cS[\bar{E}]$ then $|S| \geq \frac{1}{\alpha} |\bar{S}|$ for all $\bar{S} \in \cS[\bar{E}]$.
\end{enumerate}
We will show the Greedy Algorithm, denoted $\cA$, is a $\frac{1}{\alpha}$-approximation algorithm for the problem of finding a maximum weight $S \in \cS$.
\paragraph{}
Since we have a polynomial time oracle for membership in $\cS$ the algorithm clearly runs in polynomial time and returns a feasible set.
\paragraph{}
Let $S$ be the set of edges returned by the greedy algorithm. Order the edges as $S = \{e_1, \dots, e_n\}$ in the order they were taken during the operation of $\cA$. Let $F_i := E\backslash \tilde{E}$ be the edges removed from $E$ at the end of iteration $i$ of $\cA$ (the iteration where $e_i$ is added to $S$). Let $S^*$ be the optimal solution and partition $S^* = S^*_1 \cup \dots \cup S^*_n$ where $S^*_i = S^* \cap F_i$.
\paragraph{}
We want that $|S^*_i| \leq \alpha$ to hold, but we may need to redistribute some edges to ensure that this happens (and we will do this without effecting our ability to bound the optimal solution). Run the following algorithm, denoted $\cB$, to redistribute the edges in our partition of $S^*$:
\begin{enumerate}
\item For $i = 1, \dots, n$
\begin{enumerate}
\item While $|S^*_i| > \alpha$: take an edge $e$ from $S^*_i$ and redistribute $e$ to some $S^*_j$ for $j<i$ with $|S^*_j| < \alpha$.
\end{enumerate}
\end{enumerate}
\paragraph{Claim 1}
After the operation of $\cB$, for all $i = 1, \dots, n$, $|S^*_i| \leq \alpha$ and $w_{e_i} \geq w_e$ for all $e \in S^*_i$.
\paragraph{Proof of Claim 1}
We proceed by induction. For the base case $e_1$, $\{e_1\}$ is inclusion-wise maximal on $\cS[F_1]$ (by definition of $F_1$), and $S_1^*$ is feasible on $\cS[F_1]$ since $S^*_1 \subseteq F_1$ and $S^* \in \cS$. Thus we have $|\{e_1\}| \geq \frac{1}{\alpha} |S_1^*|$, and hence
$$|S^*_1| \leq \alpha.$$
Further by our greedy choice of $e_1$, $w_{e_1} \geq w_e$ for all $e \in F_1$ (and hence for all $e \in S^*_1$).
\paragraph{}
Suppose that the claim holds for $S^*_1 ,\dots, S^*_i$. Now by our greedy choice, and induction, $w_{e_{i+1}} \geq w_{e_j} \geq w_e$ for all $e \in S^*_j$ for all $j = 1,\dots, i$. So when $\cB$ redistributes edges of $S^*_{i+1}$ the cost property on the earlier sets is not violated. Also by our greedy choice, any edges that were not moved from $S^*_{i+1}$ by $\cB$ have cost at most $w_{e_{i+1}}$. To finish the proof of the claim, we demonstrate that while $|S_{i+1}^*| > \alpha$ there exists some $S^*_j$ with $j\leq i$ and $|S^*_j| < \alpha$. Observe that $\{e_1, \dots, e_{i+1}\}$ is inclusion-wise maximal on $\cS[F_1 \cup \dots \cup F_{i+1}]$ by definition of $F_1 \cup \dots \cup F_{i+1}$. Also $\bigcup_{j=1}^{i+1} S^*_j \in \cS[F_1 \cup \dots \cup F_{i+1}]$ so we have that
$$|\{e_1, \dots, e_{i+1}\}| \geq \frac{1}{\alpha} |\bigcup_{j=1}^{i+1} S^*_j | = \frac{1}{\alpha} \sum_{j=1}^{i+1} |S^*_j|.$$
Thus we see that
$$ \frac{1}{i+1}\sum_{j=1}^{i+1} |S^*_j| \leq \alpha.$$
Now since the desired bound holds on average, if $|S^*_{i+1}| > \alpha$ there exists some $S^*_j$ with $j \leq i$ and $|S^*_j| < \alpha$ (otherwise the bound does not hold). Thus we have proven Claim $1$. $\blacksquare$
\paragraph{}
With Claim $1$ in hand we are prepared to compute the desired approximation factor:
\begin{align*}
w(S^*) &= \sum_{i=1}^n c(S^*_i) \\
&\leq \sum_{i=1}^n w_{e_i} |S^*_i| \\
&\leq \sum_{i=1}^n w_{e_i} \alpha \\
&= \alpha w(S).
\end{align*}
Thus $w(S) \geq \frac{1}{\alpha} w(S^*)$ and hence the approximation factor holds as desired. $\blacksquare$
%Q3
\section{}
\paragraph{}
Let $G = (V,E)$ be a graph. For any $S, T \subseteq V$ we denote by $E(S,T)$ the set of edges between $S$ and $T$. Formally $E(S,T) = \{\{s,t\} \in E: s \in S, t \in T\}$.
%Pa
\subsection{a}
\paragraph{}
To see that the Greedy Algorithm runs in polynomial time observe that $|E(v,S)|$ can be computed in $O(|E|)$ time and the while loop runs for $O(|V|)$ iterations. Further the algorithm returns a feasible solution by definition. It remains to verify the approximation factor of $\frac{1}{2}$ holds.
\paragraph{}
We show that the approximation factor is maintained throughout operation of the algorithm on the graph $G[V\backslash \bar{V}]$ induced by vertices considered so far. After the first iteration only one vertex has been considered and the approximation factor holds trivially on the graph induced by that one vertex. Now for induction let $V' = V \backslash \bar{V}$ be the set of vertices considered so far by the Greedy Algorithm, and at the start of the next iteration the vertex $v \in V$ is being considered. Let $S \subseteq V$ be the cut set considered so far, and let $S^*$ be the optimal cut set on $G[V']$. Let $\bar{S} = V'\backslash S$ and $\bar{S^*} = V' \backslash S^*$. We may assume without loss of generality that the Greedy Algorithm puts $v \in S$ and the optimal solution puts $v \in S^*$ after this iteration (if this does not hold simply interchange $S$ with $\bar{S}$ below, or $S^*$ with $\bar{S^*}$ with respect to which assumption does not hold). 
\paragraph{}
So after this iteration the optimal solution has value
$$|E(S^* \cup \{v\}, \bar{S^*})| =  \sum_{u \in S^* \cup \{v\}} |E(u, \bar{S^*})| = |E(v,\bar{S^*})| + \sum_{u \in S^*} |E(u, \bar{S^*})| \leq |E(v,\bar{S^*})| + 2\sum_{u \in S} |E(u, \bar{S})|$$
with the inequality following by induction. Hence the approximation factor will hold as desired provided that
$$|E(v,\bar{S^*})| \leq 2|E(v,\bar{S})|.$$
Suppose for a contradiction that
$$|E(v,\bar{S^*})| > 2|E(v,\bar{S})|.$$
We observe that
$$|E(v, \bar{S^*})| = |E(v,\bar{S^*}\backslash S)| + |E(v, \bar{S^*}\cap S)| \leq  |E(v, \bar{S})| + |E(v, S)|.$$
The inequality follows since $\bar{S^*}\backslash S \subseteq V' \backslash S = \bar{S}$, and $\bar{S^*} \cap S \subseteq S$. Combining this inequality with the contradiction assumption we see
$$2|E(v,\bar{S})| <|E(v, \bar{S})| + |E(v, S)|$$
and subtracting $|E(v, \bar{S})|$ from both sides yields
$$ |E(v,\bar{S})| < |E(v,S)|.$$
This contradicts our Greedy choice $v \in S$ since such choice implies
$$ |E(v,\bar{S})| \geq |E(v,S)|.$$
Hence we have
$$|E(v,\bar{S^*})| \leq 2|E(v,\bar{S})|.$$
Thus after this iteration the optimal solution has value:
$$|E(S^* \cup \{v\}, \bar{S^*})|  \leq |E(v,\bar{S^*})| + 2\sum_{u \in S} |E(u, \bar{S})| \leq 2|E(v,\bar{S})| + 2\sum_{u \in S} |E(u, \bar{S})| = 2 |E(S \cup \{v\}, \bar{S})|.$$
Therefore $|E(S \cup \{v\}, \bar{S})| \geq \frac{1}{2} |E(S^* \cup \{v\}, \bar{S^*})|$ as desired. 
\paragraph{}
Now we observe that, from the invariant we just demonstrated, upon termination of the algorithm the approximation factor holds for the greedy solution versus the optimal solution on $G[V \backslash \emptyset] = G$, and hence the Greedy Algorithm is $\frac{1}{2}$-approximation algorithm.$\blacksquare$
%Pb
\subsection{b}
\paragraph{}
Our goal is to find $k$ cuts of $G$ $V_1, \dots, V_k \subseteq V$ maximizing:
$$|\bigcup_{i=1}^k \delta(V_i)|.$$
Let $\cB$ denote the $2$-approximation algorithm for max cut given in problem $3a$. Consider the following algorithm, which we will denote by $\cA$:
\begin{enumerate}
\item Set $V_1 = \dots = V_k = \emptyset$. Set $G_1 = G$
\item For $i = 1, \dots, k$
\begin{enumerate}
\item Let $S$ be the cut returned by $\cB$ run on $G_i$.
\item Set $V_i = S$. Set $G_{i+1} = G_i \backslash \delta(S).$
\end{enumerate}
\item Return $V_1, \dots, V_k$.
\end{enumerate}
The idea behind the operation of $\cA$ is to find an optimal cut, then remove the edges of the cut and iterate.
\begin{lemma}\label{lemma:3b1}
Let $\cO = \{V^*_1, \dots, V^*_k\}$ denote the optimal solution. For any $i=1,\dots, k$ let $\cS^i = \{V_1, \dots, V_{i}\}$ be the set of partitions chosen so far at the end of iteration $i$ by $\cA$. Then 
$$2k(c(\cS^i) - c(\cS^{i-1})) \geq c(\cO) - c(\cS^{i-1}).$$
With $S^0 = \emptyset$.
\end{lemma}
\begin{proof}
For $i = 1,\dots, k$ let $S_i^* = V_i^k \backslash \bigcup_{S\in \cS^{i-1}} S$. Then
$$c(\cO) - c(\cS^{i-1}) = c(\{S^*_1,\dots, S^*_l\}) = |\bigcup_{i=1}^k \delta(S^*_i)|.$$
Let $$S^* = \text{arg max}_{S^*_i : i=1,\dots, k} |\delta(S^*_i)|.$$
Then 
$$c(\cO) - c(\cS^{i-1}) \leq k|\delta(S^*)|.$$
Now observe that for all $i = 1,\dots, k$, $\delta(S^*_i) \subseteq E(G_i)$, hence in particular $\delta(S^*) \subseteq E(G_i)$. Thus $|\delta(S^*)|$ has size at most that of an optimal cut in $G_i$. Now by our choice of $V_i$, using $2$-approximation algorithm $\cB$, $2|\delta(V_i)|$ is at most the size of an optimal cut in $G_i$. Hence we have
$$|\delta(S^*)| \leq 2|\delta(V_i)|.$$
Thus, combining inequalities, we observe
$$c(\cO) - c(\cS^{i-1}) \leq 2k |\delta(V_i)|.$$
But $|\delta(V_i)| = c(\cS^i) - c(\cS^{i-1})$ and thus
$$c(\cO) - c(\cS^{i-1}) \leq 2k (c(\cS^i) - c(\cS^{i-1}))$$
as desired.
\end{proof}
\paragraph{}
Using the previous lemma, we can demonstrate that $\cA$ is a $\alpha := 1-(1-\frac{1}{2k})^k$-approximation algorithm for the max $k$-cut problem.
\paragraph{Main Proof}
Since $\cB$ can be run in polynomial time, and $k$ is at most $|V|$, it is clear that step $2$ of $\cA$, and hence $\cA$, runs in polynomial time. At termination the algorithm returns $k$ subsets of $V$, so the algorithm returns a feasible solution.
\paragraph{} 
Now to see the approximation factor holds let $\cS^i$ denote the solution maintained by $\cA$ at the end of iteration $i$. Then the returned solution is $\cS^{k}$. Let $\cO$ be an optimal solution. Observe from rewriting Lemma \ref{lemma:3b1} that $c(\cS^{i}) \geq \frac{1}{2k} c(\cO) + (1- \frac{1}{2k})c(S^{i-1})$. We will apply this inequality inductively to achieve the bound:
\begin{align*}
c(\cS^{k}) &\geq \frac{1}{2k} c(\cO) + (1- \frac{1}{2k})c(S^{k-1}) \\
		&\geq \frac{1}{2k} c(\cO) + (1- \frac{1}{2k})(\frac{1}{2k} c(\cO) + (1- \frac{1}{2k})c(S^{k-2})) \\
		&\geq \frac{1}{2k} c(\cO) + (1- \frac{1}{2k})(1 + (1-\frac{1}{2k}) + (1-\frac{1}{2k})^2 + \dots + (1-\frac{1}{2k})^{k-1}) \\
		&= \frac{c(\cO)}{2k} \cdot \frac{1-(1-\frac{1}{2k})^k}{1-(1-\frac{1}{2k})} \\
		&=\frac{c(\cO)}{2k} \cdot \frac{1-(1-\frac{1}{2k})^k}{\frac{1}{2k}}\\
		&= (1-(1-\frac{1}{2k})^k)\cdot c(\cO).
\end{align*}
Thus the approximation factor holds as desired. $\blacksquare$
%Q4
\section{}
\paragraph{}
Let $G = (V,E)$ be a graph with edge costs $c: E \rightarrow \R_+$. Let $r \in V$ be a root vertex. Let $R \subseteq V \backslash \{r\}$ be a set of terminals. Let $d_v \in \{2^0, 2^1, \dots, 2^\delta\}$ denote the demand of $v$ for each $v \in R$. We assume we have an $O(1)$-approximation algorithm for the Steiner Tree problem. Then each iteration of step $2$ of the given algorithm runs in polynomial time using the assumed approximation algorithm. There are $\delta+1$ iterations of step $2$ which is $O(\log \max_v d_v)$ and hence polynomial in the number of bits needed to specify the input to the problem. Thus the entirety of step $2$ runs in polynomial time and hence the given algorithm runs in polynomial time. It is clear the tree returned spans $R \cup \{r\}$ since the partial solution at each iteration $i$ spans $R_\delta \cup \dots \cup R_i \cup \{r\}$. Further at each iteration $i$ the paths in the final solution connecting $v \in R_i$ to $r$ are assigned costs $2^i$ on their edges and hence a feasible flow of value $d_v$ from $r$ to $v$ is possible. Thus the algorithm returns a feasible solution in polynomial time.
\paragraph{}
It remains to verify the approximation factor of $O(1)$.We will describe solutions by an ordered pair $(T, x)$ where $T$ is a tree and $x : T \rightarrow \R_+$ maps edges of $T$ with their corresponding capacity. Then the objective function, which we denote by $f$, in our problem maps feasible solutions $(T,x)$ as follows:
$$f(T,x) = \sum_{e \in T} c_e x_e.$$
\paragraph{}
Let $(T,x)$ be the solution returned by running our algorithm, and let $(T^*, x^*)$ be the optimal solution. Let $T_{i}$ be the $O(1)$-approximate Steiner Tree connecting $R_i \cup \{r\}$ in iteration $i$ of our algorithm. Similarly let $T^*_i$ denote the sub-tree of $T^*$ which spans $\{r\} \cup R_i$ after contracting $T^*_{i+1} \dots T^*_\delta$. Observe that the capacities on edges, $e$ of $T_i$ or $T^*_i$ are necessarily $2^i$, as otherwise a flow of value $2^i$ could not be routed along that edge to a vertex, $v$, in $R_i$ which $e$ is on the unique path to in $T_i$ or $T^*_i$.
\paragraph{}
Thus we observe that $f(T_i,x) = 2^i |T_i|$ and $f(T^*_i, x^*) = 2^i |T^*_i|$. Now in the first iteration (iteration $\delta$), $T_\delta$ is computed as an $O(1)$-approximation of $T^*_\delta$. Thus
$$2^\delta |T_\delta| = f(T_\delta, x) \leq O(1) f(T^*_\delta, x^*) = 2^\delta O(1) |T^*_\delta|.$$
Now consider the sub-tree that forms by taking the iterations of $$\bigcup_{j=\delta}^i T^*_j$$
and contracting $T_\delta \dots T_{i+1}$ into $r$. If any cycles are formed then do the following: while there still exists a cycle $C$ take the edge $e = \text{argmin}_{e\in C} x^*_e$ and remove it. Call the resulting tree $S^*$.
\paragraph{}
We claim that $(S^*,x^*)$ is feasible on $G_i$ for the nodes in $V(G_i) \cap R_i$, the graph which results from contracting $T_\delta, \dots, T_{i+1}$ into $r$. Since trimming cycles preserves connectivity, and we contracted edges into $r$, all $rv$-paths in $V(G_i) \cap R_i$ are preserved in $S^*$. Hence for all $v \in V(G_i) \cap R_i$ there exists an $rv$-path in $S^*$. Now we claim that the capacities on any $rv$-path still admit a feasible flow after converting to $S^*$. Observe that contraction will not decrease the minimum capacity. Further observe that deleted edges had minimum capacity on a cycle, so any flow that used a deleted edge $uv$ could send the same flow through the $uv$-path remaining in $S^*$ instead. Hence the flows on $S^*$ under capacities $x^*$ still meet the demands in $v \in V(G_i) \cap R_i$. Therefore $(S^*, x^*)$ feasible on $G_i$. Thus by our choice of $T_i$,
$$f(T_i, x) \leq O(1) f(S^*,x^*).$$
That is $$2^i |T_i| \leq 2^i O(1)|S^*| \leq 2^iO(1) (\sum_{j=\delta}^i |T^*_j|).$$
\paragraph{}
Summing these inequalities over $i$ we obtain:
\begin{align*}
f(T,x) &= \sum_{i=1}^\delta 2^i|T_i| \\
&\leq \sum_{i=1}^\delta 2^i O(1)(\sum_{j=\delta}^i |T^*_j|)\\
&= O(1) \sum_{i=1}^\delta \sum_{j=i}^\delta 2^i|T^*_j| \\
&= O(1) \sum_{i=1}^\delta (\sum_{j=1}^{i}(2^j) |T^*_j| &\text{reindexing, and collecting coefficients on each $T^*_j$} \\
&= O(1) \sum_{i=1}^\delta 2^{i+1} |T^*_i| \\
&= 2O(1) \sum_{i=1}^\delta 2^i |T^*_i| \\
&= 2O(1) \sum_{i=1}^\delta f(T^*_i, x^*) \\
&= O(1) f(T^*, x^*).
\end{align*}
Thus the $O(1)$ approximation factor holds, and hence the given algorithm is an $O(1)$-approximation algorithm.$\blacksquare$
%Q5
\section{}
\paragraph{}
Let $G=(V,E)$ be a metric graph with edge lengths $d: E \rightarrow \N_+$ and let $f : 2^V \rightarrow \N_+$  be a a function satisfying:
\begin{enumerate}
\item (Symmetric) $f(S) = f(V\backslash S)$ for all $S \subseteq V$
\item (Subadditive) $f(A \cup B) = f(A) + f(B)$ for all $A,B \subseteq V$.
\end{enumerate}
Consider the following integer program, to be labelled $IP$:
\begin{align*}
\text{min} \sum_{e \in E} d(e) x_e\\
\text{s.t.} \sum_{e\in \delta(S)} x_e &\geq f(S) &\forall S \subseteq V \\
x &\geq 0, \text{ integer}.
\end{align*}
\paragraph{Solving $IP$ of Trees}
We first observe that this $IP$ is easy to solve on trees. Indeed let $T$ be a tree, and for every $uv \in E(T)$ let $S^{uv}_u$ denote the set of vertices in the connected component of $T-uv$ containing $u$, and define $S^{uv}_v$ similarly. By definition, $\delta(S^{uv}_u) =  \{e\} = \delta(S^{uv}_v)$. We define the following algorithm $\cA$ to solve $IP$ on a tree $T$:
\begin{enumerate}
\item For all $uv \in E(T)$ set $x_e = f(S^{uv}_u)$.
\item Return $x$.
\end{enumerate}
To see that $\cA$ solves $IP$ on trees, we will show that $(1)$ the solution returned is feasible, and $(2)$ that every feasible $x$ has necessarily $x_{uv} \geq f(S^{uv}_u)$, implying that the $x$ returned by $\cA$ is optimal.  Let $x$ be the solution returned by $\cA$. 
\paragraph{}
To see $(1)$, it is immediate from the definition of $f$ that $x \geq 0$. It remains to verify the cut inequalities. Let $S \subseteq V$. Enumerate $\delta(S)$ as $\delta(S) = \{e_1, \dots, e_k \}$, where each $e_i := u_iv_i$ and $u_i \in S$. Let $S_1, \dots, S_\ell$ be the partition of $S$ induced by the connected components of $G-\delta(S)$ contained in $S$.  Now let $D(S_i) = \{ e_j \in \delta(S) : u_j \in S_i \}$ be the set of edges in $\delta(S)$ incident with $S_i$. Notice that $\{D(S_i): i = 1\dots \ell\}$ is a partition of $\delta(S)$. 
\paragraph{}
We claim that $\bigcap_{e_j \in D(S_i)} S^{e_j}_{u_j} = S_i.$ If $v \in S_i$ then for all $e_j \in D(S_i)$ there exists a $vu_j$-path that does not use $e_j$, since $v$ and $u_j$ are in the same connected component $S_i$. Thus $v \in \bigcap_{e_j \in D(S_i)}S^{e_j}_{u_j}$. If $v \not\in S_i$ then either $v \in S_{k}$ for $k \neq i$ or $v \in V\backslash S$. In either case, $v$ is connected to any vertex in $S_i$ via a path using some $e_j \in D(S_i)$. But then $v \in S^{e_j}_{v_j} = V \backslash S^{e_j}_{u_j}$. Therefore $v \not\in \bigcap_{e_j \in D(S_i)}S^{e_j}_{u_j}$. Thus the claim holds.
\paragraph{}
So we have $$S = \bigcup_{i=1}^\ell S_i = \bigcup_{i=1}^\ell \bigcap_{e_j \in D(S_i)}S^{e_j}_{u_j}.$$ Now observe that
\begin{align*}
f(S) &\leq \sum_{i=1}^\ell f(\bigcap_{e_j \in D(S_i)}S^{e_j}_{u_j}) &\text{by subadditivity} \\
&= \sum_{i=1}^\ell f(V\backslash \bigcap_{e_j \in D(S_i)} S^{e_j}_{u_j}) &\text{by symmetry} \\
&= \sum_{i=1}^\ell f(\bigcup_{e_j\in D(S_i)} V\backslash S^{e_j}_{u_j}). \\
&\leq \sum_{i=1}^\ell \sum_{e_j \in D(S_i)} f(V\backslash S^{e_j}_{u_j}) &\text{by subadditivity} \\ 
&= \sum_{i=1}^\ell \sum_{e_j \in D(S_i)} f(S^{e_j}_{u_j}) &\text{by symmetry} \\
&= \sum_{e_j \in \delta(S)} f(S^{e_j}_{u_j}) \\
&= \sum_{e_j \i \delta(S)} x_{e_j}.
\end{align*}
Hence $x$ satisfies the cut constraint for $S$. Therefore $x$ is feasible and $(1)$ holds.
\paragraph{}
Now to demonstrate $(2)$, let $uv \in E(T)$. Consider the cut $S^{uv}_u$. We have $\delta(S^{uv}_u) = \{uv\}$ and hence by the cut constraint for $S^{uv}_u$, all feasible $x$ satisfy
$$x_{uv} \geq f(S^{uv}_u).$$
Now the $x$ we construct is tight for all the above, and since $d \geq 0$ we can do no better without violating feasibility. Thus $(2)$ holds and our $x$ is optimal. So we conclude that we can solve $IP$ on trees easily. Further this proof shows that $x$ is optimal for the linear programming relaxation of $IP$ on $T$, and since $x$ is integral this implies the equivalence of the linear programming relaxation and the integer program itself on $T$.
\paragraph{Integrality Gap for $IP$}
We now give an algorithm that produces an integral solution to $IP$ for a general metric graph $G$ which approximates the optimal $LP$-relaxation of $IP$ within a factor of $O(\log n)$. Our algorithm will use the following theorem from class:
\begin{theorem} \label{th:metric-tree}
Let $G=(V,E)$ be a graph. Let $d: E \rightarrow \R_+$ be a metric function. Let $\lambda^*_{uv} \geq 0$ be coefficients for all $(u,v) \in V\times V$. There exists a polynomial time algorithm which constructs a weighted tree $T$ on $V$ such that $d^T(u,v) \geq d(u,v)$ for all $u,v\in V$ and $$\sum_{u,v} \lambda^*_{uv}d^T(u,v) \leq O(\log n) \sum_{u.v} \lambda^*_{uv} d(u,v).$$
\end{theorem}
Consider the algorithm $\cB$:
\begin{enumerate}
\item Let $x^*$ be the optimal solution to the linear programming relaxation.
\item Use Theorem \ref{th:metric-tree} with $\lambda^* = x^*$ to obtain a weighted tree $T$.
\item Solve $IP$ (equivalently its $LP$ relaxation) on $T$ as discussed above and return the solution $\bar{x}$.
\end{enumerate}
\paragraph{}
We claim that the solution $\bar{x}$ returned by $\cB$ is feasible for $IP$. To see this, observe that completing $T$ to the graph $G$ does not remove any edges across any cut set $S$, hence the bound $\sum_{e \in \delta(S)} \bar{x}_e \geq f(S)$ still holds, since additional edges contribute non-negative amounts to $\sum_{e \in \delta(S)}$. This implies that $\bar{x}$ is feasible for $IP$ on $G$.
\paragraph{}
Now we analyze the cost. We see that
$$\sum_{u,v \in V} d(u,v) \bar{x}_{uv} \leq \sum_{u,v \in V} d^T(u,v) \bar{x}_{uv}$$
from Theorem \ref{th:metric-tree}. Now we convert $x^*$, the optimal solution for the linear programming relaxation of $IP$ on $G$, to a feasible solution on $T$. We will call such a solution $x'$. Our transformation will  preserve that $\sum_{u,v \in V} d^T(u,v) x^*_{uv} = \sum_{u,v \in V} d^T(u,v) x'_{uv}$. 
\begin{enumerate}
\item Begin with $x'_{uv} = x^*_{uv}$ for all $uv \in E(T)$.
\item Now for all $u,v \in V$ such that $uv \not\in E(T)$ do:
\begin{enumerate}
\item Let $P$ be the $uv$-path in $T$.
\item For each $e \in P$ add $x^*_{uv}$ to the current value of $x'_e$.
\end{enumerate}
\end{enumerate}
First we show that $x'$ is feasible on $T$. Let $\delta_G(S)$ denote $\delta(S)$ with respect to $G$ and similarly define $\delta_T(S)$ as $\delta(S)$ with respect to $T$. Let $P_{uv}$ denote the $uv$-path in $T$. Then for any cut set $S$ we have
$$f(S) \leq \sum_{e \in \delta_G(S)} x^*_e = \sum_{e\in \delta_T(S)} x^*_e + \sum_{e \in \delta_G(S)\backslash \delta_T(S)} x^*_e \leq \sum_{e \in \delta_T(S)} x'_e.$$
The second inequality follows from the observation that for every edge $e \in \delta_G(S) \backslash \delta_T(S)$ there exists some $e' \in P_{e} \cap \delta_T(S)$ and $$x'_{e'} = x^*_e + \sum_{uv \in E(G) \backslash E(T) : e' \in P_{uv}} x^*_{uv}.$$
The fact that $\sum_{u,v \in V} d^T(u,v) x^*_{uv} = \sum_{u,v \in V} d^T(u,v) x'_{uv}$ is immediate from the definitions:
\begin{align*}
\sum_{u,v \in V} d^T(u,v) x^*_{uv} &= \sum_{uv \in E(T)} d^T(u,v)x^*_{uv} + \sum_{e \in E(G) \backslash E(T)} \sum_{uv \in P_{e}} d^T(u,v) x^*_{e}\\
&= \sum_{uv \in E(T)} d^T(u,v)( x^*_{uv} + \sum_{e \in E(G)\backslash E(T): uv \in P_{e}} x^*{e}) \\
&= \sum_{uv \in E(T)} d^T(u,v) x'_{uv} \\
&= \sum_{u,v \in V} d^T(u,v) x'_{uv}.
\end{align*}
More intuitively, the distance between two vertices $u,v$ in $T$ is the sum of the distances on the edges in the path that joins them. Thus if we transfer the allocation $x^*_{u,v}$ to each edge in $P_{uv}$ we lose nothing in the objection function as $d^T(u,v) x^*_{u,v} = \sum_{ab \in P_{uv}} d^T(a,b) x^*_{uv}$.
\paragraph{}
Now we have that $x'$ is feasible on $T$, so by the optimality of $\bar{x}$ on $T$ we have
$$\sum_{u,v \in V} d^T(u,v) \bar{x}_{uv} \leq \sum_{u,v \in V} d^T(u,v) x'_{uv} = \sum_{u,v \in V} d^T(u,v) x^*_{uv}.$$
Thus combining inequalities we see that
$$\sum_{u,v \in V} d(u,v) \bar{x}_{uv} \leq \sum_{u,v \in V} d^T(u,v) x^*_{uv}.$$
Now by Theorem \ref{th:metric-tree} and our choice of $\lambda^* =x^*$ we see that 
$$\sum_{u,v \in V} d^T(u,v) x^*_{uv} \leq O(\log n) \sum_{u,v \in V} d(u,v) x^*_{uv}$$
and hence
$$\sum_{u,v \in V} d(u,v) \bar{x}_{uv} \leq O(\log n) \sum_{u,v \in V} d(u,v) x^*_{uv}.$$
Now since $\bar{x}$ is feasible for the $IP$ on $G$, this inequality tell us that the ratio of the optimal value of $IP$ to the the optimal value of its linear programming relaxation is bounded above by $O(\log n)$. That is, the integrality gap of $IP$ is $O(\log n)$. $\blacksquare$

%Q^6
\section{}
%Pa
\subsection{a}
\paragraph{}
We want to show the following theorem:
\begin{theorem}\label{th:tree-metric}
Let $G = (V,E)$ be a metric graph with edge lengths $d:E\rightarrow \R_+$ and $W \subset V$. One can randomly find a tree $T$ such that
\begin{enumerate}
\item $d(u,v) \leq d^T(u,v)$ for all $u,v\in W$
\item $E[d^T(u,v)] \leq O(\log n) d(u,v)$ for all $u,v\in V$.
\end{enumerate}
\end{theorem}
\begin{proof}
We may assume $\max_{u,v \in V} d(u,v) = 2^\delta$ for some $\delta \in \N$ and $d(u,v) > 1$ for all $u,v\in V$. Construct a tree $T$ by:
\begin{enumerate}
\item Choose randomly a permutation $\pi$ of $W$.
\item Choose $\beta \in [0,1]$ uniformly at random.
\item Set $D_\delta = \{V\}$.
\item For $i = \delta -1$ to $0$ do:
\begin{enumerate}
\item Assign every node to the first node according to $\pi$ that has distance at most $2^\beta \cdot 2^{i-1}$.
\item For each node such that its distance from each vertex in $W$ is greater that $2^\beta \cdot 2^{i-1}$ assign it its own cluster.
\item All nodes assigned to the same node that are currently in the same cluster will form a new cluster in $D_i$.
\item Add a node to $T$ for each new cluster in $D_i$ and add edges to $T$ from each such node to the parent node in $T$ corresponding to their common cluster in $D_{i+1}$.
\item For the clusters formed in Step $(b)$ assign distance $0$ to the new edge in $T$. For all others (ie those from $(a)$) assign distance $2^{i+1}$.
\end{enumerate}
\item Return tree $T$.
\end{enumerate}
Note that the clusters of $D_0$ are singletons because the bound on the maximum distance is $2^\beta 2^0 \leq 1$. Hence all nodes of $V$ are invariably separated at some level by the end of operation of the algorithm.
\paragraph{Proof of 1.}
Suppose $u,v \in W$ are in the same cluster at level $D_i$ and are separated at level $D_{i-1}$. Let $w$ be the node identified with $u$ and $v$'s cluster $S_w$ at level $D_i$. Then
$$d(u,v) \leq d(u,w) + d(w,v) \leq 2^\beta 2^{i-1} + 2^\beta 2^{i-1} \leq 2^{i+1}.$$ The first inequality follows from the triangle inequality. The second inequality follows since the distance from $u$ (similarly $v$) to the next node on the $uw$-path is $2^{i-1}$ and the distance of the rest of the path is at most $\sum_{k=0}^{i-2} 2^k \leq 2^{i-1}$. On the other hand $d^T(u, S_w) \geq 2^i$ and $d^T(S_w, v) \geq 2^i$ so $d^T(u,v) = d^T(u, S_w) + d^T(S_w, v) \geq 2^i + 2^i = 2^{i+1}$. This proves part $1.$ of the theorem.
\paragraph{Proof of 2.}
Observe that if $u,v$ are separated first at level $D_i$ then $d^T(u,v) \leq 2^{i+2}$. Let $S_i$ be the cluster in $D_i$ containing $u,v$. Let $S^u_{i-1}$ be descended cluster of $S_i$ from which $u$ descends and similarly define $S^v_{i-1}$. Then 
\begin{align*}
d^T(u,v) &= d^T(S_i, S^u_{i-1}) + d^T(S_i, S^v_{i-1}) + d^T(S^u_{i-1}, u) + d^T(S^v_{i-1}, v)\\
&\leq \max\{2^i,0\} + \max\{2^i,0\} +  d^T(S^u_{i-1}, u) + d^T(S^v_{i-1}, v) \\
&\leq 2^i + 2^i + 2\sum_{j = 1}^{i-1} 2^j \\
&= 2^{i+1} + 2^{i+1} \\
&= 2^{i+2}.
\end{align*}
Now we say that node $c$ {\it settles} $(u,v)$ at level $D_i$ if $c$ is the first node with respect to $\pi$ to which one of $u$ or $v$ gets assigned at level $D_i$. We say that node $c$ {\it cuts} $(u,v)$ at level $D_i$ if exactly one of $u,v$ is at distance at most $2^\beta 2^i$ from $c$. If $u,v$ are separated at level $D_i$ then either there exists some $c \in W$ which both settles and cuts $u,v$ at level $D_i$ (in which case they were separated by Step $4(a)$), or every $c \in W$ does not settle $u,v$ (in which case they were separated by Step $4(b)$).
\paragraph{}
Note that in the first case the distance between $u$ and $v$ is at most $2^{i+2}$ by our previous observation. In the second case both $u$ and $v$ were assigned their own cluster in $D_i$ and given distance $0$ to their common ancestor. Hence $d^T(u,v) = 0$ in this second case. So we have the following bound on $E[d^T(u,v)]$:
 \begin{align*}E[d^T(u,v)] &\leq \sum_{i=0}^{\delta - 1} (\sum_{c \in W} Pr [ \text{$c$ settles $u,v$ and  $c$ cuts $u,v$ at $D_i$}] 2^{i+2} + Pr[\text{all $c \in W$ do not settle $u,v$}]\cdot 0) \\&= \sum_{i=0}^{\delta - 1} \sum_{c \in W} Pr [ \text{$c$ settles $u,v$ and  $c$ cuts $u,v$ at $D_i$}] 2^{i+2}.
 \end{align*}
 For $c \in W$, $u,v \in V$, and level $D_i$,  let $A$ be the event that $c$ settles $u,v$ and let $B$ be the event that $c$ cuts $u,v$ at $D_i$. We are interested in $Pr[A \cap B]$, or equivalently, $Pr[A \mid B] \cdot Pr[B]$.
 \paragraph{}
We first analyze $Pr[B]$. Without loss we may assume $d(c,u) \leq d(c,v)$. For $c$ to cut $u,v$ we need $d(c,u) \leq 2^\beta 2^{i-1} < d(c,v)$ by definition.  Since we sample $2^\beta$ uniformly at random, the probability of this happening is simply $$\frac{1}{2^{i-1}}$$ times the length of the interval intersecting $[2^{i-1},2^i]$ with $[d(c,u), d(c,v)]$. More formally:
$$Pr[B] = \frac{| [2^{i-1},2^i] \cap [d(c,u),d(c,v)]|}{|[2^{i-1},2^i]|} = \frac{| [2^{i-1},2^i] \cap [d(c,u),d(c,v)]|}{2^{i-1}}.$$
\paragraph{}
Now we need $Pr[A \mid B]$. Let us order the nodes of $W$ according to their distance to $u,v$. That is, $W = \{c_1, \dots, c_n\}$ where $\min\{d(c_i, v), d(c_i, u)\} \leq \min \{d(c_{i+1}, v), d(c_{i+1}, u)\}$ for all $i$. If $c= c_k$ then the probability that $c$ comes before $c_1, \dots, c_{k-1}$ in $\pi$ is at most $\frac{1}{k}$. In order for $c$ (given that $c$ cuts $u,v$) to settle $u,v$ at least that has to happen. Hence
$$Pr[A \mid B] \leq \frac{1}{k}.$$
\paragraph{}
Putting it all together we obtain the following expectation:
\begin{align*}
E[d^T(u,v)] &\leq \sum_{i=0}^{\delta -1} \sum_{k=1}^n Pr [A \mid B]Pr[B] 2^{i+2} \\
&\leq \sum_{k=1}^n \frac{1}{k} \sum_{i=0}^{\delta -1 }  2^{i+2}\frac{| [2^{i-1},2^i] \cap [d(c,u),d(c,v)]|}{2^{i-1}} \\
&= 8 \sum_{k=1}^n \frac{1}{k} \sum_{i=0}^{\delta -1 }  | [2^{i-1},2^i] \cap [d(c,u),d(c,v)]| \\
&\leq 8 d(u,v) \sum_{k=1}^n \frac{1}{k} \\
&= 8 H_n d(u,v) \\
&= O(log n) d(u,v).
\end{align*}
Thus the second claim and therefore the theorem holds.
\end{proof}

%Pb
\subsection{b}
\paragraph{}
Let $G = (V,E)$ be a metric graph with edge costs $c:E \rightarrow \R_+$ and requirements $r(u,v) \in \N$ for all $u,v \in V$. We wish to operate on a tree with the same vertex set at $G$. The following theorem will let us do so.
\begin{theorem}
Let $T$ be a tree obtained by the algorithm of Theorem \ref{th:tree-metric}. Then there exists a tree $T' = (V,F)$ such that $d^T(u,v) \leq d^{T'}(u,v) \leq 4 d^T(u,v),$ and every $v \in V\backslash W$ is a leaf of $T'$.
\end{theorem}
\begin{proof}
Pick $v \in W$ with a parent $w \not \in V$ and contract edge $vw$ in $T$. Repeat until all nodes of $T$ are in $V$. Multiply by $4$ the weights on all remaining edges. Call the resulting tree $T'$. To see this algorithm works observe that every node not in $V$ is a associated with some cluster $S_w$ corresponding to $w$. Descendant of the node $S_w$ in $T$ is the node associated with $w$. The path from $S_w$ to $w$ contains only cluster nodes not in $V$ or nodes in $W$. Thus we can find some cluster node with child in $W$ by following the path from $S_w$ to $w$ until we reach the first node in $W$ along this path. Its parent is a cluster node. The contraction operation preserves this structure of the tree, and hence we can repeat until all cluster nodes have been contracted into nodes in $W$.  Since each $v \in V\backslash W$ began with its associated vertex in $T$ as a leaf, and was never contracted it remains a leaf vertex in the resulting tree $T'$.
\paragraph{}
Note that clearly $d^{T'}(u,v) \leq 4d^T(u,v)$ as contracting only shortens the distances, but then edge distances were multiplied by a factor of $4$ at the end of operation. Furthermore, in the worst case $d^{T'}(u,v) = 2^i \cdot 4$ if contraction results in contracting $u$ and $v$ in $W$ right up to the cluster where they were separated (that is, contracting preserves the largest edge distance on the $uv$-path in $T$). But then
$$d^{T'}(u,v) \geq 2^i \cdot 4 = 2^i + 2^i + 2^i + 2^i \geq d(S^u_{i-1}, S_i) + d(S^v_{i-1}, S_i) +  d(S^u_{i-1}, u) + d(S^v_{i-1}, v) = d^T(u,v).$$
Hence the theorem holds.
\end{proof}
\paragraph{}
Combining the above with Theorem \ref{th:tree-metric} and hiding the factor of $4$ in the $O$-notation we have the important corollary
\begin{corollary}\label{cor:tree-metric}
Let $G = (V,E)$ be a metric graph with edge lengths $d:E\rightarrow \R_+$ and $W \subset V$. One can randomly find a tree $T$ with vertex set $V$ such that every $v \in V\backslash W$ is a leaf of $T$ and
\begin{enumerate}
\item $d(u,v) \leq d^T(u,v)$ for all $u,v\in W$
\item $E[d^T(u,v)] \leq O(\log n) d(u,v)$ for all $u,v\in V$.
\end{enumerate}
\end{corollary}
\paragraph{}
Consider the following algorithm, $\cA$, for GSN on $G$:
\begin{enumerate}
\item Let $W = \{v \in V: \exists u \in V, r(u,v) > 0\}$.
\item Use Corollary \ref{cor:tree-metric} to compute a tree metric $T$ which approximates $G$ with $W$.
\item Solve GSN on $T$ and let $x$ be the resulting solution.
\item Return $x$.
\end{enumerate}
\paragraph{}
We want to show that $\cA$ is an $O(log |W|)$-approximation algorithm for GSN on $G$. Thus we need to show the following things:
\begin{enumerate}
\item That we can solve GSN on trees in polynomial time. This implies that $\cA$ runs in polynomial time since the tree of Theorem \ref{th:tree-metric} can be found in polynomial time, and the contractions necessary to yield the tree in Corollary \ref{cor:tree-metric} can be done in polynomial time. 
\item The $x$ returned is feasible on $G$.
\item The approximation factor holds.
\end{enumerate}
\paragraph{Proof of $1$}
Observe that every edge $ab \in E(T)$ is a bridge. Thus is we let $T_a$ denote the connected component of $T-ab$ containing $a$ and $T_b$ be the connected component of $T-ab$ containing $b$ then any feasible solution $x$ to GSN on $T$ necessarily has
$$x_{ab} \geq \max_{u \in V(T_a), v \in V(T_b)} \{r(u,v)\}$$
since any $uv$-flow from $u\in V(T_a)$ to $v \in V(T_b)$ routes its entire value across $ab$. We claim that setting
$$x_{ab} = \max_{u \in V(T_a), v\in V(T_b)} \{r(u,v)\}$$
is actually feasible, and hence optimal. Let $u,v \in V$. Let $ab$ be any edge along the $uv$-path in $T$. Then deleting $ab$ separates $u$ from $v$. That is, without loss  of generality we have $u \in T_a$ and $v \in T_b$. So,
$$x_{ab} = \max_{u \in V(T_a), v\in V(T_b)} \{r(u,v)\} \geq r(u,v).$$
Thus we can route a flow of value of $r(u,v)$ from $u$ to $v$ along the path from $u$ to $v$ in $T$. Thus we can solve GSN on trees in $O(|E| |V|^2)$ time by iterating over all edges, and then iterating over all pairs of vertices to compute the optimal $x$ discussed above.
\paragraph{Proof of $2$}
To see that $x$ is feasible for $G$ observe that since $G$ is a metric graph it is necessarily complete. So $T$ is a subgraph of $G$. Since $T$ spans $G$ every demand $r(u,v) \in V(G)$ can be satisfied by installing capacities $x_e$ on the edges in $e\in E(T) \subseteq E(G)$. Hence $x$ is feasible for GSN on $G$.
\paragraph{Proof of $3$}
It remains to verify the desired approximation factor. To do so we first make an interesting observation about the structure of our tree $T$ and the resulting solution $x$ on $T$. Let $v \in V\backslash W$. Then $v$ is a leaf of $T$. Let $w \in W$ be the adjacent vertex of $v$ in $T$. Then
$$x_{vw} = \max_{a \in T_v, b \in T_w} \{r(a,b)\} = \max_{b \in T_w} \{r(v, b)\} = 0$$
with the last inequality following since $v \not\in W$ and by the definition of $W$ this implies $r(v,b) = 0$ for all $b \in V$ (otherwise there would exist some vertex $b$ where $r(v,b) > 0$ and that would force $v \in W$). Therefore
$$\sum_{u,v \in V} d(u,v) x_{uv} = \sum_{u,v \in W} d(u,v) x_{uv} .$$
Now we use that $d(u,v) \leq d^T(u,v)$ for all $u,w \in W$ to see that
$$\sum_{u,v \in V} d(u,v) x_{uv} \leq \sum_{u,v \in W} d^T(u,v) x_{uv} = \sum_{u,v \in V} d^T(u,v) x_{uv}.$$
Let $x^*$ be the optimal solution to GSN on $G$. We translate $x^*$ to a feasible solution on $T$ as follows. Start with $x' = 0$. For each $uv \in E(G)$ for each $ab \in P_{uv}$ update
$$x'_{ab} = x'_{ab} + x^*_{uv}.$$
That is we send each $uv$'s capacity on an edge of $G$ to each edge on the $uv$-path in $T$. Since $d^T(u,v) = \sum_{ab \in P_{uv}} d^T(ab)$ we have
$$\sum_{u,v \in V} d^T(u,v) x^*_{uv} = \sum_{u,v \in V} d^T(u,v) x'_{uv}.$$
Now we claim that $x'$ is feasible for $T$. Observe that since $x^*$ is feasible on $G$ every $uv$ pair has their demand satisfied by $G$. On $T$ the same demand can be satisfied by rerouting any flow on an edge no longer present in $T$ by routing along the $uv$-path using the capacity transferred to that path from $x^*_{uv}$. Thus $x'$ can route flows satisfying all demands using only edges of $T$. Hence $x'$ is feasible on $T$. Then,
$$\sum_{u,v\in V} d^T(u,v) x_{uv} \leq \sum_{u,v in V} d^T(u,v) x'_{uv} = \sum_{u,v \in V} d^T(u,v) x^*_{uv}$$
with the inequality following from $x$ being optimal on $T$ while $x'$ is feasible on $T$. So combining our inequalities so far we have
$$ \sum_{u,v \in V} d(u,v) x_{uv} \leq \sum_{u,v \in V} d^T(u,v) x^*_{uv}.$$
Taking Expectation of both sides and applying property $2.$ of Corollary \ref{cor:tree-metric} we see that
$$E[\sum_{u,v \in V} d(u,v) x_{uv}] \leq O(\log |W|) E[\sum_{u,v\in V} d(u,v) x^*_{uv}].$$
Therefore, in expectation, the cost of the solution returned by our algorithm is at most $O(\log |W|)$ times the cost of the optimal solution.
Hence the algorithm $\cA$ is an $O(\log k)$-approximation algorithm for GSN where $k = |W| = |\{ v \in V: \exists u \in V, r(u,v) > 0 \}|.\blacksquare$
\end{document}
