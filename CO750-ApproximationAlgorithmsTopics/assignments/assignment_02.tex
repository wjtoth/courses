\documentclass[letterpaper,12pt,oneside,onecolumn]{article}
\usepackage[margin=1in, bottom=1in, top=1in]{geometry} %1 inch margins
\usepackage{amsmath, amssymb, amstext}
\usepackage{fancyhdr}
\usepackage{mathtools}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{theorem}
\usepackage{tikz}
\usepackage{tkz-berge}

%Macros
\newcommand{\A}{\mathbb{A}} \newcommand{\C}{\mathbb{C}}
\newcommand{\D}{\mathbb{D}} \newcommand{\F}{\mathbb{F}}
\newcommand{\N}{\mathbb{N}} \newcommand{\R}{\mathbb{R}}
\newcommand{\T}{\mathbb{T}} \newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
 
 
\newcommand{\cA}{\mathcal{A}} \newcommand{\cB}{\mathcal{B}}
\newcommand{\cC}{\mathcal{C}} \newcommand{\cD}{\mathcal{D}}
\newcommand{\cE}{\mathcal{E}} \newcommand{\cF}{\mathcal{F}}
\newcommand{\cG}{\mathcal{G}} \newcommand{\cH}{\mathcal{H}}
\newcommand{\cI}{\mathcal{I}} \newcommand{\cJ}{\mathcal{J}}
\newcommand{\cK}{\mathcal{K}} \newcommand{\cL}{\mathcal{L}}
\newcommand{\cM}{\mathcal{M}} \newcommand{\cN}{\mathcal{N}}
\newcommand{\cO}{\mathcal{O}} \newcommand{\cP}{\mathcal{P}}
\newcommand{\cQ}{\mathcal{Q}} \newcommand{\cR}{\mathcal{R}}
\newcommand{\cS}{\mathcal{S}} \newcommand{\cT}{\mathcal{T}}
\newcommand{\cU}{\mathcal{U}} \newcommand{\cV}{\mathcal{V}}
\newcommand{\cW}{\mathcal{W}} \newcommand{\cX}{\mathcal{X}}
\newcommand{\cY}{\mathcal{Y}} \newcommand{\cZ}{\mathcal{Z}}

\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}


\newenvironment{proof}{{\bf Proof:  }}{\hfill\rule{2mm}{2mm}}
\newenvironment{proofof}[1]{{\bf Proof of #1:  }}{\hfill\rule{2mm}{2mm}}
\newenvironment{proofofnobox}[1]{{\bf#1:  }}{}\newenvironment{example}{{\bf Example:  }}{\hfill\rule{2mm}{2mm}}

%\renewcommand{\thesection}{\lecnum.\arabic{section}}
%\renewcommand{\theequation}{\thesection.\arabic{equation}}
%\renewcommand{\thefigure}{\thesection.\arabic{figure}}

\newtheorem{fact}{Fact}[section]
\newtheorem{lemma}[fact]{Lemma}
\newtheorem{theorem}[fact]{Theorem}
\newtheorem{definition}[fact]{Definition}
\newtheorem{corollary}[fact]{Corollary}
\newtheorem{proposition}[fact]{Proposition}
\newtheorem{claim}[fact]{Claim}
\newtheorem{exercise}[fact]{Exercise}
\newtheorem{note}[fact]{Note}
\newtheorem{conjecture}[fact]{Conjecture}

\newcommand{\size}[1]{\ensuremath{\left|#1\right|}}
\newcommand{\ceil}[1]{\ensuremath{\left\lceil#1\right\rceil}}
\newcommand{\floor}[1]{\ensuremath{\left\lfloor#1\right\rfloor}}

%END MACROS
%Page style
\pagestyle{fancy}

\listfiles

\raggedbottom

\lhead{2017-03-14}
\rhead{W. Justin Toth CO750-Approximation Algorithms Assignment 2} %CHANGE n to ASSIGNMENT NUMBER ijk TO COURSE CODE
\renewcommand{\headrulewidth}{1pt} %heading underlined
%\renewcommand{\baselinestretch}{1.2} % 1.2 line spacing for legibility (optional)

\begin{document}

%Question 1
\section{}
\paragraph{}
Let $G = (V,E)$ be an undirected graph with edge cost $c: E\rightarrow \R_+$. Consider the problem $(IP)$:
\begin{align*}
\min\ \sum_{e\in E} c(e) x_e&\\
\text{s.t.}\  \sum_{e\in\delta(S)} x_e &\geq f(S) &\forall S \subseteq V \\
x&\geq 0\\
x&\in \Z^{|E|},
\end{align*}
where $f : S \rightarrow \{0,1\}$ satisfies the maximality property:
$$\text{ for all disjoint } A,B\subseteq V, \text{ we have } f(A \cup B ) \leq \max\{f(A), f(B)\},$$
and $f$ satisfies that there does not exist $S \subseteq V$ and $A, B \subseteq S$ such that
\begin{equation}\label{eq:1}
A\cap B = \emptyset,\ f(S)=f(A)=f(B)=0 \text{ and } f(S\backslash A) = f(S\backslash B) = 1.
\end{equation}
We may assume that $f(V') = 0$, for the every connected component vertex set $V'$, otherwise $(IP)$ is infeasible. Let $(LP)$ denote the linear programming relaxation of $(IP)$ and let $(D)$ denote the dual of $(LP)$:
\begin{align*}
\max\ \sum_{S\subseteq V} f(S) y_S&\\
\text{s.t.}\ \sum_{S : e \in \delta(S)} y_S &\leq c(e) &\forall e \in E \\
y&\geq 0. 
\end{align*}
\paragraph{Algorithm}
Consider the primal-dual algorithm, $\cA$, for this problem:
\begin{enumerate}
\item Start with $x=0$, $y=0$, and $F \rightarrow \emptyset$.
\item While $F$ is not feasible do:
	\begin{enumerate}
	\item Increase $y_S$ uniformly for ``minimal violated cuts" with respect to $F$ until some dual constraint corresponding to an edge $e$ becomes tight.
	\item Set $x_e = 1$, $F = F\cup \{e\}$.
	\end{enumerate}
\item Reverse Deletion: Consider all edges $e$ in $F$ in reverse of the order they were added to $F$ and if $F\backslash \{e\}$ is feasible then set $F = F\backslash\{e\}$ and $x_e = 0$.
\end{enumerate}
\paragraph{}
We intend to show that $\cA$ is a polynomial time $2$-approximation algorithm for $(IP)$. We will need the following lemma from class:
\begin{lemma}\label{lemma:maximality}
Let $f$ be a $01$-function satisfying maximality. Let $F\subseteq E$. Then
\begin{enumerate}
\item $F$ is feasible for $(IP)$ with $f$ if and only if every connected component $C$ of $(V,F)$ satisfies $f(C) = 0$.
\item The minimal violated cuts are connected components $C$ that have $f(C) = 1$.
\end{enumerate}
\end{lemma}
This lemma allows us to show that $\cA$ terminates in polynomial time.
\begin{lemma}\label{lemma:poly1}
Suppose we have a polynomial time oracle for $f$. Then $\cA$ returns a feasible solution for $(IP)$ in polynomial time
\end{lemma}
\begin{proof}
By definition of $\cA$ if $\cA$ terminates then the result is feasible. To see that $\cA$ terminates observe that in every iteration of step $2$ some edge $e \in |E|$ is added to $F$. Since $E$ is feasible, the while loop terminates after $O(|E|)$ iterations. Hence $\cA$ terminates, and returns a feasible solution.
\paragraph{} 
It remains to show that $\cA$ runs in polynomial time. Since we have a polynomial time oracle for $f$ it is easy to see that step $3$ runs in polynomial time. Further we need only maintain $y_S$ for non-zero $y_S$, of which we will show there are a polynomial number. We need to verify step $2(a)$ can be done in polynomial time. By lemma \ref{lemma:maximality} we need only maintain connected components of $F$ since the connected components $C$ with $f(C) = 1$ are precisely the minimal violated cuts. There are a polynomial number of connected components at each iteration (hence a polynomial number of non-zero $y_S$), and since we have a polynomial time oracle for $f$ this implies step $2(a)$ can be done in polynomial time.
\end{proof}
\paragraph{}
Now we need to check the approximation factor. We present the following reduction from class as a lemma:
\begin{lemma}\label{lemma:degree}
Let $w_i$ denote the number of minimally violated sets at iteration $i$.  Let $\cS_i$ denote the set of minimal violated cuts at iteration $i$. If $\sum_{S\in \cS_i} |\delta_F(S)| \leq 2w_i$ for all $i$ then $\cA$ is a $2$-approximation algorithm for $(IP)$ (for any $01$-function $f$).
\end{lemma}
\begin{proof}
It is immediate from our construction of $x$ and $y$ that if $e \in F$ then
$$\sum_{S : e\in\delta(S)} y_S = c(e).$$
Thus we have the objection function value on $F$ is
$$\sum_{e\in F} c(e) = \sum_{e\in F} \sum_{S : e\in\delta(S)} y_S = \sum_{S\subseteq V} |\delta_F(S)|y_S.$$
Let $\Delta_i$ denote the amount by which all minimal violated cuts get increased in iteration $i$. Then
$$\sum_{S\subseteq V} |\delta_F(S)|y_S = \sum_{i} \sum_{S\in \cS_i} |\delta_F(S)|\Delta_i \leq \sum_{i} 2w_i \Delta_i = 2\sum_{S\subseteq V} f(S) y_S.$$
The inequality follows from our hypothesis, and the last equality follows since $f$ is a $01$-function for which non-zero $y_S$ (minimally violated sets) have $f(S) = 1$. 
\end{proof}
\paragraph{}
Now we intend to show that the hypothesis of the previous lemma holds. Consider some iteration $i$ step $2$ of $\cA$, and let $A$ be the of edges chosen so far at the end of $i$. Let $B = F \cap A$. Construct a graph $H$ by taking the graph $(V,F)$ and contracting the connected components of $(V,B)$ to vertices. Then $E(H) = F\backslash B$. Let $S_v$ denote the set of vertices in $G$ corresponding to vertex $v \in V(H)$. It is not hard to see that $H$ is a forest. Indeed if $H$ has a cycle $C$ then, since for all $v \in V(C)$, $S_v$ is a connected component in $G$, we can find a cycle $C'$ in $G$ by expanding each $v \in V(C)$ and joining its vertices which lie on $C$ by a path in $S_v$. But $F$ is a forest since it is necessary that any edge $e$ be a bridge to survive the reverse deletion step as $f$ is a $01$-function (observe that for all $S$ $|\delta(S) \backslash \{e\}| \geq 1 \geq f(S)$ if $e$ is not a bridge).
\paragraph{}
Now let $W\subseteq V(H)$ be the set of vertices of $H$ such that for all $w \in W$, $f(S_w) = 1$. Then $\{S_w : w\in W\} = \cS_i$. Let $d_v$ denote the degree of vertex $v$ in $H$. So the condition
$$\sum_{S \in \cS_i} |\delta_F(S)| \leq 2w_i$$
is equivalent to
$$\sum_{w \in W} d_w \leq 2|W|.$$
\begin{lemma}\label{lemma:leaves}
$\sum_{w\in W} d_w \leq 2|W|$.
\end{lemma}
\begin{proof}
First we claim that for every connected component $C$ of $H$, at most one leaf of $C$ satisfies $f(S_v) = 0$. Suppose for a contradiction there exists some connected component $C$ of $H$ that has two leaves $v$ and $w$ such that $f(S_v) = f(S_w) = 0$. Let $S_C = \cup_{u\in C} S_u$. Let $e_v, e_w \in F$ be the edges incident to $v$ and $w$ respectively in $H$. By minimality via the reverse deletion step, $F\backslash \{e_v\}$ and $F\backslash \{e_w\}$ are infeasible. But then $f(S_C\backslash S_v) =1 $ and $f(S_C\backslash S_w) = 1$ as these are precisely the cuts which would have no incident edges when $e_v$ and $e_w$ are removed respectively (we already know $f(S_v) = f(S_w) = 0$). Since $C$ is a connected component of $H$ we have $f(S_C) = 0$ by the feasibility of $F$. Hence
$$f(S_v) = f(S_w) = f(S_C) = 0 \quad\text{and}\quad f(S_C\backslash S_v) = f(S_C \backslash S_w) = 1$$
but $S_v, S_w \subseteq S_C$ and $S_v \cap S_w = \emptyset$. This contradiction property (\ref{eq:1}).
\paragraph{}
Now we proceed to demonstrate the inequality. First discard isolated vertices of $H$ since they affect the count in no way. Let $c$ denote the number of connected components of $H$. Now we have,
\begin{align*}
\sum_{w\in W} d_w &= \sum_{v \in H} d_v - \sum_{v\not\in W} d_v \\
&= 2|H| -2c - \sum_{v\not\in W} d_v &\text{since $H$ is a forest} \\
&\leq 2|H| - 2c - 2(|H| - |W| - c) \\
&= 2|W|.
\end{align*}
The inequality follows from our claim which implies all vertices not in $W$ have degree at least $2$ except for one leaf per connected component of $H$, which has degree $1$.
\end{proof}
\paragraph{}
Hence combining lemma \ref{lemma:degree} and lemma \ref{lemma:leaves} with our discussion about $H$ we have shown that $\cA$ returns a solution $F$ with $$c(F) \leq 2\sum_{S\subseteq V} f(S) y_S \leq 2\cdot OPT,$$
where $OPT$ is the optimal solution of $(IP)$. Therefore we have shown that
\begin{theorem}
$\cA$ is a $2$-approximation algorithm for $(IP)$.
\end{theorem}

%Q2
\section{}
\paragraph{}
Let $G = (V,E)$ be a metric graph with edge costs $c: E\rightarrow \R_+$. Consider the problem of finding a minimum cost subset of edges $F\subseteq E$ such that each connected component of $(V,F)$ is an eulerian subgraph with at least $100$ vertices. We will denote this problem $(E100)$.
\paragraph{}
Define the function $f:2^V \rightarrow \{0,2\}$ by:
$$f(S) = \begin{cases}
2, &\text{if } 1\leq |S| < 100 \\
0, &\text{otherwise.}
\end{cases}$$
Then we can define a linear programming relaxation $(LP)$ for $(E100)$:
\begin{align*}
\min\ \sum_{e\in E} c(e) x_e&\\
\text{s.t.}\  \sum_{e\in\delta(S)} x_e &\geq f(S) &\forall S \subseteq V \\
x&\geq 0.
\end{align*}
To justify this relaxation consider a feasible solution $F$ to $(E100)$. Let $x^F$ be the incidence vector of $F$. Then $x^F$ is feasible for $(LP)$. Let $S \subseteq V$ such that $1 \leq |S| < 100$. Since $|S| < 100$, $S$ does not contain any single connected component of $(V,F)$. Since $1\leq |S|$, $S$ has non-trivial intersection with some connected component $C$ of $(V,F)$ (i.e. $S\cap C \neq \emptyset$ and $S\cap C \supset C$). Since $F$ is feasible, $C$ is eulerian. Hence the degree of each vertex in $C$ (with respect to $F$) is even. Thus $|E(C\cap S, C\backslash S)| \geq 2$. Therefore
$$\sum_{e \in \delta(S)} x_e \geq \sum_{e \in E(C\cap S, C\backslash S)} x_e \geq 2 \geq f(S),$$
and hence $x$ is feasible for $(LP)$.
\paragraph{}
We now define another function $g: 2^V \rightarrow \{0,1\}$, given by
$$g(S) = f(S)/2 = \begin{cases}
1, &\text{if } 1\leq |S| < 100 \\
0, &\text{otherwise.}
\end{cases}$$  
The integer program $(IP)$ for $g$ given by:
\begin{align*}
\min\ \sum_{e\in E} c(e) x_e&\\
\text{s.t.}\  \sum_{e\in\delta(S)} x_e &\geq g(S) &\forall S \subseteq V \\
x&\geq 0 \\
x\in \Z^{|E|}
\end{align*}
describes the problem of finding a forest where each connected component is a tree with at least $100$ vertices. From our choice of $g$ it is clear that the connected components of any feasible solution must have at least $100$ vertices, as the cut defined by any smaller component does not satisfy the inequality for $g$. Optimal solutions to $(IP)$ do not have cycles, as we can remove any single edge present in a cycle and preserve connectivity.
\paragraph{}
Now we observe that $g$ satisfies property (\ref{eq:1}) from question $1$. Let $S \subseteq V$ and let $A, B \subseteq S$ such that $A \cap B = \emptyset$. Suppose that $g(S)=g(A) = g(B) = 0$. If $g(S\backslash A) = 1$ then $1 \leq |S\backslash A| < 100$. But $B \subseteq S\backslash A$, and hence $|B| \leq |S\backslash A|$. Since $g(B) = 0$ this means $|B| = 0$, that is $B = \emptyset$. Thus $g(S\backslash B) = g(S) = 0$. Similarly we can show if $g(S\backslash B) = 1$ then $g(S\backslash A) = 0$.  Hence property $(\ref{eq:1})$ is satisfied by $g$.
\paragraph{}
Consider the following algorithm, $\cA$, for $(E100)$:
\begin{enumerate}
\item Set $S = \emptyset$.
\item Solve $(IP)$ for $g$ using the algorithm of question $1$, obtaining a forest $F$.
\item For each connected component $T$ of $(V,F)$:
	\begin{enumerate}
	\item Compute a matching $M$ on the odd vertices of $T$.
	\item Shortcut $T\cup M$ obtaining an Euler Tour $A$.
	\item Set $S = S \cup A$.
	\end{enumerate}
\item Return $S$.
\end{enumerate}
\paragraph{}
We claim that $\cA$ is a $2$-approximation algorithm for $(E100)$. In question $1$ we showed that step $2$ can be done in polynomial time. In the lecture notes we have shown that the shortcutting process of each iteration of step $3$ can be done in polynomial time. Further there are a polynomial number of components the shortcutting is done on, so step $3$ also runs in polynomial time. Therefore $\cA$ returns a solution in polynomial time. The solution is feasible since each component is a cycle (hence is eulerian) on at least $100$ vertices.
\paragraph{}
We now demonstrate the approximation factor holds. We know from lecture that the shortcutting process of step $3$ produces a set of cycles of cost at most twice that of the original trees in a metric graph. That is,
$$c(A)\leq 2c(F).$$
Let $y$ be the optimal dual solution produces by the primal-dual algorithm of step $1$. We have from question $1$ that
$$c(F) \leq 2\sum_{S\subseteq V} g(S) y_S = \sum_{S\subseteq V} f(S) y_S \leq OPT.$$
where $OPT$ is the optimal solution to $(LP)$. This follows since the dual to $(LP)$ (with respect to $(E100)$) has the same feasible region as the dual to the linear programming relaxation of $(IP)$. Combining inequalities we see that
$$c(A) \leq 2c(F) \leq 2 \cdot OPT.$$
Hence $\cA$ is a $2$-approximation algorithm for $(E100)$.$\blacksquare$

%Q3
\section{}
\paragraph{}
Consider again the problem $(IP)$, associated with a graph $G=(V,E)$ with edges costs $c: E \rightarrow \R_+$ given by:
\begin{align*}
\min\ \sum_{e\in E} c(e) x_e&\\
\text{s.t.}\  \sum_{e\in\delta(S)} x_e &\geq f(S) &\forall S \subseteq V \\
x&\geq 0 \\
x&\in \Z^{|E|}.
\end{align*}
Suppose that $f:2^V \rightarrow \N$ is symmetric, meaning for all $S\subseteq V$:
$$f(S) = f(V\backslash S)$$
and that $f$ is subadditive, meaning for all $A,B \subseteq V$:
$$f(A\cup B) \leq f(A) + f(B).$$
We also suppose that the graph $G$ is a cycle on $n$ vertices.
%3a
\subsection*{a}
\paragraph{}
We begin by working towards a compact formulation for $(IP)$ on $G$. Since $G$ is a cycle we may label the vertices of $G$ as $V(G) = \{v_1,\dots, v_n\}$ such that $v_iv_j \in E(G)$ if and only if $$j \equiv i\pm1\mod n.$$ For $v_i, v_j \in V(G)$ we define the path from $v_i$ to $v_j$ as 
$$P(i, \ell) := \{v_i, v_{i+1}, \dots, v_{j} \}$$
where addition is taken modulo $n$. We define the family of all paths on $G$ as
$$\cP := \{P(v_i, v_j) : v_i, v_j \in V(G)\}.$$
Observe that the size of $\cP$ is polynomial in $n$, in particular $|\cP| = O(n^2)$ (since there are $n$ choices for $v_i$ and $n$ choices for $v_j$).
\begin{lemma}\label{lemma:compact}
The problem $(IP)$ on $G$ is equivalent to the compact integer program $(\overline{IP})$ given by:
\begin{align*}
\min\ \sum_{e\in E} c(e) x_e&\\
\text{s.t.}\  \sum_{e\in\delta(P)} x_e &\geq f(P) &\forall P \in \cP \\
x&\geq 0 \\
x&\in \Z^{|E|}.
\end{align*}
\end{lemma}
\begin{proof}
Since the objective functions are the same, we need only demonstrate that the feasible regions of $(IP)$ and $(\overline{IP})$ are equal. Since each $P \in \cP$ is a subset of $V$, it is clear that the feasible region of $(IP)$ is a subset of the feasible region of $(\overline{IP})$. So we just have to show the reverse containment.
\paragraph{}
Let $x$ be a feasible solution for $(\overline{IP})$. Let $S \subseteq V$. If $S = V$ then $f(S)= f(V)=0\leq x(\delta(S)) $ (otherwise $(\overline{IP})$ and $(IP)$ are infeasible). By symmetry of $f$ the same holds for $S =\emptyset$. Thus we may assume $\emptyset \subset S \subset V$. If we consider the connected components of $G[S]$ we observe that they are all paths. Hence there exist disjoint $P_1, \dots, P_k \in \cP$ such that
$$S = P_1 \dot\cup \dots \dot\cup P_k$$
where each $P_i$ corresponds to a connected component of $G[S]$. Now we see that
\begin{align*}
f(S) &= f(P_1 \dot\cup \dots \dot \cup P_k) \\
&\leq f(P_1) + \dots + f(P_k) &\text{by subadditivity} \\
&\leq x(\delta(P_1)) + \dots x(\delta(P_k)) &\text{by feasibility of $x$}.
\end{align*}
We claim that the edges incident to each path are disjoint. To see this, suppose that we have $P_i$ and $P_j$ with $\delta(P_i) \cap \delta(P_j) \neq \emptyset$. Let $v_iv_j \in \delta(P_i) \cap \delta(P_j)$. Since $P_i$ and $P_j$ are disjoint we may assume $v_i \in P_i \backslash P_j$ and $v_j \in P_j \backslash P_i$. But then $P_i$ and $P_j$ are connected by $v_iv_j$ violating that they are distinct connected components. So the claim holds. Thus we observe
\begin{align*}
f(S) &\leq x(\delta(P_1)) + \dots x(\delta(P_k)) \\
&= x(\bigcup_{i=1}^k \delta(P_i)) &\text{by disjointness}\\
&= x(\delta(S)) &\text{by disjointness}.
\end{align*}
Therefore $x$ satisfies the cut constraint for all $S$, and hence $x$ is feasible for $(IP)$.
\end{proof}
\paragraph{}
Hence by lemma \ref{lemma:compact} it is equivalent for us to solve $(\overline{IP})$. Now we define a linear programming relaxation for $(\overline{IP})$, which we will denote $(\overline{LP})$:
\begin{align*}
\min\ \sum_{e\in E} c(e) x_e&\\
\text{s.t.}\  \sum_{e\in\delta(P)} x_e &\geq f(P) &\forall P \in \cP \\
x&\geq 0.
\end{align*}
Since $(\overline{LP})$ has a polynomial number of constraints and variables it can be solved in polynomial time. We now define our rounding-based approximation algorithm, $\cA$, for $(\overline{IP})$:
\begin{enumerate}
\item Solve $(\overline{LP})$ on $G$  returning solution $\bar{x}$.
\item Let $\bar{x}_I \in \Z^{|E|}$ denote the integral part of $\bar{x}$. Let $\bar{x}_F \in [0,1)^{|E|}$ denote the fractional part of $\bar{x}$. Thus $\bar{x} = \bar{x}_I + \bar{x}_F.$
\item Construct solution $x \in \Z^{|E|}$ by rounding. For each $e \in E$ set:
$$x_e = \begin{cases}
\bar{x}_I + 1, &\text{if } x_F \geq 0.5 \\
\bar{x}_I, &\text{otherwise.}
\end{cases}$$
\item Return solution $x$.
\end{enumerate}
\begin{lemma}\label{lemma:feasible-rounding}
If $\bar{x}$ is a feasible solution to $(\overline{LP})$ then the $x$ obtained from rounding $\bar{x}$ in step $3$ of $\cA$ is a feasible solution to $(\overline{IP})$.
\end{lemma}
\begin{proof}
It is immediate from the description of step $3$ that $x$ is integral and non-negative. We need only verify the cut constraints. Let $P \in \cP$. Let $v_i, v_j \in P$ be the start and end vertices of $P$. That is,
$$\delta(P) = \{v_{i-1}v_i,  v_jv_{j+1}\}.$$
For convenience let $e_i = v_{i-1}v_i$ and let $e_j = v_jv_{j+1}$. Since $\bar{x}$ is feasible for $(\overline{LP})$:
$$ f(P)\leq \bar{x}(e_i) + \bar{x}(e_j) = \bar{x}_I(e_i) + \bar{x}_F(e_i)  + \bar{x}_I(e_j) + \bar{x}_F(e_j).$$
Since $f$ maps to $\N$ we have:
$$f(P) \leq \floor{\bar{x}_I(e_i) + \bar{x}_F(e_i)  + \bar{x}_I(e_j) + \bar{x}_F(e_j)}.$$
We consider the following cases: Case 1: $\bar{x}_F(e_i) + \bar{x}_F(e_j) < 1$ and Case 2: $\bar{x}_F(e_i) + \bar{x}_F(e_j) \geq 1$.
\paragraph{}
In Case 1 the fractional parts vanish during the floor-ing operation and hence
$$ f(P) \leq \bar{x}_I(e_i) + \bar{x}_I(e_j)  \leq x(e_i) + x(e_j).$$
In Case 2 we observe that $x_F(e_i) + x_F(e_j) = 1 + \alpha$ for some $\alpha \in [0,1)$. Thus
$$f(P) \leq \floor{\bar{x}_I(e_i) + \bar{x}_F(e_i)  + \bar{x}_I(e_j) + \bar{x}_F(e_j)} = \bar{x}_I(e_i) + \bar{x}_I(e_j)  + 1.$$
Since $\bar{x}_F(e_i) + \bar{x}_F(e_j) \geq 1$ at least one of $\bar{x}_F(e_i) ,\bar{x}_F(e_j) $ is greater than or equal to $0.5$. Assume without loss of generality $\bar{x}_F(e_i) \geq 0.5$. Then $x_(e_i) = \bar{x}_I(e_i) + 1$. So
$$f(P) \leq \bar{x}_I(e_i) + \bar{x}_I(e_j)  + 1 = x(e_i) +\bar{x}(e_j) \leq x(e_i) + x(e_j).$$
Therefore in either case $x$ satisfies the cut constraint for $P$. Therefore $x$ is feasible for $(\overline{IP})$.
\end{proof}
\begin{theorem}
The algorithm $\cA$ is a $2$-approximation algorithm for $(\overline{IP})$.
\end{theorem}
\begin{proof}
\paragraph{}
As we have argued above, $(\overline{LP})$ can be solved in polynomial time, so step $1$ can be done in polynomial time. Step $3$ can be done in $O(|E|)$ time. Hence $\cA$ terminates in polynomial time. From lemma \ref{lemma:feasible-rounding} we see that $x$ is feasible for $(\overline{IP})$. It remains to check the approximation factor.
\paragraph{}
Let $e \in E$. If $\bar{x}_F(e) < 0.5$ then we have immediately $x(e) \leq \bar{x}(e)$ and so trivially we have $x(e) \leq 2\bar{x}(e)$. It remains to consider the pertinent case when $\bar{x}_F(e) \geq 0.5$. Therein we have
\begin{align*}
\frac{x(e)}{\bar{x}(e)} &= \frac{\bar{x}_I(e) + 1}{\bar{x}_I(e) + \bar{x}_F(e)} \\
&\leq  \frac{\bar{x}_I(e) + 1}{\bar{x}_I(e) +0.5} \\
&= \frac{2\bar{x}_I(e) + 2}{2\bar{x}_I(e) + 1}.
\end{align*}
Since $\frac{2\bar{x}_I(e) + 2}{2\bar{x}_I(e) + 1}$ is a non-increasing function, it is maximized when $\bar{x}_I(e) = 0$. Hence
$$\frac{2\bar{x}_I(e) + 2}{2\bar{x}_I(e) + 1} \leq \frac{2\cdot 0 + 2}{2\cdot 0 + 1} = 2.$$
Thus $x(e) \leq 2\bar{x}(e)$ as desired. Therefore
$$\sum_{e\in E}c(e) x(e) \leq \sum_{e \in E}c(e)2\bar{x}(e) = 2\sum_{e\in E}c(e)\bar{x}(e) \leq 2\cdot OPT,$$
where $OPT$ is the cost of an optimal solution to $(\overline{IP})$ (the last inequality follows immediately from $\bar{x}$ being optimal for $(\overline{LP})$. So $\cA$ is a $2$-approximation algorithm.
\end{proof}
%3b
\subsection*{b}
\paragraph{}
We make an observation about optimal solutions to $(\overline{IP})$ which contain a $0$-valued edge. This will lead us to an approximation algorithm with a better bound than the one found in $3(a)$.
\begin{lemma}\label{lemma:0edge}
Let $x$ be an optimal solution $(\bar{IP})$. Suppose there exists $v_iv_{i+1} \in E(G)$ such that $x(v_iv_{i+1}) = 0$. Reindex (by subtracting $i$ from each index) $V(G)$ so that $i = 0$. That is $x(v_0v_1)= 0$. Then for all $v_jv_{j+1} \in E(G)$, $j \neq 0$,
$$x(v_jv_{j+1}) = f(P(v_1, v_j)).$$
\end{lemma}
\begin{proof}
For each $v_jv_{j+1} \neq v_0v_{1} \in E(G)$. Since $x$ is feasible and $x(v_0v_{1}) = 0$,
$$f(P(v_1, v_j)) \leq x(\delta(P(v_1,v_j))) = x(v_jv_{j+1}) + x(v_0v_{1}) = x(v_jv_{j+1}).$$
Suppose for a contradiction there exists $e_j = v_{j}v_{j+1} \in E(G)$ such that
$$f(P(v_1,v_j)) < x(v_jv_{j+1}).$$
Since $f(V) = 0$, $j\neq 0$. Let $x' \in \Z^{|E|}$ we given by
$$x'(e) = \begin{cases}
x(e), &\text{ if $e\neq v_jv_{j+1}$} \\
f(P(v_1,v_{j})), &\text{if $e=v_jv_{j+1}$}. 
\end{cases}$$
We claim that $x'$ is feasible for $(\overline{IP})$. By the feasibility of $x$, we need only check cut constraints involving $v_jv_{j+1}$. Let $v_i \in V(G)$, such that $i \neq j-1$. We consider $P(v_i,v_j)\in \cP$.
\paragraph{}
First consider the case where $0 < i \leq j$. In this case $v0,v_1 \in P(v_{j+1}, v_{i-1})$. We compute:
\begin{align*}
f(P(v_i,v_j)) &= f(P(v_{j+1}, v_{i-1})) &\text{by symmetry} \\
&\leq f(P(v_{j+1}, v_0)) + f(P(v_{1},v_{i-1})) &\text{by subadditivity} \\
&= f(P(v_1, v_j)) + f(P(v_1, v_{i-1})) &\text{by symmetry} \\
&= x'(v_jv_{j+1}) + f(P(v_1, v_{i-1})) &\text{by definition of $x'$} \\
&\leq x'(v_jv_{j+1}) + x(v_1v_{i-1}) +x(v_0v_1) &\text{by feasibility of $x$} \\
&= x'(v_jv_{j+1}) + x'(v_1v_{i-1}) &\text{by definition of $x'$, and $x(v_0v_1)=0$} \\
&=x'(\delta(P(v_i,v_j))).
\end{align*}
Hence $x'$ satisfies the cut constraint.
\paragraph{}
Now consider the case where $i > j$. In this case $v_0,v_1 \in P(v_i,v_j)$. We compute (similarly):
\begin{align*}
f(P(v_i,v_j))&\leq f(P(v_{i}, v_0)) + f(P(v_{1},v_{j})) &\text{by subadditivity} \\
&= f(P(v_1, v_{i-1})) + f(P(v_1, v_{j})) &\text{by symmetry} \\
&= f(P(v_1,v_{i-1})) + x'(v_jv_{j+1}) &\text{by definition of $x'$} \\
&\leq x(v_1v_{i-1}) + x(v_0v_1) + x'(v_jv_{j+1}) &\text{by feasibility of $x$} \\
&= x'(v_jv_{j+1}) + x'(v_1v_{i-1}) &\text{by definition of $x'$, and $x(v_0v_1)=0$} \\
&=x'(\delta(P(v_i,v_j))).
\end{align*}
Therefore in either case $x'$ is feasible. But
\begin{align*}
\sum_{e\in E} c(e) x(e) &= c(v_jv_{j+1})x(v_jv_{j+1}) + \sum_{e\in E\backslash\{v_jv_{j+1}\}} c(e)x(e) \\
&> c(v_jv_{j+1})x'(v_jv_{j+1}) +  \sum_{e\in E\backslash\{v_jv_{j+1}\}} c(e)x(e) &\text{since $x'(v_jv_{j+1}) = f(P(v_1,v_j) < x(v_jv_{j+1})$} \\
&= c(v_jv_{j+1})x'(v_jv_{j+1}) + \sum_{e\in E\backslash\{v_jv_{j+1}\}} c(e)x'(e) &\text{by our choice of $x'$} \\
&= \sum_{e\in E} c(e) x'(e),
\end{align*}
contradicting the optimality of $x$.
\end{proof}
\paragraph{}
This lemma tells us precisely how to construct optimal solutions to $(\overline{IP})$ in the presence of $0$-valued edges. The idea for our algorithm is to construct from this lemma each potential solution with a $0$-valued edge, as well as an approximate solution with no $0$-valued edges, and return the best possible of those $|E|+1$ solutions.
\paragraph{}
In our algorithm we will use the following linear program, which is $(\overline{LP})$ with the constraint $x \geq 1$ instead of $x\geq 0$. We define the linear program $(\tilde{LP})$ as
\begin{align*}
\min\ \sum_{e\in E} c(e) x_e&\\
\text{s.t.}\  \sum_{e\in\delta(P)} x_e &\geq f(P) &\forall P \in \cP \\
x&\geq 1.
\end{align*}
\paragraph{}
Consider the following improved algorithm for solving $(\overline{IP})$ which we will denote by $\cA'$:
\begin{enumerate}
\item Let $E' = \emptyset$. For each $e=v_iv_{i+1} \in E$:
	\begin{enumerate}
	\item Construct the solution $x^{e}$ as follows.
	\item Set $x^{e}(e)=0$.
	\item For each $v_jv_{j+1} \neq e$ Set $x^{e}(v_jv_{j+1}) = f(P(v_{i+1},v_j))$.
	\item If $x^{e}$ is feasible for $(\overline{IP})$ add $e$ to $E'$.
	\end{enumerate} 
\item Solve $(\tilde{LP})$ on $G$  returning solution $\bar{x}$.
\item Let $\bar{x}_I \in \Z^{|E|}$ denote the integral part of $\bar{x}$. Let $\bar{x}_F \in [0,1)^{|E|}$ denote the fractional part of $\bar{x}$. Thus $\bar{x} = \bar{x}_I + \bar{x}_F.$
\item Construct solution $x \in \Z^{|E|}$ by rounding. For each $e \in E$ set:
$$x_e = \begin{cases}
\bar{x}_I + 1, &\text{if } x_F \geq 0.5 \\
\bar{x}_I, &\text{otherwise.}
\end{cases}$$
\item Return $\arg\min\{c^Tx : x \in \{x^e : e \in E'\} \cup \{x\}\}$.
\end{enumerate}
\begin{theorem}
The algorithm $\cA'$ is a $\frac{4}{3}$-approximation algorithm for $(\overline{IP})$.
\end{theorem}
\begin{proof}
Step $1$ can be done in $O(|E|)$ iterations, each using $O(|E|)$ calls to the (we may assume polynomial time) oracle for $f$, and hence runs in polynomial time. Step $2$ can be done in polynomial time via the same argument used for $(\overline{LP})$. Step $4$ can be done in $O(|E|)$ time, as can step $5$. Therefore $\cA'$ runs in polynomial time. 
\paragraph{}
By definition, for each $e \in E'$, $x^{e}$ is feasible. Since the feasible region of $(\tilde{LP})$ is a subset of the feasible region of $(\overline{LP})$, $\bar{x}$ is feasible for $(\overline{LP})$ and therefore by lemma \ref{lemma:feasible-rounding} $x$ is feasible for $(\overline{IP})$ as Step $4$ of $\cA'$ is the same rounding procedure as step $3$ of $\cA$. Hence $\cA'$ returns a feasible solution for $(\overline{IP})$.
\paragraph{}
If there exists an optimal solution $x^*$ for $(\overline{IP})$ and $e \in E$ such that $x^*(e) = 0$ then $x^* = x^{e}$ by lemma \ref{lemma:0edge} and $e \in E'$. Thus in step $5$, $\cA'$ returns a solution of cost at most that of $x^{e} = x^*$ which is therefore optimal. Trivially such a solution is of cost at most $\frac{4}{3}$ that of optimal.
\paragraph{}
It remains to consider the case where no optimal solution $x^*$ for $(\overline{IP})$ has $e\in E$ such that $x^*(e) = 0$. Since $x^*$ is integral and no component is zero optimal solutions for $(\overline{IP})$ satisfy the constraint $x\geq 1$. Therefore $(\tilde{LP})$ is a relaxation of $(\overline{IP})$ in this case.
\paragraph{}
Now it suffices to show that the returned solution $x$ has cost at most $\frac{4}{3}$ that of $\bar{x}$. Let $e \in E$. If $\bar{x}_F(e) < 0.5$ then we have immediately $x(e) \leq \bar{x}(e)$ and so trivially we have $x(e) \leq \frac{4}{3}\bar{x}(e)$. It remains to consider the pertinent case when $\bar{x}_F(e) \geq 0.5$. Therein we have
\begin{align*}
\frac{x(e)}{\bar{x}(e)} &= \frac{\bar{x}_I(e) + 1}{\bar{x}_I(e) + \bar{x}_F(e)} \\
&\leq  \frac{\bar{x}_I(e) + 1}{\bar{x}_I(e) +0.5} \\
&= \frac{2\bar{x}_I(e) + 2}{2\bar{x}_I(e) + 1}.
\end{align*}
Since $\frac{2\bar{x}_I(e) + 2}{2\bar{x}_I(e) + 1}$ is a non-increasing function, it is maximized when $\bar{x}_I(e) = 1$ (since $\bar{x}_I(e) \geq 1$). Hence
$$\frac{2\bar{x}_I(e) + 2}{2\bar{x}_I(e) + 1} \leq \frac{2\cdot 1 + 2}{2\cdot 1 + 1} = \frac{4}{3}.$$
Thus $x(e) \leq \frac{4}{3}\bar{x}(e)$ as desired. Therefore
$$\sum_{e\in E}c(e) x(e) \leq \sum_{e \in E}c(e)\frac{4}{3}\bar{x}(e) = \frac{4}{3}\sum_{e\in E}c(e)\bar{x}(e)$$
and the approximation factor follows.
\end{proof}

%Q4
\section{}
\paragraph{}
Consider the packing problem $(P)$: Let $U$ be a set of elements. Let $\cS$ be a non-empty family of sets $S\subseteq U$. The goal is to find a subcollection of $\cS$ of maximum cardinality, subject to the constraint that each element is contained in at most $1$ set.
\paragraph{}
We can write a linear programming relaxation for $(P)$, denoted $(LP)$ as
\begin{align*}
\max \sum_{S\in\cS} x_S\ & \\
\text{s.t.} \sum_{S:e\in S} x_S &\leq 1 &\forall e\in U \\
x&\geq 0.
\end{align*}
Let $k = \max_{S\in \cS} \{ |S|\}$.
%pa
\subsection*{a}
\paragraph{}
Let $x$ be a basic optimal solution to $(LP)$. Suppose for a contradiction that for all $S \in \cS$, $x_S < \frac{1}{k}$. We will contradict the fact that the number of tight constraints with respect to $x$ is at least the number of variables, that is $|\cS|$. We will proceed by a token argument. Give a token to each $S \in \cS$. Redistribute the tokens according to the following rules, for each $S \in \cS$:
\begin{enumerate}
\item If $x_S = 0$ distribute the token to the non-negativity constraint corresponding to $S$.
\item Otherwise, for each $e \in S$ distribute $x_S$ token value to $e$.
\end{enumerate}
To achieve the desired contradiction, we need to show three things. First, that each $S \in \cS$ gives away at most $1$ token. Second, that each tight constraint receives at least $1$ token. Third, that some $S \in \cS$ keeps some fraction of their token. Together these propositions imply that $|\cS|$ is greater than the number of tight constraints at $x$. We complete the proof by showing these three porpositions.
\paragraph{}
We begin with the first proposition. If $x_S = 0$ then $S$ gives away at most one token by rule $1$. If $x_S > 0$ then $S$ gives away $\sum_{e: e\in S} x_S$ token value. Observe that
$$\sum_{e: e\in S} x_S < \sum_{e: e\in S} \frac{1}{k} = \frac{|S|}{k} \leq 1.$$
Thus $S$ gives away strictly less than one token. Thus to show proposition three we need only to show that there exists some $S$ with $x_S > 0$. This follows since $x$ is optimal. If all $S$ had $x_S = 0$, since $\cS \neq\emptyset$ we could simply set $x_S = 1$ for one $S \in \cS$ and increase cardinality, contradicting the optimality of $x$. Hence some $S$ exists with $x_S > 0$. Therefore we have show propositions one and three.
\paragraph{}
It remains to verify proposition two: that each tight constraint receives at least $1$ token. It is easy to see by rule $1$ that each tight non-negativity constraint receives $1$ token. Now consider a tight constraint corresponding to some $e \in U$. By rule two $e$ receives
$$\sum_{S : e\in S, x_S > 0} x_S$$
token value. But this is equal to
$$\sum_{S: e\in S} x_S$$
which in turn, since $e$ corresponds to a tight constraint, satisfies
$$\sum_{S: e\in S} x_S = 1.$$
That is, $e$ receives $1$ token. Therefore every tight constraint receives $1$ token and this completes the proof. $\blacksquare$
%pb
\subsection*{b}
\paragraph{}
Consider the following iterative rounding algorithm, $\cA$, for packing problem $(P)$:
\begin{enumerate}
\item Set $H = \emptyset$, $\cS' = \cS$, and $U' = U$.
\item While $\cS' \neq \emptyset$ do:
	\begin{enumerate}
	\item Let $x$ be a basic optimal solution to $(LP)$ on $\cS'$ and $U'$.
	\item Let $S \subseteq \cS'$ such that $x_S \geq \frac{1}{k}$ where $k = \max_{S' \in \cS'}\{|S'|\}$ .
	\item Set $H = H \cup \{S\}$.
	\item For each $S' \in \cS'$ such that $S' \cap S \neq \emptyset$, remove $S'$ from $\cS'$.
	\item For each $e \in S$, remove $e$ from $U'$. 
	\end{enumerate}
\item Return $H$.
\end{enumerate}
\begin{theorem}
The algorithm $\cA$ is a $\frac{1}{k-1+\frac{1}{k}}$-approximation algorithm for $(P)$.
\end{theorem}
\begin{proof}
\paragraph{}
To see that $\cA$ terminates observe that by problem $3(a)$, a set $S \subseteq \cS'$ is always found in step $2(b)$, and hence $\cS'$ is eventually empty. To verify that $\cA$ runs in polynomial time, observe that $(LP)$ has a constraint for every $e \in U$ and a variable for every $S \in \cS$. Thus the size of $(LP)$ is polynomial in the size of the input to the problem. Each subsequent $(LP)$ is smaller than the previous, thus $2(a)$ runs in polynomial time. Step $2(b)$ can be done in $O(|\cS|)$ time, $2(c)$ in constant time, $2(d)$ in $O(|\cS|^2)$ time, and step $2(e)$ in $O(k|U|)$ time. These are all very pessimistic estimates,  but in any case they show that each iteration of step $2$ runs in polynomial time. Step $2$ has at most $|\cS|$ iterations and hence runs in polynomial time. Thus $\cA$ terminates in polynomial time.
\paragraph{}
Now we verify that the solution returned is feasible for $(P)$. We claim that $\cA$ maintains the invariant that after each iteration of step $2$, for each $S \in \cS'$ and each $T \in H$, $S\cap T = \emptyset$. It is easy to see that this invariant holds before step $2$. Suppose it holds inductively at the beginning of some iteration of step $2$. Let $S$ be the set chosen to enter $H$ in step $2(c)$. By step $2(d)$ each $S' \in \cS'$ that intersects $S$ is removed from$ \cS'$. Since no set in $\cS'$ intersects sets in $H$ by induction, after step $2(d)$ no set in $\cS'$ intersects $H \cup \{S\}$. Therefore the invariant holds. Using this invariant it is easy to see that at each iteration of step $2$ a set is added to $H$ which intersects no other set in $H$. Therefore at the termination of step $2$ the set $H$ returned is feasible for $(P)$.
\paragraph{}
It remains to demonstrate the approximation factor. We will bound the increase in $H$ versus the decrease in the fractional solution at each iteration of step $2$. Let $\Delta_i H$ denote the change in $|H|$ from the beginning of iteration $i$ to the end of iteration $i$. Exactly one set is added to $H$ in each iteration so $\Delta_i H = 1$ for all $i$. Let $x^i$ be the fractional solution to $(LP)$ found at the beginning of iteration $i$. Let $\Delta_i x = \sum_{S \in \cS}x^{i}_S - \sum_{e \in \cS}x^{i+1}_S$, where we take $x^{n+1} = 0$ if step $2$ terminates in $n$ iterations, and take $x^{i}_S = 0$ if $S \not\in \cS'$ during iteration $i$.
\paragraph{}
Consider an arbitrary iteration of step $2$ of $\cA$. Let $x^i$ be the solution to $(LP)$ found in step $2(a)$. Let $S$ be the set chosen in step $2(b)$. For each $e \in S$, let $\cS^e = \{S' \in \cS'\backslash\{S\} : e \in S'\}$. Let the collection of deleted sets from step $2(d)$ be $D: = \bigcup_{e\in S} \cS^e$. Then
$$x(D) \leq \sum_{e \in S} x(\cS^{e}) = \sum_{e\in S} \sum_{S' \in \cS'\backslash\{S\} : e\in S'} x_{S'} \leq \sum_{e \in S} (1 - x_S) = |S|(1-x_S),$$
with the last inequality following from the corresponding packing constraints of $(LP)$ for each $e$. Now we observe that $x^{i}$ restricted to the variables of $\cS' \backslash (\{S\} \cup D)$ is feasible for $(LP)$ during iteration $i+1$. Hence $$\Delta_i x \leq \sum_{S'\in \cS'} x^i_{S'} - \sum_{S' \in \cS'\backslash (\{S\} \cup D)\}} x^i_{S'} = x_S + x(D).$$
Thus by our bound on $x(D)$ we have
\begin{align*}
\Delta_i x &\leq x_S + |S|(1-x_S) \\
&= 1 - (1-x_S) + |S|(1-x_S) \\
&= 1 + (|S| -1)(1-x_S) \\
&\leq 1+(k - 1)(1-x_S) &\text{by our choice of $k$ in step $2(b)$} \\
&\leq 1 + (k-1)(1-\frac{1}{k}) &\text{by our choice of $x_S$} \\
&= 1 + k - 1 - 1 + \frac{1}{k} \\
&= k-1 + \frac{1}{k}.
\end{align*}
So we observe that $$\Delta_i H - \frac{1}{k-1+\frac{1}{k}}\Delta_i x \geq 0.$$
Summing over each iteration $i$ (and taking $k$ to be the initial $k$ from the first iteration, since $k$ is non-increasing as the algorithm proceeds) we obtain
$$\sum_i \Delta_i H - \frac{1}{k-1+\frac{1}{k}} \sum_i \Delta_i x \geq 0.$$
Since $\sum_i \Delta_i H = |H|$, and $\sum_i \Delta_i x = \sum_i (x^i - x^{i+1}) = x^1$, we obtain
$$|H| - \frac{1}{k-1+\frac{1}{k}}x^1 \geq 0.$$
Since $x^1$ is an optimal solution to $(LP)$ for our original problem $(P)$ this implies that $|H|$ is at least $\frac{1}{k-1+\frac{1}{k}}$ times the cardinality of an optimal solution to $(P)$.
\end{proof}

%Q5
\section{}
\paragraph{}
When I refer to ``the paper" below, I mean Dr. Sanita's paper she gave us on Learn: Steiner Tree Approximation via Iterative Randomized Rounding.
\paragraph{}
Given a metric graph $G=(V,E)$ with edge costs $c:E\rightarrow \R_+$ and a set of terminals $R\subseteq V$, the $k$-restricted Steiner Tree problem asks for a tree of minimum cost spanning $R$ whose full components each contain at most $k$ terminals. From class the following algorithm was shown to be a $\ln(4)$-approximation algorithm for this problem.
\paragraph{}
Let $\cC$ denote the set of all directed components which can be obtained in the following way. Let $R' \subseteq R$ such that $|R'| \leq k$. and let $r' \in R'$. Let $C$ be the minimum cost Steiner tree on $R'$, and direct all edges of $C$ towards $r'$. Then $r'$ denotes the unique sink terminal in this case. A component $C \in \cC$ ``crosses" $U\subseteq R$ if $C$ has at least one source in $U$ and its unique sink in $R\backslash U$. The set $\delta^+(U)$ will denote the set of $C \in \cC$ which cross $U$. Fix an arbitrary root terminal $r \in R$. The directed cut relaxation $(k-DCR)$ is
\begin{align*}
\min \sum_{C\in \cC} c(C)x_C& \\
\text{s.t.} \sum_{C\in\delta^+(U)} x_C &\geq 1 &\forall U\subseteq R\backslash \{r\}, U\neq \emptyset \\
x&\geq 0.
\end{align*}
As argued in class this is a relaxation of the $k$-restricted Steiner tree problem. The iterative randomized rounding algorithm, $\cA$, for this problem is:
\begin{enumerate}
\item For $t =1,2,\dots$
\begin{enumerate}
\item Solve $(k-DCR)$ on $G$ to obtain $x^t$.
\item Sample one component $C^t$ where $C^t=C$ with probability $x^t_C/\sum_{C\in\cC} x^t_C$. Contract $C^t$ into its sink in $G$.
\item If a single terminal remains, return $\bigcup_{i=1}^t C^i$.
\end{enumerate}
\end{enumerate}
\paragraph{}
Since we already know from class that $\cA$ is a $\ln(4)$-approximation algorithm for $k$-restricted Steiner Tree, it only remains to verify the new approximation factor of $\frac{5}{4}$ when we fix $k=4$. Let $S^*$ be an optimal $4$-restricted Steiner Tree with undirected full components $Z_1, \dots, Z_\ell$. We will start by studying the structure of each $Z_i$. The first observation, made in the paper, is that we may assume Steiner Nodes of $S^*$ have degree at least $3$. This follows since $G$ is a metric graph we may shortcut any Steiner Node of degree two by connecting its endpoints and removing it. By the triangle inequality this shortcutting operation does not increase the cost of the solution.
\paragraph{}
We now move to observations about each $Z_i$.
\begin{lemma}\label{lemma:dk}
If $Z_i$ is a $k$-restricted component for any $k$, then the degree of any Steiner Node in $Z_i$ is at most $k$.
\end{lemma}
\begin{proof}
Suppose for a contradiction that $Z_i$ has a Steiner Node $s$ of degree greater than $k$. Then $s$ has at least $k+1$ neighbours: $v_1, \dots, v_{k+1}$. Since $Z_i$ is a tree, there is a unique path from $s$ to each of the $k$ terminals of $Z_i$, $r_1, \dots, r_k$. Each path $P_{s,r_j}$ can use only one of $v_1,\dots, v_{k+1}$. If they use more, say first $v_i$ and then next $v_j$ then the subpath $r,v_i,\dots,v_j$ with the edge $v_js$ forms a cycle in $Z_i$ contradicting that $Z_i$ is a tree. But then one of $v_1, \dots, v_{k+1}$ is not used on any path from $s$ to any of $r_1, \dots, r_k$. The edge from $s$ to the unused $v_j$ can then be removed from $Z_i$ without disconnecting the terminals, contradicting optimality.
\end{proof}
\paragraph{}
We now consider the classes of graphs these components appear as. It is an easy observation that $1$-restricted components only appear in the trivial case where $|R| = 1$. It is also easy to see that $2$-restricted components arise as a single edge connecting two terminals since Steiner Nodes have degree at least three. Further we see that $k$-restricted components with exactly one Steiner Node are star graphs centred at the Steiner Node. By lemma \ref{lemma:dk} $3$-restricted components have a Steiner Nodes with degree exactly three, and thus they can only have one Steiner Node. This implies that $3$-restricted components are star graphs. Things get slightly more interesting with $4$-restricted components.
\begin{lemma}\label{lemma:four}
A $4$-restricted component $Z_i$ is either a star graph or consists of exactly two adjacent degree three Steiner Nodes each adjacent to precisely two distinct terminals.
\end{lemma}
\begin{proof}
Suppose $Z_i$ is not a star graph. Then $Z_i$ has more than one Steiner Node. We claim that $Z_i$ has at most two adjacent Steiner Nodes. Suppose for a contradiction $Z_i$ has at least three adjacent Steiner Nodes. Say $s_1,s_2,s_3$ is a path of Steiner Nodes in $Z_i$. If the removal of edge $s_1s_2$ separates one terminal in the component containing $s_1$ then $s_1$ is adjacent to a redundant edge since its degree is at least three. So the edge removal separates at least two terminals in the component containing $s_1$. Similarly, by removing $s_2s_3$ we see that there are two terminals in the component containing $s_3$. But then, since $s_2$ of degree at least three, any third component it connects to (that is, not containing $s_1$ or $s_3$) is redundant to connect the terminals. This violates optimality. Hence $Z_i$ contains exactly two Steiner Nodes, and we see by the above argument that the removal of their adjacent edge separates two terminals into each connected component.
\end{proof}
\paragraph{}
We are prepared to analyze the algorithm. Following the notation in the paper we let $D(e) = \max\{t : e \in S^t\}$ be the iteration at which $e$ is deleted. We construct the witness tree via the following procedure:
\begin{enumerate}
\item Start with $\tilde{B} = \emptyset$
\item For each $i = 1,\dots, \ell$
	\begin{enumerate}
	\item If $Z_i$ is a $2$-restricted component add its edge to $\tilde{B}$.
	\item If $Z_i$ is a $3$-restricted component or $4$-restricted star graph component add all the edges of $Z_i$ but one chosen uniformly at random.
	\item If $Z_i$ is a $4$-restricted component with two adjacent Steiner Nodes $s_1,s_2$ and terminals $r_1, r_2$ connected to $s_1$ and terminals $r_3,r_4$ connected to $s_2$ then:
	\begin{enumerate}
		 \item add edge $s_1s_2$ to $\tilde{B}$
		 \item add one edge of $r_1s_1$, $r_2s_1$ to $\tilde{B}$ chosen uniformly at random
		 \item add one edge of $r_3s_2$, $r_4s_2$ to $\tilde{B}$ chosen uniformly at random
\end{enumerate}
	\end{enumerate}
\item Return Witness Tree $W= \{\{u,v\} \in {R\choose 2} : |P_{uv} \cap \tilde{B}| = 1\}$.
\end{enumerate}
\paragraph{}
By lemma \ref{lemma:four} and the preceding discussion, this procedure covers all possible forms the components $Z_i$ can take. Further it is immediate from our choice of $\tilde{B}$ that $W$ is a terminal spanning tree. A simple inspection of the edges taken in $W$ with respect to the terminals of each $Z_i$ verifies this.
\paragraph{}
For any $e \in E(S^*)$ we define $W(e) = \{ uv \in E(W): e \in P_{uv}\}$. When the edges of $W(e)$ are $\textit{marked}$ (see the paper) the edge $e$ is deleted. We have the following lemma from the paper/class (rephrased in terms of $W(e)$ for simplicity):
\begin{lemma}\label{lemma:20}
Let $e \in E(S^*)$. Then the expected number of iterations until all the edges of $W(e)$ are $\textit{marked}$ satisfies $$E[X(W(e))] \leq H_{|W(e)|} \cdot M,$$
where $M$ is an upperbound on $|\cC|$ polynomial in $n$.
\end{lemma}
\begin{lemma}\label{lemma:de}
For any edge $e \in E(S^*)$, the expected number of iterations until $e$ is deleted is
$$E[D(e)] \leq \frac{5}{4} M.$$
\end{lemma}
\begin{proof}
Let $Z_i$ be the undirected full component containing $e$. We proceed by case analysis on the class of graph $Z_i$ belongs to. We have the following cases:
\begin{enumerate}
\item $Z_i$ is $2$-restricted,
\item $Z_i$ is $3$ or $4$-restricted star,
\item $Z_i$ is a $4$-restricted component with two degree three Steiner Nodes.
\end{enumerate}
\paragraph{}
We begin with Case $1$. This case is trivial: with probability $1$, $e$ is the edge connecting the terminals. Thus by lemma \ref{lemma:20} $E[D(e)] = 1\cdot H_1 \cdot M = M \leq \frac{5}{4} M.$
\paragraph{}
Now we consider Case $2$. Suppose $Z_i$ is a $k$-restricted star where $k \in \{3,4\}$. Then with probability $\frac{1}{k}$ (when $e$ is the edge not in $\tilde{B}$), $|W(e)| = k-1$. With probability $(1-\frac{1}{k}$), $|W(e)| = 1$. Thus by lemma \ref{lemma:20} we have
\begin{align*}E[D(e)] &\leq \frac{1}{k} H_{k-1} M + (1-\frac{1}{k})H_1 M
\\&=\begin{cases}
(\frac{1}{3}H_2 + \frac{2}{3})M, &\text{if $k = 3$} \\
(\frac{1}{4}H_3 + \frac{3}{4})M, &\text{if $k=4$}.
\end{cases} \\
&=\begin{cases}
\frac{7}{6}M, &\text{if $k = 3$} \\
(\frac{29}{24})M, &\text{if $k=4$}.
\end{cases} \\
&\leq \frac{5}{4} M.
\end{align*}
\paragraph{}
Finally we consider Case $3$. By lemma \ref{lemma:four} we have the following structure.Let $s_1,s_2$ be the two adjacent Steiner Nodes of $Z_i$ and let $r_1,r_2$ be the terminals adjacent to $s_1$, and let $r_3,r_4$ be the terminals adjacent to $s_2$. Suppose without loss of generality that $s_1r_1, s_2r_4 \in \tilde{B}$. The the probability that $|W(e)| = 1$ is $\frac{3}{5}$ (such edges are $s_1r_1, s_2r_4$ and $s_1s_2$). The probability that $|W(e)| = 2$ is $\frac{2}{5}$ (such edges are $s_1r_2$ and $s_2r_3$). Thus by lemma \ref{lemma:20} we have
\begin{align*}
E[D(e)] &\leq \frac{3}{5}H_1M + \frac{2}{5}H_2M \\
&= (\frac{3}{5} + \frac{3}{5}) M \\
&= (\frac{6}{5})M \\
&\leq \frac{5}{4}M.
\end{align*} 
Therefore the expectation holds in each case as desired.
\end{proof}
\paragraph{}
We now finish by computing the expected cost of the solution returned by $\cA$. Let $opt^t_f$ denote the optimal fractional cost to $(k-DCR)$ at iteration $t$. Let $opt$ denote the cost of $S^*$.
\begin{theorem}
The algorithm $\cA$ is $\frac{5}{4}$-approximation algorithm for $4$-restricted Steiner Tree.
\end{theorem}
\begin{proof}
We observe that
\begin{align*}
E[\sum_{t\geq 1} c(C^t)] &\leq \sum_{t\geq 1} \frac{1}{M}E[opt^t_f] \\
&\leq \frac{1}{M} \sum_{t\geq 1} E[c(S^t)] \\
&=\frac{1}{M} \sum_{t\geq 1} E[D(e)]c(e) \\
&\leq \frac{1}{M} \sum_{t\geq 1} \frac{5}{4}M\cdot c(e) &\text{by lemma \ref{lemma:de}} \\
&=\frac{5}{4} \sum_{t \geq 1} c(e) \\
&= \frac{5}{4} opt.
\end{align*}
\end{proof}

%Q6
\section{}
\paragraph{}
In the metric Travelling Salesman Problem $(TSP)$ we are given a metric graph $G=(V,E)$ with edge costs $c: E\rightarrow \R_+$. The goal is to find a minimum cost Hamilton Cycle visiting each $v \in V$. The Held-Karp relaxation for $(TSP)$, denoted $(HK)$ is
\begin{align*}
\min \sum_e c(e) x_e& \\
\text{s.t.} x(\delta(v)) &= 2 &\forall v \in V \\
x(\delta(S)) &\geq 2 &\forall S \subseteq V \\
x &\geq 0.
\end{align*}
Let $n = |V|$. Let $x$ be a basic solution to $(HK)$. Our goal is to show that $x$ has $O(n)$ non-zero variables. The number of variables in $x$ correspond to a maximum size set of linearly independent tight constraints. Therefore the non-zero variables correspond to a collection $\cL \subseteq 2^V$ such that $x(\delta(S) = 2$ for all $S \in \cL$, and $\{\chi(S): S \in \cL\}$ is maximal linearly independent and $\chi(S)$ denotes the incidence vector of $S$. Hence we need to show that $|\cL| = O(n)$.
\paragraph{}
The constant function $f: 2^V \rightarrow \{2\}$ is trivially weakly supermodular. Therefore we may invoke a theorem from class saying we may choose $\cL$ to be a laminar family.
\paragraph{}
To show $|\cL| = O(n)$ we proceed by induction on $|V|$, the size of the ground set. The claim is trivial for $|V| = 1$: $|\cL| \leq |V|  = O(n)$. Suppose $|V| >1$. For the inductive hypothesis suppose for all $V' \subseteq V$, any laminar family on ground set $V'$ has size $O(|V'|)$. Let $S \in \cL\backslash\{V\}$ such that $|S|$ is maximal. We may assume such $S$ exists, as otherwise $|\cL| = 1 = O(n)$ as desired. Since $\cL$ is laminar, for all $S' \in \cL \backslash \{V\}$ either $S' \subseteq S$ or $S' \cap S = \emptyset$. Let $\cL' = \{ S' \in \cL\backslash\{V\} : S' \subseteq S\}$ and let $\cL'' = \{S' \in \cL\backslash \{V\} : S'\cap S \neq \emptyset\}$. Since $\cL'$ and $\cL''$ are subsets of $\cL$ they are both laminar families. The ground set of $\cL'$ is $S$, and the ground set of $\cL''$ is $V\backslash S$. Therefore by the inductive hypothesis $|\cL'| = O(|S|)$ and $|\cL''| = O(|V\backslash S|)$. Also, $\cL \subseteq \{V\} \cup \cL' \cup \cL''$. Then 
$$|\cL| \leq |\{V\}| + |\cL'| + |\cL''| = 1 + O(|S|) + O(|V\backslash S|) = O(|V\backslash S| + |S| + 1) = O(|V|).$$
Hence by induction $|\cL| = O(n)$ as desired. $\blacksquare$
\end{document}
