\documentclass[11pt]{amsart}
\usepackage{amsmath,amsthm,amsfonts,amssymb}
\usepackage{fullpage}
\usepackage[usenames]{xcolor}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newtheorem{proposition}{Proposition}
\theoremstyle{definition}
\newtheorem{definition}{Definition}
\newtheorem{exercise}{Exercise}
\newtheorem{remark}{Remark}
\newtheorem{problem}{Problem}
\newtheorem{challenge}{Challenge Problem}
\newtheorem{open}{Open Problem}
\theoremstyle{plain}

\newcommand{\argmin}{\mathrm{argmin}}
\newcommand{\calX}{\mathcal{X}}
\newcommand{\calY}{\mathcal{Y}}
\newcommand{\calZ}{\mathcal{Z}}
\newcommand{\cost}{\mathrm{cost}}
\newcommand{\dHam}{\mathrm{d}_{\mathrm{Ham}}}
\newcommand{\disc}{\mathrm{disc}}
\newcommand{\discu}{\mathrm{disc_u}}
\newcommand{\Disj}{\textsc{Disj}}
\newcommand{\Eq}{\textsc{Eq}}
\newcommand{\GHD}{\textsc{GHD}}
\newcommand{\GT}{\textsc{GT}}
\newcommand{\HD}{\textsc{HD}}
\newcommand{\ICext}{\mathrm{IC}^{\mathrm{ext}}}
\newcommand{\ICint}{\mathrm{IC}^{\mathrm{int}}}
\newcommand{\icostext}{\mathrm{icost}^{\mathrm{ext}}}
\newcommand{\icostint}{\mathrm{icost}^{\mathrm{int}}}
\newcommand{\IP}{\textsc{IP}}
\newcommand{\Index}{\textsc{Index}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\rank}{\mathrm{rank}}
\newcommand{\Rpriv}{R^{\mathrm{priv}}}
\newcommand{\replacethistext}[1]{\textcolor{red}{#1}}

\newcommand{\exercises}{\bigskip \noindent\rule{8cm}{0.4pt} \medskip}



\title{Communication complexity: Chapter 6 \\ One-way communication and simultaneous message passing}
\author{Eric Blais and Justin Toth}


\begin{document}

\maketitle

Every communication complexity question we have studied so far in this course have been set in the model where there are two players, Alice and Bob, who exchange bits with each other to compute functions on their joint inputs. This is of course not the only model of communication that is of interest---other models where we change the number of parties or restrict how messages can be sent between them have also been studied extensively. In this chapter, we examine the communication complexity of functions when the communication between Alice and Bob is restricted.



\newpage 
\section{One-way communication complexity}

The first modification to the standard communication complexity setting that we will consider is the \emph{one-way communication} model, where the only communication taking place during a protocol is from Alice to Bob; then Bob outputs the value of the function.

\begin{definition}[One-way protocol]
A communication protocol $\pi$ is a \emph{one-way protocol} if all the internal nodes except the ones right above the leaves in the rooted binary tree $T(\pi)$ are labelled with $A$.
\end{definition}

As in the standard two-way communication setting, we can consider deterministic, public-coin, and private-coin protocols.

\begin{definition}[One-way communication complexity]
The deterministic and (public-coin) randomized \emph{one-way communication complexities} of a function $f : \calX \times \calY \to \{0,1\}$, denoted
\[
D^{\rightarrow}(f) \qquad \mbox{and} \qquad R^{\rightarrow}(f)
\]
respectively, are the minimum cost of a deterministic one-way protocol that computes $f$ and of a public-coin randomized one-way protocol that computes $f$ with error at most $\frac13$, respectively.
\end{definition}

\begin{remark}
We can also define one-way analogues of the other models of communication such as distributional complexity, private-coin randomized complexity, etc.
\end{remark}

The communication complexity of some functions remains identical in the one-way and two-way settings.

\begin{theorem}
The one-way communication complexity of $\Eq : \{0,1\}^n \times \{0,1\}^n \to \{0,1\}$ satisfies
\[
D^\rightarrow(\Eq) = D(\Eq) = \Theta(n) \qquad \mbox{and} \qquad 
R^\rightarrow(\Eq) = R(\Eq) = \Theta(1).
\]
\end{theorem}

\begin{proof}
	
First we consider the deterministic setting. Clearly $n$ bits suffice to send Alice's input. Thus $D^\rightarrow(\Eq) = O(n)$. For the lower bound consider a deterministic one-way protocol $\pi$ computing $\Eq$. Let $\pi'$ be the two-way protocol where Alice and Bob simulate $\pi$ and in $1$ bit Bob sends the result to Alice. Clearly $\pi'$ solves $\Eq$, and its cost is
$$D^\rightarrow(\Eq) + 1.$$
Since $D(\Eq) = \Omega(n)$ this implies $D^\rightarrow(\Eq) = \Omega(n)$ as desired.

Now consider the randomized setting. $R^\rightarrow(\Eq) = \Omega(1)$ is immediate. For the upper bound, consider the following randomized protocol, which is entirely analogous to what we do in the two-way model. Alice and Bob generate $r_1, r_2 \in \{0,1\}^n$ independently and uniformly at random using their shared randomness. Alice computes $x\cdot r_1$ and $x\cdot r_2$ and sends the results to Bob. Bob computes $y\cdot r_1$ and $y\cdot r_2$ and outputs $1$ if and only if $x\cdot r_1 = y\cdot r_1$ and $x\cdot r_2 = y\cdot r_2$. The success probability of this protocol is $\frac{3}{4}$ following the same analysis as the two-way model.

\end{proof}


\newpage 
\section{Index function}

In general, however, the one-way and two-way complexity of a function can differ greatly. A simple example that highlights this difference is the \emph{index function} $\Index : \{0,1\}^n \times [n] \to \{0,1\}$ defined by
\[
\Index(x,i) = x_i.
\]

\begin{theorem}
The communication complexity of the $\Index : \{0,1\}^n \times [n] \to \{0,1\}$ function satisfies
\[
D(\Index) = O(\log n) \qquad \mbox{and} \qquad D^\rightarrow(\Index) = \Omega(n).
\]
\end{theorem}

\begin{proof}
Trivially $D(\Index) = O(\log n)$: Bob sends his input $y$ using $\log n$ bits, and Alice sends $x_y$ using $1$ bit.

To see that $D^\rightarrow(\Index) = \Omega(n)$, suppose for contradiction there is a one-way protocol $\pi$ which computes $\Index$, and Alice sends strictly less than $n$ bits. Let $m\in\{0,1\}^{|m|}$ be the message Alice sends to Bob. Our assumption says $|m| < n$. Since $\pi$ is deterministic, Bob's output is entirely a function of $m$ and his input $y$. Since $|m|<n$ there exist a pair of inputs $x^1$ and $x^2$ for Alice for which she sends the same message $m$. If $y$ is the index of a bit on which $x^1$ and $x^2$ differ, then $\pi$ will not be correct on both $(x^1,y)$ and $(x^2,y)$. On message $m$ and input $y$ Bob can output either $x^1_y$ or $x^2_y$. In either case, he is wrong on some input. Thus any determinsitc one-way protocol for $\Index$ must use at least $n$ bits of communication.
\end{proof}



\newpage 
\section{One-way vs.~two-way communication complexity}

The gap between the one-way and two-way communication complexity of the index function is the largest possible.

\begin{theorem}
For every function $f : \calX \times \calY \to \{0,1\}$,
\[
D^\rightarrow(f) \le 2^{D(f)}.
\]
\end{theorem}

\begin{proof}
Let $\pi$ be a two-way deterministic protocol computing $f$, and achieving cost $D(f)$. We construct a one-way protocol $\pi'$ for $f$ as follows. Alice considers the sub-tree of the protocol tree $\pi$ corresponding to what she would send on her input, and all of Bob's possible replies. The size of this sub-tree is at most $2^{D(f)}$. After reading this sub-tree, Bob outputs the bit at the leaf to which his input with Alice's communication corresponding.
\end{proof}



\newpage 
\section{Index II}

We can strengthen our lower bound on the one-way communication complexity of the index function to show that it holds in the public-coin randomness model as well.

\begin{theorem}
The one-way randomized communication complexity of the $\Index : \{0,1\}^n \times [n] \to \{0,1\}$ function is
\[
R^\rightarrow(\Index) = \Omega(n).
\]
\end{theorem}

\begin{proof}
Consider a one-way randomized communication protocol for $\Index$ with success probability $\frac{2}{3}$, achieving communication cost $R^\rightarrow(\Index)$. Let $X$ and $Y$ be random variables for Alice and Bob's inputs respectively, each distributed uniformly and independently at random. Let $M$ be a random variable for the message $m$ that Alice sends Bob during the operation of this protocol. Our goal is to use information-theoretic techniques to lower bound $|M|$, the maximum length of a message in $M$'s support. We have,
\begin{align*}
|M| &\geq H(M)\\
&\geq I(X;M) \\
&=\sum_{i=1}^n I(X_i; M \mid X_1\dots X_{i-1}) &\text{by chain rule} \\
&=\sum_{i=1}^n H(X_i\mid X_1\dots X_{i-1}) - H(X_i\mid M, X_1\dots X_{i-1})\\
&= n - \sum_{i=1}^n H(X_i\mid M, X_1\dots X_{i-1}) &\text{since the bits of $X$ are chosen uniformly independently} \\
&\geq n-\sum_{i=1}^n H(X_i\mid M).
\end{align*} 
We claim $H(X_i\mid M)\leq \frac{2}{3}$. With this claim in hand, we see that
$$|M| \geq n-\frac{2}{3}n =\Omega(n).$$
To see the claim, we use that the protocol succeeds with probability $\frac{2}{3}$. We expand the conditional entropy as an expectation and compute
$$H(X_i\mid M) = \sum_{m\in M} H(X_i\mid M=m)\cdot \Pr(M=m) = \sum_{m\in M}(\frac{2}{3}\log(\frac{3}{2}) + \frac{1}{3}\log(3))\Pr(M=m) \leq \frac{2}{3}.$$
The second inequality follows by the success probability of the protocol. The probability that the protocol say $X_i$ is the correct bit is $\frac{2}{3}$, and the probability that $X_i$ is the wrong bit is $\frac{1}{3}$.
\end{proof}



\newpage 
\section{Disjointness}

The lower bound on the index function can be used to obtain a simple proof of the $\Omega(n)$ lower bound on the one-way randomized communication complexity of the disjointness function.

\begin{theorem}
The one-way randomized communication complexity of the $\Disj : \{0,1\}^n \times \{0,1\}^n \to \{0,1\}$ function is
\[
R^\rightarrow(\Disj) = \Omega(n).
\]
\end{theorem}

\begin{proof}
Consder any randomized one-way protocol $\pi$ which solves $\Disj$.. We will construct a protocol for $\Index$ of the same cost, and apply the lower bound from the previous section to obtain the result. Let $x \in \{0,1\}^n$ and let $y \in [n]$ be Alice and Bob's respective inputs to $\Index$. They run their $\Disj$ oracle $\pi$ as if Alice input were $x$ and Bob's was the standard basis vector $e_y\in\{0,1\}^n$, where only the $y$-th bit $e_y$ is $1$. Since
$$\Disj(x,e_y) = \Index(x,y)$$ 
this protocol solves $\Index$ using the same communication cost as $\pi$.
\end{proof}

\begin{remark}
Your proof should \emph{not} use our previous lower bounds on the two-way randomized communication complexity of the disjointness function; instead, you should be able to use the lower bound on the one-way communication complexity of the index function to get this result.
\end{remark}



\newpage 
\section{Gap Hamming (Bonus)}

Define the \emph{Hamming distance} between strings $x,y \in \{0,1\}^n$ to be $\dHam(x,y) = |\{i \in [n] : x_i \neq y_i\}|$.
The \emph{gap Hamming function} $\GHD : \{0,1\}^n \times \{0,1\}^n \to \{0,1,*\}$ is the partial function defined by
\[
\GHD(x,y) = \begin{cases}
1 & \mbox{if } \dHam(x,y) = \frac n2 \\
0 & \mbox{if } |\dHam(x,y) - \frac n2| \ge \sqrt{n} \\
* & \mbox{otherwise.}
\end{cases}
\] 
(We assume throughout this section that $n$ is even.)

\begin{definition}
A randomized protocol \emph{computes} the partial function $f : \calX \times \calY \to \{0,1,*\}$ with error at most $\epsilon$ if for every input $(x,y) \in f^{-1}(0) \cup f^{-1}(1)$, it outputs the value $f(x,y)$ with probability at least $1-\epsilon$.\footnote{For the inputs in $f^{-1}(*)$, the protocol is free to output anything.}
\end{definition}

\begin{theorem}
The one-way randomized communication complexity of the $\GHD : \{0,1\}^n \times \{0,1\}^n \to \{0,1\}$ function is
\[
R^\rightarrow(\GHD) = \Omega(n).
\]
\end{theorem}

\begin{proof}
\replacethistext{Enter your proof here, if you complete it.}
\end{proof}


\bigskip
\emph{Hint.} The lower bound for the one-way complexity of the \textsc{Index} function should be helpful for this proof as well, though the reduction is not (nearly) as straightforward as the one in the last section.


\newpage 
\section{Simultaneous message passing}

In the \emph{simultaneous message passing (SMP)} model of communication, Alice and Bob both send messages to a third party that we will call the Referee, without seeing each other's transmissions. (We picture both Alice and Bob sending their communications to the referee simultaneously.) Note that the Referee does not see either Alice or Bob's inputs, and must then output the result of the protocol using only the messages received from both parties. In the private-coin model of SMP communication, Alice and Bob each have a private source of randomness that is not visible to any other party.

\begin{definition}
The \emph{SMP (private randomness) complexity} with error $\epsilon$ of a function $f : \calX \times \calY \to \{0,1\}$, denoted
\[
R^{\parallel,\mathrm{priv}}_\epsilon(f),
\] 
is the minimum (worst-case) communication cost of any private-coin SMP protocol that computes $f$ with error at most $\epsilon$ on any input $(x,y) \in \calX \times \calY$.
\end{definition}

As usual, we write $R^{\parallel,\mathrm{priv}}(f) = R^{\parallel,\mathrm{priv}}_{1/3}(f)$.

\begin{theorem}
The SMP complexity of every function $f : \calX \times \calY \to \{0,1\}$ satisfies
\[
R^{\parallel,\mathrm{priv}}(f) = \Omega(\sqrt{D(f)}).
\]
\end{theorem}

\begin{proof}
	Let $\pi$ be a SMP protocol for $f$ of cost $R^{\parallel,\mathrm{priv}}(f)$. For each input $x\in\calX$ let $M_x$ be the set of messages Alice has non-zero probability of sending on input $x$. Similarly for each input $y\in\calY$ let $M_y$ be the set of messages Bob has non-zero probability of sending on input $y$.
	
	We want to show that for each $x \in \calX$ there exists a set of messages $S_x \subseteq M_x$ such that for any $y \in \calY$, and message $m_y \in M_y$, the protocol correctly outputs $f(x,y)$ on the majority of messages $S_x$ with $m_y$. Let $\tau(m_x,m_y)$ be the referees (deterministic) output on messages $(m_x,m_y)$ for protocol $\pi$. I.e.
	$$\frac{2}{3} \leq \Pr(\pi(x,y) = f(x,y)) = \Pr_{m_x,m_y}(\tau(m_x,my) = f(x,y)).$$
	Fix $y\in \calY$, and $m_y \in M_y$. Draw a uniform random subset $S_x \subseteq M_x$  of size $t$. By a Chernoff Bound, the probability that $\tau(m_x,m_y) \neq f(x,y)$ for at least half of message pairs $m_x, m_y$ is at most
	$$2^{-\Omega(t)}.$$
	Since $\pi$ has cost $R^{\parallel,\mathrm{priv}}(f)$, over all $y$ there are at most $2^{R^{\parallel,\mathrm{priv}}(f)}$ messages the referee sees from Bob. So by union bound, the probability $\tau(m_x,m_y) \neq f(x,y)$ for some message $m_y$ is at most
	$$2^{R^{\parallel,\mathrm{priv}}(f)}2^{-\Omega(t)}.$$
	This probability is strictly less than one for $t = \Omega(R^{\parallel,\mathrm{priv}}(f))$. Thus by the probabilistic method the set $S_x$ exists. By a symmetric argument, the analogous set $S_y$ exists for each $y \in \calY$.
	
	Now our deterministic protocol for $f$ is as follows. Alice sends to Bob each message in $S_x$ for her input $x$. Since $|S_x| \leq R^{\parallel,\mathrm{priv}}(f)$ and each message in $S_x$ has size at most $R^{\parallel,\mathrm{priv}}(f)$, this has communication cost $R^{\parallel,\mathrm{priv}}(f)^2$. Bob sends to Alice each message $S_y$ for his input $y$, at communication cost $R^{\parallel,\mathrm{priv}}(f)^2$. They both can compute the majority referee decision on $S_x\times S_y$, and output that. By our choice of $S_x$ and $S_y$, this yields the correct answer. The cost of this protocol is $O(R^{\parallel,\mathrm{priv}}(f)^2)$, as desired.
\end{proof}


\bigskip
\noindent \emph{Hint.} The ideas from the proofs of Sections 8 and 9 in Chapter 4 might be helpful here as well. In particular, what can we say about the success probability of a protocol in which Alice and Bob send $t$ messages to the referee instead of $1$, when all $t$ messages are sent for the same input but with independent randomness?


\newpage 
\section{Equality X}

\begin{theorem}
The SMP complexity of the equality function $\Eq : \{0,1\}^n \times \{0,1\}^n \to \{0,1\}$ is
\[
R^{\parallel,\mathrm{priv}}(\Eq) = \Theta(\sqrt{n}).
\]
\end{theorem}

\begin{proof}
\replacethistext{Enter your proof here.}
\end{proof}

\bigskip
\noindent \emph{Hint.} By using (basic but still remarkable) results from error-correcting codes, it is sufficient to design a randomized protocol that computes the partial function $\Eq^* : \{0,1\}^n \times \{0,1\}^n \to \{0,1,*\}$ defined by
\[
\Eq^*(x,y) = \begin{cases}
1 & \mbox{if } x = y \\
0 & \mbox{if } \dHam(x,y) = \frac n2 \\
* & \mbox{otherwise}
\end{cases}
\]
with error at most $\frac13$.
\end{document}