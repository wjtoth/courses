\documentclass[11pt]{amsart}
\usepackage{amsmath,amsthm,amsfonts,amssymb}
\usepackage{fullpage}
\usepackage[usenames]{xcolor}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newtheorem{proposition}{Proposition}
\theoremstyle{definition}
\newtheorem{definition}{Definition}
\newtheorem{exercise}{Exercise}
\newtheorem{remark}{Remark}
\newtheorem{problem}{Problem}
\newtheorem{challenge}{Challenge Problem}
\newtheorem{open}{Open Problem}
\theoremstyle{plain}

\newcommand{\argmin}{\mathrm{argmin}}
\newcommand{\calX}{\mathcal{X}}
\newcommand{\calY}{\mathcal{Y}}
\newcommand{\calZ}{\mathcal{Z}}
\newcommand{\cost}{\mathrm{cost}}
\newcommand{\disc}{\mathrm{disc}}
\newcommand{\discu}{\mathrm{disc_u}}
\newcommand{\Disj}{\textsc{Disj}}
\newcommand{\Eq}{\textsc{Eq}}
\newcommand{\GT}{\textsc{GT}}
\newcommand{\HD}{\textsc{HD}}
\newcommand{\IP}{\textsc{IP}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\rank}{\mathrm{rank}}
\newcommand{\Rpriv}{R^{\mathrm{priv}}}
\newcommand{\replacethistext}[1]{\textcolor{red}{#1}}

\newcommand{\exercises}{\bigskip \noindent\rule{8cm}{0.4pt} \medskip}



\title{Communication complexity: Chapter 4 \\ Randomized communication complexity}
\author{Eric Blais and Justin Toth}


\begin{document}

\maketitle

Until now, we have been focused on deterministic communication protocols. In this chapter, we study \emph{randomized} communication protocols. We first focus on what is known as the \emph{public-randomness model} of communication. 

\begin{definition}[Randomized protocol]
A \emph{randomized communication protocol} $\Pi$ is a distribution over deterministic communication protocols. The \emph{(worst-case) cost} of $\Pi$ is the maximum cost of any protocol in its support. The randomized protocol $\Pi$ \emph{computes} $f : \calX \times \calY \to \{0,1\}$ with error at most $\epsilon$ if for every $(x,y) \in \calX \times \calY$,
\[
\Pr_{\pi \sim \Pi}[ \pi(x,y) \neq f(x,y) ] \le \epsilon.
\]
\end{definition}


\begin{definition}[Randomized communication complexity]
For $0 < \epsilon < \frac12$, the \emph{randomized communication complexity} of $f : \calX \times \calY \to \{0,1\}$ at error $\epsilon$ is
\[
R_\epsilon(f) := \min_{\Pi} \cost(\Pi)
\]
where the minimum is taken over all randomized communication protocols that compute $f$ with error at most $\epsilon$.
\end{definition}

\begin{remark}
The notation for randomized communication complexity can vary. In Kushilevitz and Nisan, for instance, the randomized communication complexity defined above is written as $R^{\mathrm{pub}}_\epsilon$ and our symbol $R_\epsilon$ is reserved for another model of randomized communication (namely: the private randomness model we will see at the end of the chapter).
\end{remark}

\exercises

\begin{exercise}
Another way to define randomized communication protocols is to consider a model where Alice and Bob have access to their respective inputs \emph{and} to a common sequence of coins that are flipped at random. Show that this model is equivalent to the one described above.
\end{exercise}

\begin{exercise}
Show that with the definition above, $R_0(f) = D(f)$ always holds. For this reason, $R_0(f)$ is usually reserved to the randomized model of communication where the cost of $\Pi$ on input $(x,y)$ is defined to be the \emph{average} cost of the protocols in the support of $\Pi$.
\end{exercise}


\newpage 
\section{Randomized and distributional complexity}

Lower bounds in randomized communication complexity are often (usually?) obtained by establishing the lower bounds in the distributional communication complexity model instead. This approach is justified by the following relation, which is sometimes referred to as \emph{(the easy direction of) Yao's minimax principle}.

\begin{theorem}
For every $0 \le \epsilon \le \frac12$, every function $f : \calX \times \calY \to \{0,1\}$, and every distribution $\mu$ over $\calX \times \calY$,
\[
R_\epsilon(f) \ge D_\epsilon^\mu(f).
\]
\end{theorem}

\begin{proof}
Let $\Pi$ be a distribution over deterministic communication protocols computing $f$ such that $R_\epsilon(f) = \cost(\Pi)$. Let $\mu$ be a distribution over $\calX\times\calY$. 
$$\Pr_{(x,y)\sim \mu ;\pi\sim \Pi}[\pi(x,y)\neq f(x,y)]] \leq \epsilon.$$
Let $I^\pi_{x,y}$ be the indicator random variable for the event $\pi(x,y)\neq f(x,y)$. Since $\Pr_{\pi \sim \Pi}[\pi(x,y)\neq f(x,y)]\leq \epsilon$ for all $(x,y) \in \calX \times \calY$ we have
$$ \epsilon \geq \mathbb{E}_{ (x,y) \sim \mu} \Pr_{\pi\sim \Pi}(\pi(x,y)\neq f(x,y)) = \mathbb{E}_{ (x,y) \sim \mu} \mathbb{E}_{\pi\sim \Pi}[I^\pi_{x,y}].$$ 
Rearraning the order of expectation we see that
$$ \mathbb{E}_{ \pi\sim \Pi} \mathbb{E}_{(x,y) \sim \mu}[I^\pi_{x,y}] \leq \epsilon.$$
Hence there exists deterministic protocol $\pi$ such that 
$$\Pr_{(x,y)\sim \mu}[\pi(x,y) \neq f(x,y)] \leq \epsilon.$$
\end{proof}


\newpage 
\section{Yao's minimax principle (Bonus)}

Yao's minimax principle says something quite a bit stronger than the bound seen in the last chapter: it says that the randomized communication complexity of a function is \emph{equal} to the maximum distributional complexity of the function over any distributions on its domain.

\begin{theorem}
For every $0 \le \epsilon \le \frac12$ and every function $f : \calX \times \calY \to \{0,1\}$, 
\[
R_\epsilon(f) = \max_{\mu} D_\epsilon^\mu(f)
\]
where the maximum is taken over all distributions on $\calX \times \calY$.
\end{theorem}

\begin{proof}
We will use the following result by Von Neumann,
$$\min_{x\in X}\max_{y\in Y} f(x,y) = \max_{y \in Y} \min_{x \in X} f(x,y)$$
for compact convex sets $X,Y$ and continuous function $f: X\times Y \rightarrow \R$ such that 
\begin{itemize}
    \item for any $x \in X$, $f(x,\cdot): Y\rightarrow \R$ is concave, and
    \item for any $y \in Y$, $f(\cdot, y): X\rightarrow \R$ is convex.
\end{itemize}
Let $P_t$ be the set of deterministic protocols on inputs $\calX \times \calY$ such of cost of at most $t$. Let $A^t$ be the $P_t \times (\calX\times\calY)$ matrix where 
$$A^t_{\pi, (x,y)} = \begin{cases}
1, &\text{ if } \pi(x,y) \neq f(x,y) \\
0, &\text{ otherwise.}
\end{cases}$$
Let $\Delta_t = \{x \in \R^t_{\geq 0} : \mathbf{1}^Tx = 1\}$. Let $t_R = R_\epsilon(f)$ and let $t_D = \max_\mu D^\mu_\epsilon(f).$ Then
$$\epsilon \geq \min_{x \in \Delta_{t_R}} \max_{y \in \Delta_{|\calX\times\calY|}}x^TAy =\max_{y \in \Delta_{|\calX\times\calY|}} \min_{x \in \Delta_{t_R}} x^TAy.$$
The inequality above follows since $t_R = R_\epsilon (f)$, and hence there is a distribution $x$ on protocols of cost at  most $t_R$ such that for any distribution on inputs $y$, the error $x^TAy$ is at most $\epsilon$. The equality above follows by Von Neumann's Theorem. Observe that $\Delta_{t_R}$ is a polytope and hence it has an extreme point minimizer for linear objective $x^TAy$ for any fixed $y$. Hence there is a deterministic protocol $\pi$ of cost at most $t_R$ such that for any $\mu \in \Delta_{|\calX\times \calY|}$, the error of $\pi$ with respect to $\mu$ is at most $\epsilon$. Therefore
$$\max_{\mu} D^\mu_{\epsilon}(f) \leq t_R = R_\epsilon(f).$$ 
On the other hand, similarly,
$$\epsilon \geq \max_{y \in \Delta_{|\calX\times\calY|}} \min_{x \in \Delta_{t_D}} x^TAy  =  \min_{x \in \Delta_{t_D}} \max_{y \in \Delta_{|\calX\times\calY|}}x^TAy.$$
Thus there is a distribution $\Pi$ on protocols of cost at most $t_D$ with worst-case error on any distribution of inputs at most $\epsilon$. Thus
$$R_\epsilon(f) \leq t_D = \max_\mu D^\mu_\epsilon(f).$$
Combining inequalities yields the desired result.
\end{proof}


\newpage 
\section{Error reduction}

The randomized communication complexity functions is usually studied in the setting where $\epsilon = \frac13$. In fact, this is so common that shorthand notation is used for that case.

\begin{definition}
The \emph{(two-sided error) randomized communication complexity} of $f : \calX \times \calY \to \{0,1\}$ is
\[
R(f) = R_{1/3}(f).
\]
\end{definition}

The reason most of the work focuses on this particular choice of $\epsilon$ is that a general transformation can be used to bound $R_\epsilon(f)$ for any $\epsilon > 0$ as a function of $R(f)$. This result is sometimes known as the \emph{(majority) confidence amplification} trick.

\begin{theorem}
For every function $f : \calX \times \calY \to \{0,1\}$ and every $0 < \epsilon < \frac12$, 
\[
R_\epsilon(f) = O\left( R(f) \cdot \log \tfrac1\epsilon \right).
\]
\end{theorem}

\begin{proof}
Let $\Pi$ be a distribution over deterministic communication protocols computing $f$ such that $R(f) = \cost(\Pi)$. Consider the following random construction of a protocol: for some $t$ to be chosen later, sample independently $t$ protocols according to $\Pi$: $\pi_1,\dots, \pi_t$. Protocol $\pi$ will run $\pi_i$ on its input obtaining output $b_i \in\{0,1\}$, for each $i = 1,\dots, t$. Then $\pi$ will output the median of $b_1, \dots, b_t$ (suppose without loss they are ordered non-decreasingly). Call the distribution on protocols obtained by this random construction $\Pi'$.

A protocol chosen according to $\Pi'$ fails only if at least half of its constituent $t$ protocols chosen according $\Pi$ fail. Hence for any $(x,y) \in \calX\times \calY$,
$$\Pr_{\pi \sim \Pi'}[\pi(x,y)\neq f(x,y)] \leq \prod_{i=1}^{t/2} \Pr[\pi_i(x,y) \neq f(x,y)] \leq (\frac{1}{3})^{\frac{t}{2}}.$$
Thus if we chose $t = 2\log_3(\frac{1}{\epsilon})$ then $\Pr_{\pi \sim \Pi'}[\pi(x,y)\neq f(x,y)] \leq \epsilon$. Hence $t = O(\log\frac{1}{\epsilon})$. The cost of $\Pi'$ is the cost of $\Pi$ repeated $t$ times, so $\cost(\Pi') = O(R(f)\cdot \log\frac{1}{\epsilon})$ as desired.
\end{proof}

\exercises

\begin{exercise}
Extend the result in the theorem to show that for every $\delta > 0$, we also have
\[
R_\epsilon(f) = O\left( R_{\frac12 - \delta}(f) \cdot \phi(\epsilon,\delta) \right)
\]
for some appropriate function $\phi$ on $\epsilon$ and $\delta$.
\end{exercise}


\newpage 
\section{Equality VI}


\begin{theorem}
For every $0 < \epsilon < \frac12$,
\[
R_\epsilon(\Eq) = O\left(\log \tfrac1\epsilon \right).
\]
\end{theorem}

\begin{proof}
By the previous Theorem it suffices to show that $R(\Eq) = O(1)$. Let $x \in \{0,1\}^n$ be Alice's input and let $y \in \{0,1\}^n$ be Bob's. They generate two random vectors $r_1, r_2  \in \{0,1\}^n$ uniformly and independently at random. Then Alice computes $a_1 = x\cdot r_1$ (inner product mod $2$) and $a_2 = x\cdot r_2$ and send the two bits to Bob. Bob computes $b_1 = y\cdot r_1$ and $b_2 = y \cdot r_2$ and sends those two bits to Alice. They output $1$ if and only if $a_1 = b_1$ and $a_2 = b_2$.

When $x = y$ they will always output $1$, thus we can bound the failure rate by the false positive rate. Suppose $x \neq y$. Then the probability that $a_1 = b_1$ is $\frac{1}{2}$ and the probability that $a_2 = b_2$ is $\frac{1}{2}$. Hence the total failure probability can be bounded by  $\frac{1}{2}\frac{1}{2} = \frac{1}{4} \leq \frac{1}{3}$. Thus $R(\Eq) = O(1)$, as desired.
\end{proof}


\newpage 
\section{Greater than II}

Recall that the function $\GT : [2^n] \times [2^n] \to \{0,1\}$ is defined by
\[
\GT(x,y) = \begin{cases}
1 & \mbox{if } x > y \\
0 & \mbox{otherwise.}
\end{cases}
\]
We saw in Chapter 2 that the deterministic communication complexity of this function is $\Theta(n)$. Its randomized communication complexity is much smaller.

\begin{theorem}
The randomized communication complexity of $\GT : [2^n] \times [2^n] \to \{0,1\}$ is bounded above by
\[
R(\GT) = O\left(\log^2 n \right).
\]
\end{theorem}

\begin{proof}
We will solve Exercise $4$ instead. Let $\epsilon = \frac{1}{3\log n}$. The protocol proceeds by a binary search, using the Equality oracle we created in the previous problem. Let $x, y \in [2^n]$ be Alice and Bob's inputs respectively. At the start of the binary search the interval in question is $[1,n]$. At any arbitrary iteration of the binary search with interval $(\ell, u)$ (with $\ell < u$), Alice and Bob run their protocol for $\Eq$ on their substrings from $\ell$ to $\lfloor (\ell+u)/2 \rfloor$ with error $\epsilon$ costing $O(\log \frac{1}{\epsilon}) = O(\log\log n)$ communication. If the equality test passes then the next interval will be $[\lfloor (\ell+u)/2 \rfloor + 1, u]$, otherwise the next interval will be $[\ell, \lfloor (\ell+u)/2 \rfloor]$. On intervals with $\ell = u$, Alice and Bob send $x_\ell$ and $y_\ell$ to each other. If $x_\ell > y_\ell$ they return $1$, otherwise they return $0$.

The protocol works since $\GT(x,y) = 1$ if and only $x_i > y_i$ where $i$ is the first bit on which $x$ and $y$ differ (if such a bit exists, otherwise $x=y$). Provided no $\Eq$ tests error, the binary search will successfully identify this bit $i$. Hence the probability of failure is bounded by the probability of an $\Eq$ test failing. Any one test will fail with probability $\epsilon$. There will be at most $\log(n)$ test (rounds of binary search), and thus the probability of failure is at most
$$\log(n) \epsilon = \frac{1}{3}.$$
\end{proof}

\exercises

\begin{exercise}
Improve the upper bound to show $R(\GT) = O\left(\log n \log \log n \right)$.
\end{exercise}

\begin{exercise}
Prove that $R(\GT) = \Theta\left(\log n \right)$.
\end{exercise}


\newpage 
\section{Hamming distance}

The \emph{$k$-Hamming distance} function $\HD_k : \{0,1\}^n \times \{0,1\}^n \to \{0,1\}$ is defined by
\[
\HD_k(x,y) = \begin{cases}
1 & \mbox{if } |\{i \in [n] : x_i \neq y_i\}| \le k \\
0 & \mbox{otherwise.}
\end{cases}
\]

\begin{theorem}
For every $1 \le k \le \frac n2$,
\[
R(\HD_k) = O( k \log k).
\]
\end{theorem}

\begin{proof}
Consider the following random protocol. Alice and Bob generate random vectors $r_1, \dots, r_t \in \{0,1\}^n.$ Alice computes $x\cdot r_1, \dots, x\cdot r_t$ (the inner product mod $2$), and Bob computes $y\cdot r_1, \dots, y \cdot r_t$, and they send the results to each other.

Alice and Bob now each independently search for a vector $z \in \{0,1\}^n$ which satisfies for every $i\in[t]$,
$$x\cdot r_i = y\cdot r_i \oplus z \cdot r_i$$
and that $z$ has at most $k$ non-zero entries.

When $|\{i \in [n]: x_i \neq y_i\}| \leq k$, they always output $1$ since 
$$x\cdot r_i = (y \oplus (x\oplus y)) \cdot r_i = y\cdot r_i \oplus((x\oplus y) \cdot r_i) $$ 
(i.e. they can both find $z= x\oplus y$). Thus we can bound the failure rate by the false positive rate.

Let $z \in \{0,1\}^n$ such that $|z| \leq k$. Observe that when $x \neq y \oplus z$ the probability that 
$$x\cdot r_i = y\cdot r_i \oplus z\cdot r_i$$
for all $i \in [t]$ is at most $2^{-t}$. There's $O(n^k)$ ways to choose $z$, so by union bound the failure probability is at most
$$O(n^k)2^{-t}.$$
For any $\epsilon \in (0,1)$ if we want this failure to be at most $\epsilon$, so we can choose
$$t = O(k \log n).$$

If $n = \text{poly}(k)$ then this protocol with give the desired bound of $O(k \log k)$. Let $d = 10k^2$. Alice and Bob can randomly partition their input indices, $[n]$, into $d$ subsets $S_1, \dots, S_d\subseteq [n]$. Then they can compute $x'=\{x\cdot \mathbf{1}_{S_d},\dots, x\cdot \mathbf{1}_{S_d}\}$ and $y'=\{y\cdot \mathbf{1}_{S_d},\dots, y\cdot \mathbf{1}_{S_d}\}$ and re-run the protocol above as if their inputs were $x'$ and $y'$. 

With probability at most $$\frac{k^2}{d} = \frac{1}{10}$$
two coordinates on which $x$ and $y$ different lie in the same subset $S_i$. Hence the probability of failure in this partitioning step is at most $\frac{1}{10}$. Thus our total probability of failure is at most 
$$\frac{1}{10} + O(n^k)2^{-t} \leq \frac{1}{3}$$
for appropriate choice of $t = O(k\log d) = O(k\log k).$
\end{proof}


\newpage 
\section{Private randomness}

In the beginning of the chapter, we saw that the (public-coin) randomized communication model corresponds to the model where some coins are flipped and both Alice and Bob see the outcome of those coin flips. It is natural to study the alternative model where Alice and Bob both have access to coins they can flip (or, more abstractly, to some sources of randomness), but they each only see the outcome of their own coin flips. This model of communication corresponds to the \emph{private-coin randomized communication} model.

\begin{definition}
A \emph{private-coin randomized communication protocol} $\pi$ is equivalent to a deterministic communication protocol with the two additions: 
\begin{enumerate}
\item Each of Alice's internal node $v$ is labelled with a function $h_v : \calX \times R_A \to \{0,1\}$ and each of Bob's internal node $w$ is labelled with a function $h_v : \calY \times R_B \to \{0,1\}$; 
\item Alice has a distribution $\mu_A$ over $R_A$ and Bob has a distribution $\mu_B$ over $R_B$.
\end{enumerate}
The \emph{(worst-case) cost} of $\Pi$ is the maximum cost of any protocol in its support. The randomized protocol $\Pi$ \emph{computes} $f : \calX \times \calY \to \{0,1\}$ with error at most $\epsilon$ if for every $(x,y) \in \calX \times \calY$,
\[
\Pr_{r_a \sim \mu_A, r_b \sim \mu_B}[ \pi(x,y) \neq f(x,y) ] \le \epsilon.
\]
\end{definition}


\begin{definition}[Private-coin randomized communication complexity]
For $0 < \epsilon < \frac12$, the \emph{private coin randomized communication complexity} of $f : \calX \times \calY \to \{0,1\}$ at error $\epsilon$ is
\[
\Rpriv_\epsilon(f) := \min_{\Pi} \cost(\Pi)
\]
where the minimum is taken over all private-coin randomized communication protocols that compute $f$ with error at most $\epsilon$.
\end{definition}

\begin{remark}
As in the public-coin setting, we write $\Rpriv(f) := \Rpriv_{1/3}(f)$.
\end{remark}
\exercises

\begin{exercise}
Show that for every function $f$ and every parameter $\epsilon > 0$, 
$R_\epsilon(f) \le \Rpriv_\epsilon(f)$.
\end{exercise}

\begin{exercise}
Show that error amplification also holds in the private coin model: for every $\epsilon > 0$, $\Rpriv_\epsilon(f) = O(\Rpriv(f) \log \frac1\epsilon)$.
\end{exercise}


\newpage 
\section{Newman's theorem (Bonus)}

The gap between the public-coin and private-coin randomized communication complexities of a function cannot be arbitrarily large. 

\begin{theorem}
For every distribution $\mu$ over $\{0,1\}^n \times \{0,1\}^n$ and every constant $\epsilon > 0$,
\[
\Rpriv_{2\epsilon}(f) \le R_\epsilon(f) + O\left(\log \frac{n}{\epsilon^2}\right).
\]
\end{theorem}

\begin{proof}
Consider a public-coin randomized protocol $\pi$ attaining cost $R_\epsilon(f)$. Let $t = 2n/\epsilon^2$ and consider $\pi$ run on $t$ random strings $r_1, \dots, r_t$ drawn independently at random. Fix an input $(x,y) \in \calX \times \calY$. For any $i \in [t]$, the probability that $\pi$ fails when run on $r_i$ is at most $\epsilon$.
Hence the probability that $(1-2\epsilon)t$ strings amongst $r_1, \dots, r_t$ fail when $\pi$ runs using that string is at most
$$2^{-\Omega(\epsilon^2t)} = 2^{-\Omega(2n)}$$
by Chernoff Bounds. 

Now, by the union bound, for any input in $\calX\times \calY$ the probability that greater than $2\epsilon t$ strings fail when $\pi$ runs them is strictly less than $1$. Thus, by the probabilistic method there exist strings $R = \{r_1^*, \dots, r_t^*\}$ with the property that for any input pair at most $2\epsilon t$ strings in $R$ fail when $\pi$ runs using that string.

Our private coin protocol is as follows. Alice samples a string $r_i^* \in R$ uniformly at random, and sends its index $i$ to Bob. This costs $O(\log\frac{n}{\epsilon^2})$ communication. Now they can use $r_i^*$ as a source of shared randomness to run $\pi$ at additional communication cost $R_\epsilon(f)$. Since at most $2\epsilon t$ string in $R$ fail, the probability of failure for this protocol is at most
$$2\epsilon t/t = 2\epsilon$$
as desired.
\end{proof}

\bigskip
\begin{remark}
\emph{Hint.} First consider what happens when you run a public-coin randomized protocol on $t = n/\epsilon^2$ random strings drawn independently at random. Then Chernoff bounds and the probabilistic method may be useful in completing the proof.
\end{remark}


\newpage 
\section{Private-coin randomness and determinism}

Interestingly (and unlike in the public-coin model), the gap between the private-coin randomized and deterministic communication complexities of a function also cannot be arbitrarily large. 

\begin{theorem}
For every function $f : \{0,1\}^n \times \{0,1\}^n \to \{0,1\}$,
\[
\Rpriv(f) = \Omega( \log D(f)).
\]
\end{theorem}

\begin{proof}
Let $\pi$ be a private coin randomized protocol attaining cost $\Rpriv(f)$. Modify $\pi$ obtaining protocol $\pi'$ by truncating the branching probability of each node of the protocol tree $\pi$ to $10\cdot 2^{\Rpriv(f)}$ decimal positions. This incurs an additional probability of failure of at most $\frac{1}{10\cdot 2^{\Rpriv(f)}}$ at each node. In total this incurs at additional probability of failure of at most $\frac{1}{10\cdot 2^{\Rpriv(f)}}2^{\Rpriv(f)} = \frac{1}{10}$. Since the original failure probability was at most $\frac{1}{3}$ the faillure probability of $\pi'$ is strictly smaller than $\frac{1}{2}$.

To simulate $\pi'$ deterministically, Alice and Bob can exchange their branching probabilities at each  node in accordance with their input, at a total communication cost of $O(2^{\Rpriv(f)})$. Once they known the probabilities of all leaves associated with an input pair $(x,y)$, they can output in their deterministic protocol the bit which is output on the majority of such leaves. Since the failure probability of $\pi'$ is strictly less than $\frac{1}{2}$, this is always the correct bit to output.
\end{proof}

\bigskip
\begin{remark}
\emph{Hint.} It might be easiest to first consider a communication setting where Alice and Bob can send real numbers with cost $O(1)$ and show that in this setting there is a deterministic protocol that computes $f$ with cost $O(2^{\Rpriv(f)})$.
\end{remark}

\newpage 
\section{Equality VII}

We can use the results obtained in the previous sections to obtain tight bounds on the private-coin randomized communication complexity of the equality function.

\begin{theorem}
The private-coin randomized communication complexity of the $\Eq : \{0,1\}^n \times \{0,1\}^n \to \{0,1\}$ function is
\[
\Rpriv(\Eq) = \Theta( \log n).
\]
\end{theorem}

\begin{proof}
To see that $\Rpriv(\Eq) = O(\log n)$ we apply Newman's Theorem to our previous result that $R(\Eq) = O(1)$.

To see that $\Rpriv(\Eq) = \Omega(\log n)$ we apply Theorem 8 of this Chapter with Theorem $1$ of Chapter $2$ which said that $D(\Eq) = \Omega(n)$.
\end{proof}

\exercises

\begin{exercise}
Prove the upper bound $\Rpriv(\Eq) = O( \log n)$ directly without using Newman's theorem.
\end{exercise}



\end{document}