\documentclass[11pt]{amsart}
\usepackage{amsmath,amsthm,amsfonts,amssymb}
\usepackage{fullpage}
\usepackage[usenames]{xcolor}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newtheorem{proposition}{Proposition}
\theoremstyle{definition}
\newtheorem{definition}{Definition}
\newtheorem{exercise}{Exercise}
\newtheorem{remark}{Remark}
\newtheorem{problem}{Problem}
\newtheorem{challenge}{Challenge Problem}
\newtheorem{open}{Open Problem}
\theoremstyle{plain}

\newcommand{\argmin}{\mathrm{argmin}}
\newcommand{\calX}{\mathcal{X}}
\newcommand{\calY}{\mathcal{Y}}
\newcommand{\calZ}{\mathcal{Z}}
\newcommand{\cost}{\mathrm{cost}}
\newcommand{\disc}{\mathrm{disc}}
\newcommand{\discu}{\mathrm{disc_u}}
\newcommand{\Disj}{\textsc{Disj}}
\newcommand{\Eq}{\textsc{Eq}}
\newcommand{\GT}{\textsc{GT}}
\newcommand{\HD}{\textsc{HD}}
\newcommand{\ICext}{\mathrm{IC}^{\mathrm{ext}}}
\newcommand{\ICint}{\mathrm{IC}^{\mathrm{int}}}
\newcommand{\icostext}{\mathrm{icost}^{\mathrm{ext}}}
\newcommand{\icostint}{\mathrm{icost}^{\mathrm{int}}}
\newcommand{\IP}{\textsc{IP}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\rank}{\mathrm{rank}}
\newcommand{\Rpriv}{R^{\mathrm{priv}}}
\newcommand{\replacethistext}[1]{\textcolor{red}{#1}}

\newcommand{\exercises}{\bigskip \noindent\rule{8cm}{0.4pt} \medskip}
\DeclareMathOperator{\height}{height}

\DeclareMathOperator*{\E}{\mathbb{E}}

\title{Communication complexity: Chapter 5 \\ Information complexity}
\author{Eric Blais and Justin Toth}


\begin{document}

\maketitle

Communication complexity is concerned mostly with the \emph{minimum number of bits} that Alice and Bob need to exchange in order to compute some function on
their joint inputs. Information complexity, on the other hand, is concerned with
the \emph{minimum amount of information} contained in the bits that Alice and
Bob exchange to compute that function. Our first task is to formally define this notion of information. To do so, we will use standard definitions from information theory.

\begin{definition}[Entropy]
The \emph{entropy} of a random variable $Z$ drawn from the probability distribution $\mu$ over a finite set $\calZ$ is
\[
H(Z) = - \sum_{z \in \calZ} \mu(z) \log_2 \mu(z).
\]
The entropy of a sequence of random variables $Z_1,Z_2,\ldots,Z_k$, denoted $H(Z_1\,Z_2\cdots Z_k)$ is the entropy of the random variable $Z = (Z_1,Z_2,\ldots,Z_k)$.
\end{definition}

\begin{definition}[Conditional entropy]
The \emph{conditional} entropy of $Z$ given another random variable $Z'$ is
\[
H(Z \mid Z') = H(Z\,Z') - H(Z').
\]
\end{definition}

\begin{definition}[Mutual information]
The \emph{mutual information} of two random variables $Z$ and $Z'$ is
\[
I( Z ; Z' ) = H(Z) - H(Z \mid Z').
\]
The \emph{conditional mutual information} of $Z$ and $Z'$ given $W$ is
\[
I( Z ; Z' \mid W) = H(Z \mid W) - H(Z \mid Z'\, W).
\]
\end{definition}

We use the following basic properties of entropy and mutual information.

\begin{theorem}
Entropy satisfies the following properties:
\begin{description}
\item[Boundedness] For every $Z$ over a finite set $\calZ$, $0 \le H(Z) \le \log_2 |\calZ|$.
\item[Chain rule] $H(Z_1\,Z_2\cdots Z_k) = H(Z_1) + H(Z_2 \mid Z_1) + \cdots + H(Z_k \mid Z_1\cdots Z_{k-1})$.
\item[Subadditivity] $H(Z \mid Z') \le H(Z)$ and $H(Z\,Z') \le H(Z) + H(Z')$.
\end{description}
Mutual information satisfies the following properties:
\begin{description}
\item[Boundedness] $0 \le I(Z ; Z') \le \min\{ H(Z), H(Z') \}$.
\item[Chain rule] $I(Z_1\,Z_2 ; W) = I(Z_1 ; W) + H(Z_2 ; W \mid Z_1)$.
\item[Data processing inequality] Whenever $Z' = f(Z)$ is determined by $Z$, $I(Z' ; W) \le I(Z ; W)$.
\end{description}

\end{theorem}



\newpage 
\section{External information complexity}

Throughout this chapter, we will consider randomized protocols $\Pi$ that have access to both public- and private-coin randomness. 

\begin{definition}[Transcript]
The \emph{transcript} of a protocol $\Pi$ on some input $(x,y) \in \calX \times \calY$, which we will denote by
\[
\Gamma_{x,y}^\Pi
\]
is a bit string that includes (i) the public-coin random string $R$ used by Alice and Bob and (ii) the sequence of bits that they exchange. 
\end{definition}

The first natural notion of information of a protocol that we will study is the amount of information that an external observer learns about Alice and Bob's input $(x,y)$ by seeing the transcript $\Gamma_{x,y}^\Pi$ of their communication protocol.

\begin{definition}[External information cost]
The \emph{external information cost} of a randomized protocol $\Pi$ over the distribution $\mu$ on $\calX \times \calY$ is
\[
\icostext_\mu(\Pi) = I\big( X\,Y ; \Gamma_{X,Y}^\Pi \big)
\]
where $(X,Y) \sim \mu$.
\end{definition}

\begin{definition}[External information complexity]
The \emph{$\epsilon$-error external information complexity} of the function $f : \calX \times \calY \to \{0,1\}$ with respect to the distribution $\mu$ is
\[
\ICext_{\mu,\epsilon}(f) = \inf_{\pi} \icostext_\mu(\pi)
\]
with the infimum taken over all randomized protocols that compute $f$ with error at most $\epsilon$.\footnote{Note that the error of a randomized protocol is still defined to be its maximum error probability over \emph{any} input in $\calX \times \calY$; it is \emph{not} the expected error over an input drawn from $\mu$.}
\end{definition}

The external information complexity of a function gives a lower bound on its randomized communication complexity.

\begin{theorem}\label{th:2}
For every function $f : \calX \times \calY \to \{0,1\}$ and every distribution $\mu$ on $\calX \times \calY$,
\[
\ICext_{\mu,\epsilon}(f) \le R_\epsilon(f).
\]
\end{theorem}


\begin{proof}

Let $\Pi$ be a distribution on protocols such that $\cost(\Pi) = R_\epsilon(f)$. That is,
$$R_\epsilon(f) = \max_{(x,y) \in \calX\times\calY}\max_{\pi \sim \Pi}\height(\pi(x,y))$$ 
where $\height(\pi(x,y))$ denotes the length of the path from the root to leaf of the protocol tree $\pi$ dictated by inputs $(x,y)$. We use $\height(\pi)$ to denote $\max_{(x,y) \in \calX\times\calY}\height(\pi(x,y))$. Let $\pi^*$ be the protocol tree achieving cost $\R_\epsilon(f)$. Thus we have
$$R_\epsilon(f) = \height(\pi^*) \geq \E_{(x,y) \sim \mu} [\height(\pi^*(x,y))].$$
Let $\Gamma^{\pi^*}_{X,Y}(r)$ denote the shared random string part of $\Gamma^{\pi^*}_{X,Y}$, and let $\Gamma^{\pi^*}_{X,Y}(c)$ denote the communication between Alice and Bob of the transcript. We claim that
$$\E_{(x,y) \sim \mu} [\height(\pi^*(x,y))] \geq H(\Gamma^{\pi^*}_{X,Y}(c)).$$
To see this claim is sufficient, observe that it implies 
\begin{align*}
R_\epsilon(f) &\geq H(\Gamma^{\pi^*}_{X,Y}(c)) \\
&\geq I(XY; \Gamma^{\pi^*}_{X,Y}(c) \mid \Gamma^{\pi^*}_{X,Y}(r)) \\
&= I(XY ;\Gamma^{\pi^*}_{X,Y}) \\
&= \icostext_\mu(\Gamma^{\pi^*}_{X,Y})\\
&\geq \ICext_{\mu,\epsilon}(f).
\end{align*}

Now we proceed to prove the claim. Let $L$ be the set of leaves of the protocol tree $\pi^*$. We define a distribution $\mu'$ on $L$ as follows: for each $\ell \in L$, we let
 $$\mu'(\ell) = \sum_{(x,y) \in \calX\times\calY : (x,y)\text{ induces a path to } \ell} \mu(x,y).$$ 
 Let $h_\ell = \height(x,y)$ where $(x,y)$ induces a path to $\ell$. i.e. $h_\ell$ is the length of the path from the root of $\pi^*$ to $\ell$ (this path is taken on input $(x,y)$). We have
\begin{align*}
\E_{(x,y)\sim \mu}[h_{x,y}] &=\sum_{\ell \in L} \mu'(\ell) h_\ell \\
&=\sum_{\ell \in L} \mu'(\ell)\left(\log(\frac{1}{\mu'(\ell)})-\log(\frac{1}{\mu'(\ell)}) +\log(2^{h_\ell})\right)\\
&=\sum_{\ell\in L} \mu'(\ell)\left(\log(\frac{1}{\mu'(\ell)})-\log(\frac{2^{-h_\ell}}{\mu'(\ell)}))\right) \\
&=H(\Gamma^{\pi^*}_{X,Y}(c)) - \sum_{\ell\in L}\mu'(\ell)\log(\frac{2^{-h_\ell}}{\mu'(\ell)}) \\
&\geq H(\Gamma^{\pi^*}_{X,Y}(c)) - \log(\sum_{\ell\in L} 2^{-h_\ell})
\end{align*}
where the last line follows by applying the convexity of $\log$. The sum inside the logarithm, $\sum_{\ell \in L} 2^{-h_\ell}$ is at most $1$, completing the proof. This is a basic fact about binary trees. We'll give a brief proof for completeness.
Proceed by induction on $|L|$. If $|L| = 1$, the result is immediate. For induction suppose the result holds for binary trees smaller that $\pi^*$. Let $r$ be the root of $\pi^*$. Split $\pi^*$ into two subtrees $T_1$ and $T_2$ by deleting $r$. Now observe that
$$\sum_{\ell \in L} 2^{-h_\ell} = \frac{1}{2}\sum_{\ell \in L \cap T_1} + \frac{1}{2}\sum_{\ell \in L \cap T_2} \leq \frac{1}{2}+\frac{1}{2} = 1.$$
With the first equality following since joining $T_1$ and $T_2$ by root $r$, adds $1$ to length of the path to each leaf, and the inequality follows by induction.
\end{proof}


\newpage 
\section{Public randomness can be eliminated}

When designing protocols to obtain upper bounds on the information complexity of functions, it is convenient to work in the framework where Alice and Bob can use both public- and private-coin randomness. For lower bounds, however, it is useful to note that without loss of generality we can consider protocols that use only private randomness.

\begin{theorem}
For every function $f : \calX \times \calY \to \{0,1\}$, every distribution $\mu$ over $\calX \times \calY$, and every $0 \le \epsilon \le \frac12$,
\[
\ICext_{\mu,\epsilon}(f) = \inf_{\pi} \icostext_\mu(\pi)
\]
even when the infimum is taken only over private-coin randomized protocols that compute $f$ with error at most $\epsilon$.
\end{theorem}

\begin{proof}
Since infimum is taken over only private-coin randomized protocols, as opposed to all randomized protocols, it is immediate that
$$\ICext_{\mu,\epsilon}(f) \leq \inf_{\pi} \icostext_\mu(\pi).$$
To complete the proof, consider any randomized protocol $\pi$ which computes $f$ with error at most $\epsilon$. We will show the existence of $\pi'$, a private coin randomized protocol, which computes $f$ with error at most $\epsilon$ and satisfying
$$\icostext_\mu(\pi) = \icostext_\mu(\pi').$$
Let $\chi$ be the distribution on shared random bit strings used by $\pi$. The protocol $\pi'$ operates as follows. First  Alice draws a random string $R$ according to $\chi$, and communicates that string to Bob. Then Alice and Bob simulate $\pi$ using $R$ as their shared randomness.

Since protocol transcripts do not record private random bits, to the external observer the transcripts for $\pi$ and $\pi'$ are indistinguishable. In fact, $\Gamma_{X,Y}^\pi$ and $\Gamma_{X,Y}^{\pi'}$ are identically distributed when $X,Y \sim \mu$ and $R\sim \chi$. Therefore
$$\icostext_\mu(\pi) = I(XY;\Gamma^{pi}_{XY}) = I(XY;\Gamma^{\pi'}_{XY}) = \icostext_\mu(\pi')$$
as desired.
\end{proof}


\newpage 
\section{Equality VIII}

External information complexity can be used to give a simple proof of the $\Omega(n)$ lower bound for the $0$-error randomized communication complexity of the equality function.

\begin{theorem}
Let $\mu$ be the uniform distribution on the set $\{(x,x) : x \in \{0,1\}^n\}$. The $0$-error information complexity of $\Eq : \{0,1\}^n \times \{0,1\}^n \to \{0,1\}$ with respect to $\mu$ is
\[
\ICext_{\mu,0}(\Eq) = n.
\]
\end{theorem}

\begin{proof}
Let $\pi$ be a private coin randomized protocol computing $\Eq$ with error $0$. Using the previous result, it suffices to prove
$$\icostext_\mu(\pi) = n.$$
We have,
$$\icostext_\mu(\pi) = I(XY, \Gamma_{X,Y}^\pi) = H(XY) - H(XY \mid \Gamma_{X,Y}^\pi).$$
Since $XY$ are uniformly distributed over $2^n$ elements,
$$H(XY)= \log(2^n) = n.$$
Now we just need to show $H(XY \mid \Gamma_{X,Y}^\pi) = 0.$ Indeed, this holds since we claim each transcript uniquely identifies the input pair $XY$. In other words, $\Pr[XY\mid \Gamma_{X,Y}^\pi = \gamma]$ is either $0$ or $1$, and thus
$$H(XY\mid \Gamma_{X,Y}^\pi) = 0.$$
To see the claim, suppose that there is a transcript of $\pi$ which is identical for distinct inputs $(x,x)$ and $(x',x')$ drawn from $\mu$, when Alice and Bob have the same random private strings $R_A$ and $R_B$ respectively. Since $\pi$, makes $0$ errors, it outputs $1$ on inputs $(x,x)$ and $(x',x')$. But the transcript would also be identical for input $(x,x')$ on random strings $R_A, R_B$, and so $\pi$ would still output $1$ on this input. This contradicts that $\pi$ makes $0$ errors.
\end{proof}

\newpage 
\section{And}

Let $\textsc{And} : \{0,1\} \times \{0,1\} \to \{0,1\}$ be the and function $\textsc{And}(x,y) = x \wedge y$ that takes the value $1$ if and only if $x = y = 1$.

\begin{theorem}
For every distribution $\mu$ on $\{0,1\} \times \{0,1\}$,
\[
\ICext_{\mu,0}(\textsc{And}) \le \log_2 3.
\]
Furthermore, this bound is tight when $\mu(0,1) = \mu(1,0) = \mu(1,1) = \frac13$.
\end{theorem}

\begin{proof}
Consider the protocol $\pi$, where Alice sends Bob the bit, $x$, she receives, and Bob sends Alice the bit he receives if and only if $x=1$. If $x=0$ the protocol $\pi$ outputs $0$, and otherwise they exchanged bits, so the protocol can output $x\wedge y$. Clearly this is a $0$-error protocol for $\textsc{And}$.

Since 
$$\icostext_\mu(\pi) = I(XY; \Gamma^\pi_{X,Y}) \leq H(\Gamma^\pi_{X,Y}) \leq \log_2(3)$$
the result holds. The last inequality follows since there are precisely three different transcripts: $0$, $10$, and $11$ which can occur as a result of running $\pi$.

Now fix $\mu$ so $\mu(0,1) = \mu(1,0) = \mu(1,1) = \frac{1}{3}$. Let $\pi$ be any random protocol with computes $\textsc{And}$ without error. Then, at the end of communication, for any input $(x,y)$, Alice knows $x$ and $x\wedge y$. Since $\mu(00) = 0$, Alice can uniquely recover $y$ from this information. Symmetrically, Bob also knows $x$ at the end of communication. In other words, knowing the transcript suffices to learn both $x$ and $y$ under this distribution $\mu$. Hence
$$H(XY \mid \Gamma^\pi_{X,Y}) = 0.$$
Thus
$$\icostext_\mu(\pi) = I(XY;\Gamma^{\pi}_{X,Y}) = H(XY) =\log_2(3)$$
with the last equality following since $XY\sim \mu$ is a uniform distribution on $3$ elements.
\end{proof}


\newpage 
\section{Internal information complexity}

Another natural notion of information of a protocol is the amount of information that Alice and Bob learn about each other's inputs by running their communication protocol.

\begin{definition}[Internal information cost]
The \emph{internal information cost} of a randomized protocol $\Pi$ over the distribution $\mu$ on $\calX \times \calY$ is
\[
\icostint_\mu(\Pi) = I\big( X ; \Gamma_{X,Y}^\Pi \, Y\big) + I\big( Y ; \Gamma_{X,Y}^\Pi \, X\big).
\]
where $(X,Y) \sim \mu$.
\end{definition}

\begin{definition}[Internal information complexity]
The \emph{$\epsilon$-error internal information complexity} of the function $f : \calX \times \calY \to \{0,1\}$ with respect to the distribution $\mu$ is
\[
\ICint_{\mu,\epsilon}(f) = \inf_{\pi} \icostint_\mu(\pi)
\]
with the infimum taken over all randomized protocols that compute $f$ with error at most $\epsilon$.
\end{definition}

The internal information complexity of a function gives a lower bound on its external information complexity (and therefore on its randomized communication complexity as well).

\begin{theorem}
For every function $f : \calX \times \calY \to \{0,1\}$, every distribution $\mu$ on $\calX \times \calY$, and every $0 \le \epsilon \le \frac12$,
\[
\ICint_{\mu,\epsilon}(f) \le \ICext_{\mu,\epsilon}(f).
\]
\end{theorem}

\begin{proof}
We may assume that every protocol with computes $f$ always achieves its worst-case communication cost on every input and random string, by simply padding communication with $0$s on inputs where this is not the case. We will show that all randomized protocols, $\pi$, which compute $f$ with error at most $\epsilon$ satisfy
$$\icostext_{\mu,\epsilon}(\pi) \geq \icostint_{\mu,\epsilon}(\pi).$$
Let $k$ be the length of the transcript for $\pi$. Use $\Gamma^\pi_{X,Y}(i)$ to denote the $i$-th bit communicated in the transcript, for $i=1,\dots, k$ and $\Gamma^\pi_{X,Y}[i,j]$ to denote the subtranscript from the $i$-th bit to the $j$-th bit, inclusive. The $i$-th bit was communicated by one player to the other player. The player sending the bit learns nothing by doing so. Therefore we have either
$$I(X;\Gamma^\pi_{X,Y}(i) \mid \Gamma^{\pi}_{X,Y}[1,i-1], Y) = 0 \quad\text{or}\quad I(Y;\Gamma^\pi_{X,Y}(i) \mid \Gamma^{\pi}_{X,Y}[1,i-1], X) = 0 $$
for all $i\in[k]$.
Now we commute
\begin{align*}
\icostext_{\mu,\epsilon}(\pi) &= \sum_{i=1}^k I(XY;\Gamma^\pi_{X,Y}(i) \mid \Gamma^\pi_{X,Y}[1,i-1]) \\
&\geq \sum_{i=1}^k\max\{I(X;\Gamma^\pi_{X,Y}(i) \mid \Gamma^{\pi}_{X,Y}[1,i-1], Y), I(Y;\Gamma^\pi_{X,Y}(i) \mid \Gamma^{\pi}_{X,Y}[1,i-1], X)\}\\
&=\sum_{i=1}^k I(X;\Gamma^\pi_{X,Y}(i) \mid \Gamma^{\pi}_{X,Y}[1,i-1], Y)+ I(Y;\Gamma^\pi_{X,Y}(i) \mid \Gamma^{\pi}_{X,Y}[1,i-1], X) \\
&= I(X;\Gamma^\pi_{X,Y} \mid Y) + I(Y; \Gamma^\pi_{X,Y} \mid X) \\
&= \icostint_{\mu,\epsilon}(\pi).
\end{align*}
The second inequality above follows by our observation that one of the terms in each max is $0$. Every other manipulation is repeated application of the chain rule.
\end{proof}

\newpage 
\section{Equality VIII}

The internal information complexity of a function can be much smaller than its external information complexity, as the following example shows.

\begin{theorem}
For every distribution $\mu$ over $\{0,1\}^n \times \{0,1\}^n$, the $0$-error information complexity of $\Eq : \{0,1\}^n \times \{0,1\}^n \to \{0,1\}$ is 
\[
\ICint_{\mu,0}(\Eq) = O(1).
\]
\end{theorem}

\begin{proof}
Let $\pi$ be a protocol where Alice, with input $x$, and Bob, with input $y$, sample $n$ linearly independent vectors $r_1, \dots, r_n \in \{0,1\}^n$ uniformly at random using their shared randomness, and then do the following: for $i=1,\dots n$ do:
\begin{enumerate}
	\item Alice sends Bob $a_i:=x\cdot r_i$.
	\item Bob sends Alice $b_i:=y\cdot r_i$.
	\item If $a_i \neq b_i$ they stop immediately and output $0$.
	\item Otherwise, if it is the last iteration they output $1$.
\end{enumerate} 
This protocol is $0$-error. Since $r_1,\dots, r_n$ are $n$ linearly independent vectors, they form a basis of $\{0,1\}^n$. Therefore $x\neq y$ if and only if there exists some $i\in[n]$ such that $x\cdot r_i \neq y\cdot r_i$. When $x=y$ they communicate $n$ bits, but it is interesting to see what happens when $x\neq y$. In this case, we want to find the expected number of bits communicated. In each round that occurs $2$ bits are sent. A round occurs with the following probability,
$$\Pr[a_j = b_j, \forall j=1,\dots, i-1]=\sum_{i=1}^n\frac{2^{n-i+1}-1}{2^n-1}$$
since is equivalent to saying that $x-y$ (with addition taken in $\mathbb{F}_2$) is in the $n-i+1$-dimensional subspace formed by the kernel of $r_1,\dots,r_{i-1}$. Since $x-y\neq 0$ there are $2^n-1$ choices for $x-y$, and there $2^{n-i+1}-1$ choices which lie in the kernel. Since our basis is chosen uniformly at random, this yields the desired probability. Hence we have that the expected number of bits communicated is
$$\sum_{i=1}^n 2\cdot\Pr[a_j = b_j, \forall j=1,\cdots, i-1] = 2\sum_{i=1}^n\frac{2^{n-i+1}-1}{2^n-1}\leq 2\sum_{i=1}^n\frac{2^{n-i+1}}{2^n} \leq 2\cdot 2 = O(1).$$
Let $\Delta$ be the random indicator variable for $X=Y$. We now compute $I(X;\Gamma^\pi_{X,Y}\mid Y)$,
\begin{align*}
I(X;\Gamma^\pi_{X,Y} \mid Y) &= I(X\Delta;\Gamma^\pi_{X,Y}\mid Y) &\text{(since $\Gamma^\pi_{X,Y}$ suffices to learn $\Delta$)} \\
	&= I(X;\Gamma^\pi_{X,Y}\mid Y, \Delta) + I(\Delta;\Gamma^\pi_{X,Y}\mid Y) \\
	&\leq  I(X;\Gamma^\pi_{X,Y}\mid Y, \Delta) + 1  &\text{(since $I(\Delta;\Gamma^\pi_{X,Y}\mid Y) \leq H(\Delta) = 1$)}\\
	&\leq  I(X;\Gamma^\pi_{X,Y}\mid Y, \Delta=1) +  I(X;\Gamma^\pi_{X,Y}\mid Y, \Delta=0) + 1 \\
	&=  I(X;\Gamma^\pi_{X,Y}\mid Y, \Delta=0) + 1 \\
	&= O(1).
\end{align*}
The second last equality follows since $I(X;\Gamma^\pi_{X,Y}\mid Y, \Delta=1)=0$, as once we know $Y$ and that $X=Y$, we know everything. The last equality follows from two facts. Firstly our prior observation that the expected number of bits communicated by $\pi$ when $X\neq Y$ is $O(1)$, and the second that $ I(X;\Gamma^\pi_{X,Y}\mid Y, \Delta=0) $ is at most that expected quantity. That second fact follows from our proof of Theorem \ref{th:2}, where be actually bound information cost by the expected communication length, as opposed to the worst-case length.

By a symmetric argument $I(X;\Gamma^\pi_{X,Y} \mid Y) = O(1)$, and hence
$$\icostint_{\mu,0}(\pi) = O(1) + O(1) = O(1)$$
so $\ICint_{\mu,0}(\Eq) = O(1).$
\end{proof}

\bigskip
\noindent \emph{Hint.} Consider a protocol where Alice and Bob use their public randomness to select $n$ linearly independent vectors $r_1,\ldots,r_n \in \{0,1\}^n$.


\newpage 
\section{Direct sum (Bonus)}

Given a function $f : \calX \times \calY \to \{0,1\}$, define $f^k : \calX^k \times \calY^k \to \{0,1\}^n$ to be the function where
\[
f^n(x^{(1)},\ldots,x^{(k)},y^{(1)},\ldots,y^{(k)}) = \big( f(x^{(1)},y^{(1)}), \ldots f(x^{(k)}, y^{(k)})\big).
\]
This corresponds to the setting where Alice and Bob must compute the value of $f$ on $n$ different pairs of inputs instead of just $1$.

It might seem obvious that the complexity of computing $f^k$ is always $k$ times the complexity of computing $f$, but this is false in general! One counter-example is given by the equality function. Since $R_\epsilon(\Eq) = O(\log \frac1\epsilon)$, we have that $R_{2^{-\sqrt{k}}}(\Eq) = \sqrt{k}$ and so we might expect that $R_{2^{-\sqrt{k}}}(\Eq^k) = k^{3/2}$. In fact, the true randomized complexity of this function is much smaller.

\begin{theorem}
$R_{2^{-\sqrt{k}}}(\Eq^k) = O(k)$.
\end{theorem}

\begin{proof}
\replacethistext{Enter your proof here, if you complete it.}
\end{proof}


\newpage 
\section{Direct sum for information complexity}

One of the main advantages of working with internal information complexity is that in this setting every function \emph{does} satisfy the intuition that computing $k$ copies of a function is exactly $k$ times harder than computing a single copy of it.
\begin{theorem}
For every function $f : \calX \times \calY \to \{0,1\}$ and every $k \ge 1$,
\[
\ICint_{\mu,0}(f^k) = k \cdot \ICint_{\mu,0}(f).
\]
\end{theorem}

\begin{proof}
Clearly $\ICint_{\mu,0}(f^k) \leq k \cdot \ICint_{\mu,0}(f)$ since we can simply run any protocol for $f$ on each coordinate of input for $f^k$, and the information cost will $k$ times that of the protocol.

Let $\pi$ be a $0$-error protocol for $f^k$. We want to construct a $0$-error protocol $\pi'$ for $f$ with
$$\icostint_{\mu,0}(\pi') = \frac{1}{k}\icostint_{\mu,0}(\pi).$$
Protocol $\pi'$ operates as follows. Let $x$ be Alice's input, and $y$ be Bob's input. Alice and Bob use shared randomness to choose $i \in [k]$ uniformly at random. They set $(x^i,y^i) = (x,y)$. Let $\mu_\calX$ be the marginal distribution over $\calX$ from $\mu$, and define $\mu_\calY$ analogously. Alice and Bob use public randomness to sample $x^1,\dots,x^{i-1}$ according to $\mu_\calX$, and $y^{i+1},\dots, y^k$ according to $\mu_\calY$. Now Alice uses private randomness to sample $x^{i+1}, \dots, x^{k}$ according to $\mu$ conditioned on $y^{i+1},\dots, y^k$ respectively (so we get valid input pairs). Similarly Bob uses private randomness to sample $y^{1}, \dots, y^{i-1}$ according to $\mu$ conditioned on $x^{1},\dots, x^{i-1}$ respectively. Now Alice and Bob run protocol $\pi$ on inputs $(x^1,\dots, x^k)$ and $(y^1,\dots, y^k)$. The protocol outputs $f(x^i,y^i)$.

We've taken care to make sure the input to $\pi$ is distributed according to $\mu$ when we call it, and furthermore since $\pi$ is $0$-error for $f^k$, $\pi'$ is $0$-error for $f$. Let $X,Y$ be the random variables for inputs to $\pi'$. Let $I$ be a random variable for the random index in $[k]$ that $\pi'$ chooses. Let $X_1,\dots, X_n$ be random variables for $x$ coordinates, and $Y_1,\dots, Y_n$ be random variables for $y$ coordinates for $\pi$'s black-box call to $\pi'$. Use shorthand $\bar{X} = (X_1,\dots, X_k)$ and $\bar{Y} = (Y_1,\dots, Y_k)$. We now compute the mutual information from Alice's perspective:
\begin{align*}
I(Y;\Gamma^{\pi'}_{X,Y}\mid X) &= I(Y_I;\Gamma^\pi_{\bar{X},\bar{Y}} \mid I, \bar{X}, Y_{I+1} \dots Y_{k}) \\
&= \frac{1}{k}\sum_{i=1}^kI(Y_i; \Gamma^\pi_{\bar{X},\bar{Y}}\mid I=i, \bar{X}, Y_{i+1}\dots Y_{k}) \\
&= \frac{1}{k}\sum_{i=1}^k \left(I(Y_i,Y_{i+1}; \Gamma^\pi_{\bar{X},\bar{Y}}\mid \bar{X}, Y_{i+2}\dots Y_{k}) - I(Y_{i+1}; \Gamma^\pi_{\bar{X},\bar{Y}}\mid \bar{X}, Y_{i+2}\dots Y_k)\right) \\
&= \frac{1}{k}I(Y_1\dots Y_k ; \Gamma^\pi_{\bar{X},\bar{Y}}\mid \bar{X}) \\
&= \frac{1}{k}I(\bar{Y};\Gamma^\pi_{\bar{X},\bar{Y}}\mid \bar{X}).
\end{align*}
In first equality follows by conditioning on what Alice knows publicly. The second by expanding the expectation. The third by the chain rule, and the fourth by canceling terms. By a symmetric argument we can see that
$$I(X;\Gamma^{\pi'}_{X,Y}\mid Y) = \frac{1}{k}I(\bar{X};\Gamma^\pi_{\bar{X},\bar{Y}}\mid \bar{Y}).$$
Thus we have
$$\icostint_{\mu,0}(\pi') = I(Y;\Gamma^{\pi'}_{X,Y}\mid X) + I(X;\Gamma^{\pi'}_{X,Y}\mid Y) = \frac{1}{k}(I(\bar{Y};\Gamma^\pi_{\bar{X},\bar{Y}}\mid \bar{X}) + I(\bar{X};\Gamma^\pi_{\bar{X},\bar{Y}}\mid \bar{Y})) =  \frac{1}{k}\icostint_{\mu,0}(\pi)$$
as desired.
\end{proof}

\bigskip
\noindent \emph{Hint.} See Piazza for a hint on this question (and feel free to start a discussion thread there as well if you have some ideas but only a partial proof).


\newpage 
\section{Set disjointness III}

It is possible to obtain tight bounds on the internal information complexity of the \textsc{And} function. These tight bounds have been used to obtain \emph{exact} bounds on the communication complexity of the set disjointness function. To establish the tight asymptotic bound on its randomized communication complexity, however, all we need is the following result.

\begin{lemma}
Let $\mu$ be the distribution on $\{0,1\} \times \{0,1\}$ defined by $\mu(0,0) = \mu(0,1) = \mu(1,0) = \frac13$.
For every $\epsilon > 0$, there is a constant $c_\epsilon > 0$ for which
\[
\ICint_{\mu,\epsilon}(\textsc{And}) = c_\epsilon.
\]
\end{lemma}

This result and (extensions of) the direct sum property of internal information complexity can be used to bound the randomized communication complexity of the set disjointness function.

\begin{theorem}
For every (small enough) $\epsilon \ge 0$, 
\[
R_\epsilon(\Disj) = \Omega(n).
\]
\end{theorem}

\begin{proof}
\replacethistext{Enter your proof here.}
\end{proof}

\bigskip
\noindent \emph{Hint.} The ideas for the proof of the theorem in the last section might be helpful. Refer to Piazza for discussions, questions, or additional hints.




\end{document}