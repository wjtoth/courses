\documentclass[11pt]{amsart}
\usepackage{amsmath,amsthm,amsfonts,amssymb}
\usepackage{fullpage}
\usepackage[usenames]{xcolor}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newtheorem{proposition}{Proposition}
\theoremstyle{definition}
\newtheorem{definition}{Definition}
\newtheorem{exercise}{Exercise}
\newtheorem{problem}{Problem}
\newtheorem{challenge}{Challenge Problem}
\newtheorem{open}{Open Problem}
\theoremstyle{plain}

\newcommand{\argmin}{\mathrm{argmin}}
\newcommand{\calX}{\mathcal{X}}
\newcommand{\calY}{\mathcal{Y}}
\newcommand{\calZ}{\mathcal{Z}}
\newcommand{\cost}{\mathrm{cost}}
\newcommand{\disc}{\mathrm{disc}}
\newcommand{\discu}{\mathrm{disc_u}}
\newcommand{\Disj}{\textsc{Disj}}
\newcommand{\Eq}{\textsc{Eq}}
\newcommand{\GT}{\textsc{GT}}
\newcommand{\IP}{\textsc{IP}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\rank}{\mathrm{rank}}
\newcommand{\replacethistext}[1]{\textcolor{red}{#1}}

\newcommand{\exercises}{\bigskip \noindent\rule{8cm}{0.4pt} \medskip}



\title{Communication complexity: Chapter 3 \\ Distributional communication complexity}
\author{Eric Blais and Justin Toth}


\begin{document}

\maketitle


Functions can become much easier to compute in the communication complexity setting when we allow the protocols to make some errors. We can define this formally using the notion of \emph{distributional communication complexity}. We first explore this notion in its most natural setting, which corresponds to the uniform distribution.

\begin{definition}[Protocol error]
Fix any $\epsilon \ge 0$. A protocol $\pi$ \emph{computes} $f$ \emph{with error at most $\epsilon$ under the uniform distribution} if it correctly outputs the value $f(x,y)$ for at least an $1-\epsilon$ fraction of all inputs, i.e., if
\[
\Pr_{(x,y)}[ \pi(x,y) \neq f(x,y)] \le \epsilon
\]
when $(x,y)$ is drawn uniformly at random from $\calX \times \calY$.
\end{definition}

\begin{definition}[Uniform distributional complexity]
For any $\epsilon \ge 0$, the \emph{$\epsilon$-error distributional communication complexity} of $f : \mathcal{X} \times \mathcal{Y} \to \{0,1\}$ with respect to the uniform distribution,
\[
D_\epsilon^{\mathrm{unif}}(f),
\]
is the minimum communication cost of a protocol that computes $f$ with error at most $\epsilon$ under the uniform distribution.
\end{definition}

\exercises

\begin{exercise}
Show that $D_0^{\mathrm{unif}}(f) = D(f)$ and that for every $\epsilon > 0$, 
$D_\epsilon^{\mathrm{unif}}(f) \le D(f)$.
\end{exercise}

\begin{exercise}
Show that every $f : \mathcal{X} \times \mathcal{Y} \to \{0,1\}$ satisfies
$D_{1/2}^{\mathrm{unif}}(f) = 0$.
\end{exercise}


\newpage \section{Equality III}

The $\epsilon$-error distributional complexity of functions can sometimes be dramatically smaller than their deterministic complexity.

\begin{theorem}
For any $\epsilon \ge \frac1{2^n}$, $D_\epsilon^{\mathrm{unif}}(\textsc{Equality}) = 0$.
\end{theorem}

\begin{proof}
Consider the trivial protocol which just outputs $1$ on every pair of inputs. Then $\Pr[\pi(x,y)\neq \Eq(x,y)]$ is precisely the probability that $x=y$. Hence
$$\Pr[\pi(x,y)\neq \Eq(x,y)] = \Pr[x=y] = (\frac{1}{2})^n$$
since each bit of $x$ and $y$ is chosen independently and uniformly at random.
\end{proof}


\newpage \section{Uniform discrepancy}

The \emph{uniform discrepancy} of a function is a different way to measure how large ``nearly $f$-monochromatic'' rectangles can be. For this measure, it is convenient to represent the function with a $\pm1$-valued (instead of $\{0,1\}$-valued) matrix.

\begin{definition}[$\pm 1$-Matrix of a function]
The \emph{$\pm 1$-matrix} $M^\pm_f$ corresponding to the function $f : \calX \times \calY \to \{0,1\}$ is the $|\calX| \times |\calY|$-dimensional $\{-1,1\}$-valued matrix with rows indexed by $\calX$ and columns indexed by $\calY$ defined by
\[
\big(M^{\pm}_f\big)_{x,y} = (-1)^{f(x,y)}
\]
for every $x \in \calX$ and $y \in \calY$.
\end{definition}

\begin{definition}[Uniform discrepancy]
The \emph{uniform discrepancy} of $f : \calX \times \calY \to \{0,1\}$ is
\[
\discu(f) = \max_{A \subseteq \calX, B \subseteq \calY} \frac{1}{|\calX| \, |\calY|} \left| \sum_{x \in A, y \in B} \big(M^{\pm}_f\big)_{x,y} \right|.
\]
\end{definition}

We can use the uniform discrepancy to bound deterministic communication complexity.

\begin{theorem}
For every function $f : \calX \times \calY \to \{0,1\}$,
\[
\chi(f) \ge \discu(f)^{-1}
\] 
and so $D(f) \ge \log_2 \frac1{\discu(f)}$.
\end{theorem}

\begin{proof}
$\chi(f) \geq \discu(f)^{-1}$ if and only if
$$\chi(f)\max_{A \subseteq \calX, B \subseteq \calY}\left| \sum_{x \in A, y \in B} \big(M^{\pm}_f\big)_{x,y} \right| \geq |\calX||\calY|.$$

Consider a partition of the input space into $f$-monochromatic rectangles, $\mathcal R$, such that $|\mathcal R| = \chi(f)$. Then
\begin{align*}\chi(f)\max_{A \subseteq \calX, B \subseteq \calY}\left| \sum_{x \in A, y \in B} \big(M^{\pm}_f\big)_{x,y} \right|  &= \sum_{R\in \mathcal R} \max_{A \subseteq \calX, B \subseteq \calY}\left| \sum_{x \in A, y \in B} \big(M^{\pm}_f\big)_{x,y} \right|\\
&\geq\sum_{R \in \mathcal{R}} \left| \sum_{(x,y) \in R} \big(M^{\pm}_f\big)_{x,y} \right|\\
&= \sum_{R \in \mathcal R} |R|
\end{align*}
using that each $R$ is $f$-monochromatic to obtain the last equality. Since $\mathcal R$ partitions the input space,
$$\sum_{R \in \mathcal R} |R| = |\calX||\calY|$$
and hence the result holds.
\end{proof}


\newpage \section{Uniform discrepancy bound}

We can also use uniform discrepancy to bound the uniform distributional  communication complexity of functions.

\begin{lemma}
For every function $f : \calX \times \calY \to \{0,1\}$ and every $0 \le \epsilon < \frac12$, 
\[
D_\epsilon^{\mathrm{unif}}(f) \ge \log_2 \left( \frac{1-2\epsilon}{\discu(f)} \right).
\]
\end{lemma}

\begin{proof}
Let $\pi$ be the protocol attaining $D_\epsilon^{\mathrm{unif}}(f)$. Since $\Pr[\pi(x,y) \neq f(x,y)] \leq \epsilon$ and $\Pr[\pi(x,y) = f(x,y)] \geq 1-\epsilon$,
$$1-2\epsilon \leq \Pr[\pi(x,y) = f(x,y)] - \Pr[\pi(x,y) \neq f(x,y)].$$
Let $\{R_1, \dots, R_t\}$ be set of nearly $f$-monochromatic rectangles corresponding to the path from the root of $\pi$ to each respective leaf. Then
\begin{align*}\Pr[\pi(x,y) = f(x,y)] &= \sum_{i=1}^t \Pr((x,y) \in R_i)\Pr(\pi(x,y) \\
&= f(x,y) \mid(x,y) \in R_i)\\
&= \sum_{i=1}^t \frac{|R_i|}{|\calX||\calY|}\frac{|R_i\cap f^{-1}(\pi(x,y))|}{|R_i|}.
\end{align*}
Symmetrically
$$\Pr[\pi(x,y) \neq f(x,y)] = \frac{1}{|\calX||\calY|} \sum_{i=1}^t|R_i \cap f^{-1}(\pi(x,y)\oplus 1)|.$$
Thus we have
\begin{align*}1-2\epsilon &= \Pr[\pi(x,y) = f(x,y)] - \Pr[\pi(x,y) \neq f(x,y)] \\
&= \frac{1}{|\calX||\calY|} \sum_{i=1}^t|R_i\cap f^{-1}(\pi(x,y))| -|R_i \cap f^{-1}(\pi(x,y)\oplus 1)| \\
&\leq \frac{1}{|\calX||\calY|} \sum_{i=1}^t  \left| \sum_{(x,y) \in R_i} \big(M^{\pm}_f\big)_{x,y} \right|\\
&\leq t\cdot\discu(f).\end{align*}
Since $\log_2(t) = D_\epsilon^{\mathrm{unif}}(f)$ this completes the proof.
\end{proof}


\newpage \section{Inner product III}

The discrepancy method can be used to give optimal bounds on the distributional communication complexity of the inner product function over the uniform distribution.

\begin{theorem}
$\discu(\IP) \le 2^{-n/2}$ and, as a result, $D_\epsilon^{\mathrm{unif}}(\IP) \ge \Omega(n)$ for every $\epsilon < \frac12$.
\end{theorem}

\begin{proof}
For convenience, define $M:= M^{\pm}_\IP$ and let $\mathbf{1}_S$ denote characteristic vector of a set $S\subseteq [n]$. Now consider an entry $x,y$ of $M^TM$:
$$M^TM_{x,y} = \sum_{z\in\{0,1\}^n} (-1)^{x\cdot z}(-1)^{y\cdot z} = \sum_{z \in \{0,1\}^n}(-1)^{z\cdot(x+y)} = \begin{cases}
(-1)2^{n-1} + (1)2^{n-1}, &\text{if $x\neq y$} \\
\sum_{z \in \{0,1\}^n}1, &\text{otherwise}.
\end{cases}$$
Hence $$M^TM_{x,y} = \begin{cases}
0, &\text{if $x\neq y$} \\
2^n, &\text{otherwise}
\end{cases}$$
i.e. $M^TM = 2^nI$.

Consider a rectangle $R = A\times B$. Then we have
\begin{align*}
\left| \sum_{x \in A, y \in B} \big(M^{\pm}_f\big)_{x,y} \right|  &= \left| \mathbf{1}_A^T M \mathbf{1}_B\right | \\
&\leq ||\mathbf{1}_A|| \cdot || M\mathbf{1}_B|| &\text{by Cauchy-Schwarz} \\
&= \sqrt{|A|}\cdot\sqrt{\mathbf{1}^T_BM^TM\mathbf{1}_B} \\
&= 2^{n/2}\sqrt{|A|}\sqrt{\mathbf{1}^T_B\mathbf{1}_B} \\
&=2^{n/2}\sqrt{|A|}\sqrt{|B|}.
\end{align*}
Therefore
$$\discu(\IP) \leq \frac{1}{2^n2^n}\max_{A \subseteq \calX, B \subseteq \calY} 2^{n/2}\sqrt{|A|}\sqrt{|B|} \leq 2^{-2n}2^n2^\frac{n}{2} = 2^{-n/2}.$$
\end{proof}


\newpage \section{Distributional complexity}

The definitions we have seen so far in this chapter all generalize to arbitrary distributions over the domain of the function.

\begin{definition}[Protocol error]
Fix any $\epsilon \ge 0$. A protocol \emph{computes} $f : \calX \times \calY \to \{0,1\}$ \emph{with error at most $\epsilon$ under the distribution $\mu$ on $\calX \times \calY$} if it correctly outputs the value $f(x,y)$ for at least a $1-\epsilon$ measure of all inputs in $\mu$, i.e., if
\[
\Pr_{(x,y) \sim \mu}[ \pi(x,y) \neq f(x,y) ] \le \epsilon.
\]
\end{definition}

\begin{definition}[Distributional complexity]
For any $\epsilon \ge 0$, the \emph{$\epsilon$-error distributional communication complexity} of $f : \calX \times \calY \to \{0,1\}$ with respect to the distribution $\mu$ on $\calX \times \calY$,
\[
D_\epsilon^{\mu}(f),
\]
is the minimum communication cost of a protocol that computes $f$ with error at most $\epsilon$ under the distribution $\mu$.
\end{definition}

\begin{definition}[$\mu$-Discrepancy]
For any distribution $\mu$ on $\calX \times \calY$, 
the \emph{$\mu$-discrepancy} of $f : \calX \times \calY \to \{0,1\}$ is
\[
\disc_\mu(f) = \max_{A \subseteq \calX, B \subseteq \calY} 
\left| \mu\big( (A \times B) \cap f^{-1}(0) \big) - \mu\big( (A \times B) \cap f^{-1}(1)\big) \right|.
\]
\end{definition}

\begin{theorem}
For every function $f : \calX \times \calY \to \{0,1\}$ and every distribution $\mu$ over $\calX \times \calY$, 
\[
\chi(f) \ge \disc_\mu(f)^{-1}
\] 
and so $D(f) \ge \log_2 \frac1{\disc_\mu(f)}$.
\end{theorem}

\begin{proof}
	
$\chi(f) \geq \disc_\mu(f)^{-1}$ if and only if
$$\chi(f)\max_{A \subseteq \calX, B \subseteq \calY}\left|\mu((A\times B)\cap f^{-1}(0) -\mu((A\times B)\cap f^{-1}(1)) \right| \geq 1.$$

Consider a partition of the input space into $f$-monochromatic rectangles, $\mathcal R$, such that $|\mathcal R| = \chi(f)$. Then
\begin{align*}
&\chi(f)\max_{A \subseteq \calX, B \subseteq \calY}\left|\mu((A\times B)\cap f^{-1}(0) -\mu((A\times B)\cap f^{-1}(1)) \right| \\
& = \sum_{R\in \mathcal R}\max_{A \subseteq \calX, B \subseteq \calY}\left|\mu((A\times B)\cap f^{-1}(0) -\mu((A\times B)\cap f^{-1}(1)) \right|\\ 
&\geq \sum_{R \in \mathcal{R}}\left|\mu(R)\cap f^{-1}(0) -\mu(R\cap f^{-1}(1)) \right|\\ 
&= \sum_{R \in \mathcal{R}} \mu(R)
\end{align*}
using that each $R$ is $f$-monochromatic to obtain the last equality. Since $\mathcal R$ partitions the input space,
$$\sum_{R \in \mathcal R} \mu(R)=1$$
and hence the result holds.
\end{proof}

\exercises

\begin{exercise}
Show that when $\mu$ is the uniform distribution over $\calX \times \calY$, the definitions of $\mu$-discrepancy and uniform discrepancy are equivalent.
\end{exercise}

\begin{exercise}
Show that for every distribution $\mu$, $D^\mu_\epsilon(f) \le D(f)$.
\end{exercise}


\newpage \section{Discrepancy bound}

The $\mu$-discrepancy of a function can be used to give lower bounds on its $\mu$-distributional communication complexity.

\begin{lemma}
For every function $f : \calX \times \calY \to \{0,1\}$, every distribution $\mu$ on $\calX \times \calY$, and every $0 \le \epsilon < \frac12$,
\[
D_\epsilon^{\mu}(f) \ge \log_2 \left( \frac{1-2\epsilon}{\disc_\mu(f)} \right).
\]
\end{lemma}

\begin{proof}
Let $\pi$ be the protocol attaining $D_\epsilon^{\mu}(f)$. 

Since $\Pr[\pi(x,y) \neq f(x,y)] \leq \epsilon$ and $\Pr[\pi(x,y) = f(x,y)] \geq 1-\epsilon$,
$$1-2\epsilon \leq \Pr[\pi(x,y) = f(x,y)] - \Pr[\pi(x,y) \neq f(x,y)].$$
Let $\{R_1, \dots, R_t\}$ be set of nearly $f$-monochromatic rectangles corresponding to the path from the root of $\pi$ to each respective leaf. Then
$$\Pr[\pi(x,y) = f(x,y)] = \sum_{i=1}^t \Pr[(x,y) \in R_i \text{ and }\pi(x,y) = f(x,y) ] = \sum_{i=1}^t \mu(R_i \cap f^{-1}(\pi(x,y))).$$
Symmetrically
$$\Pr[\pi(x,y) \neq f(x,y)] = \sum_{i=1}^t\mu(R_i \cap f^{-1}(\pi(x,y)\oplus 1)).$$
Thus we have
\begin{align*}1-2\epsilon &= \Pr[\pi(x,y) = f(x,y)] - \Pr[\pi(x,y) \neq f(x,y)] \\
&=\sum_{i=1}^t|\mu(R_i\cap f^{-1}(\pi(x,y))) -\mu(R_i \cap f^{-1}(\pi(x,y)\oplus 1))| \\
&\leq \sum_{i=1}^t \disc_\mu(f)\\
&= t\cdot\disc_\mu(f).\end{align*}
Since $\log_2(t) = D_\epsilon^{\mu}(f)$ this completes the proof.
\end{proof}


\newpage \section{Equality IV}

Using the discrepancy bound, we can show that there are distributions for which the equality function requires Alice and Bob to exchange \emph{some} bits of communication to obtain a non-trivial error.

\begin{theorem}
There exists a distribution $\mu$ over $\{0,1\}^n \times \{0,1\}^n$ for which 
$
\disc_\mu(\Eq) \leq \frac18 \frac{2^n}{(2^n-1)}.
$
and so
\[
D^\mu_\epsilon(\Eq) \ge 1
\]
for every $\epsilon \le \frac38$.
\end{theorem}

\begin{proof}
    We define $\mu : \{0,1\}^n  \times \{0,1\}^n \rightarrow [0,1]$ as follows. For any $(x,y) \in \{0,1\}^n\times \{0,1\}^n$ let
    $$ \mu(x,y) = \begin{cases}
        \frac{1}{2^{n+1}}, &\text{if } x=y \\
        \frac{1}{2^{n+1}(2^n-1)}, &\text{otherwise}.
    \end{cases}.$$
    To see that $\mu$ is a probability distribution observe that
    \begin{align*}\sum_{x,y \in \{0,1\}^n\times \{0,1\}^n} \mu(x,y) &= \sum_{x\in\{0,1\}^n}\mu(x,x) + \sum_{x\neq y \in \{0,1\}^n\times \{0,1\}^n} \mu(x,y)\\
     &= 2^n \frac{1}{2^{n+1}} + (2^{2n}-2^n)\frac{1}{2^{n+1}(2^n-1)}\\
    &= \frac{1}{2} + \frac{1}{2} = 1.
    \end{align*}
    Let $R = A\times B \subseteq \{0,1\}^n\times\{0,1\}^n$ be a rectangle such that
    $$\left|\mu(R\cap \Eq^{-1}(0)) - \mu(R\cap \Eq^{-1}(1))\right| = \disc_\mu(\Eq).$$
    Choose such $R$ so that $|R|$ is minimized. We proceed by case distinction. 
    
    First suppose that $\mu(R\cap \Eq^{-1}(1)) \geq \mu(R\cap \Eq^{-1}(0)).$ If $(x,x), (y,y) \in R\cap \Eq^{-1}(1)$ then $(x,y), (y,x) \in R\cap \Eq^{-1}(0)$. Thus we have that 
    \begin{align*}&\mu(R\cap \Eq^{-1}(1)) - \mu(R\cap \Eq^{-1}(0))\\ 
        &\leq \left|R\cap \Eq^{-1}(1)\right|(\frac{1}{2^{n+1}})-\left(\left|R\cap \Eq^{-1}(1)\right|^2-\left|R\cap \Eq^{-1}(1)\right|\right)(\frac{1}{2^{n+1}(2^n-1)}).
    \end{align*}
    This upperbound is maximized when $|R\cap \Eq^{-1}(1)|=2^{n-1}$. Therefore in this case $\disc_\mu(\Eq) \leq \frac{1}{8} + o(1).$
    
    Now consider the other case, where $\mu(R\cap \Eq^{-1}(0)) > \mu(R\cap \Eq^{-1}(1)).$ Suppose there exists $x \in \{0,1\}^n$ such that $(x,x) \in R.$ Consider $R' = A\times B\backslash\{x\}.$ Observe that
    \begin{align*}
        &\left|\mu(R'\cap \Eq^{-1}(0)) - \mu(R'\cap \Eq^{-1}(1))\right|\\ & = \mu(R\cap \Eq^{-1}(0))-\mu(R\cap \Eq^{-1}(1))  + \mu(x,x)- \sum_{y\in A\backslash\{x\}} \mu(x,y)\\
        &= \mu(R\cap \Eq^{-1}(0))-\mu(R\cap \Eq^{-1}(1)) + \frac{1}{2^{n+1}} - |A\backslash\{x\}|\frac{1}{2^{n+1}(2^n-1)} \\
        &\geq \mu(R\cap \Eq^{-1}(0))-\mu(R\cap \Eq^{-1}(1)) + \frac{1}{2^{n+1}} - \frac{1}{2^{n+1}} \\
        &= \mu(R\cap \Eq^{-1}(0))-\mu(R\cap \Eq^{-1}(1))\\
        &= \disc_\mu(\Eq).
    \end{align*}
    This contradicts that $|R|$ is minimized. Therefore $A\cap B = \emptyset$. Thus we have
    \begin{align*}
        \mu(R\cap \Eq^{-1}(0))-\mu(R\cap \Eq^{-1}(1)) &\leq \mu(R\cap \Eq^{-1}(0))\\
        &= |R\cap \Eq^{-1}(0)|\frac{1}{2^{n+1}(2^n-1)} \\
        &\leq |A|(2^n-|A|)\frac{1}{2^{n+1}(2^n-1)}\\
        &\leq \frac{2^{n}(2^{n/2}-1)}{2^{n+1}(2^n-1)}\\
        &= O\left(\frac{1}{2^{n/2}}\right).
    \end{align*}

    Hence we have shown that $\disc_\mu(\Eq) \leq \frac{1}{8} + o(1) \leq \frac{1}{8}\left(\frac{2^n}{2^n-1}\right)$.
\end{proof}



\newpage \section{Equality V (Bonus)}

Show that the bound in the last section is essentially tight in that the distributional communication complexity of the equality function is constant when $\epsilon$ is a positive constant.

\begin{theorem}
For every distribution $\mu$ over $\{0,1\}^n \times \{0,1\}^n$,
\[
D^\mu_{1/4}(\Eq) = O(1).
\]
\end{theorem}

\begin{proof}
For $r \in \{0,1\}^n$ let  $\pi_r$ be the deterministic protocol where 
$$\pi_r(x,y) = \begin{cases} 1, &\text{if } (x\cdot r)  = (y\cdot r) \\ 0, &\text{otherwise}\end{cases}.$$
This protocol has communication cost $O(1)$ since Alice can send Bob $x\cdot r$ in $1$ bit, and Bob can send Alice $y \cdot r$ in $1$ bit.

Consider a uniformly random $r \in \{0,1\}$. If $x=y$ then $\pi_r(x,y) = 1$. If $x\neq y$ then $\pi_r(x,y) =1$ with probability $\frac{1}{4}$. Let $X_r(x,y)$ be the indicator random variable indicating if $\pi_r(x,y)$ fails on input $(x,y)$. Thus we see that for any $(x,y)$,
$$\mathbb{E}_r[X_r(x,y)] \leq \frac{1}{4}.$$
Thus we have
$$\mathbb{E}_{(x,y)\sim \mu}\mathbb{E}_r[X_r(x,y)] \leq \frac{1}{4}.$$
Rearranning order of expectation yields
$$\mathbb{E}_{r}\mathbb{E}_{(x,y)\sim \mu}[X_r(x,y)] \leq \frac{1}{4}.$$
And thus there exists an $r \in \{0,1\}$ such that
$$\Pr_\mu(\pi_r \text{ fails}) \leq \mathbb{E}_{(x,y)\sim \mu}[X_r(x,y)] \leq \frac{1}{4}.$$
\end{proof}



\newpage \section{Corruption bound}

The \emph{corruption bound} is another powerful technique for proving lower bounds in distributional communication complexity.

\begin{lemma}
Fix a function $f : \calX \times \calY \to \{0,1\}$ and a distribution
$\mu$ on $\calX \times \calY$.
If there exist parameters $\alpha, \beta > 0$ for which
every rectangle $R \subseteq \calX \times \calY$ satisfies 
\[
\mu(R \cap f^{-1}(0)) \ge \alpha \cdot \mu(R) - \beta
\]
then
\[
D^\mu_\epsilon(f) \ge \log\left( \frac{\alpha \cdot \mu(f^{-1}(1)) - \epsilon}{\beta}\right).
\]
\end{lemma}

\begin{proof}
Let $\pi$ be the protocol attaining $D_\epsilon^{\mu}(f)$.
Let $\{R_1, \dots, R_t\}$ be set of nearly $f$-monochromatic rectangles corresponding to the path from the root of $\pi$ to each respective leaf.

$$1= \sum_{i=1}^t \mu(R_i) = \sum_{i=1}^t \mu(R_i \cap f^{-1}(0)) + \mu(R_i\cap f^{-1}(1)) \geq \alpha - t \beta +\mu(f^{-1}(1))$$
Thus
$$t \geq \frac{\alpha + \mu(f^{-1}(1)) -1}{\beta}.$$
Now we claim that 
$$\alpha + \mu(f^{-1}(1)) -1 \geq \alpha\cdot \mu(f^{-1}(1)) -\epsilon.$$
This claim is equivalent to
$$1-\epsilon \leq \alpha(\mu(f^{-1}(0))) +\mu(f^{-1}(1)).$$
Since 
$$\alpha(\mu(f^{-1}(0))) +\mu(f^{-1}(1)) = 1-(1-\alpha) \mu(f^{-1}(0))$$
the claim holds since the probability of failure is at least $(1-\alpha)f^{-1}(0)$ \replacethistext{(I didn't have time to formalize)}.
\end{proof}



\newpage \section{Set Disjointness II}

The corruption bound can be used to prove a strong lower bound on the distributional communication complexity of the set disjointness function. The key claim that is used to obtain this result is the following combinatorial statement.

\begin{proposition}[\textsc{Bonus}]
For every $\alpha >0$ there exists $\delta>0$ such that every rectangle $R = A \times B \subseteq 2^{[n]} \times 2^{[n]}$
that satisfies 
\[
\Pr_{(S,T) \in R}[ S \cap T = \emptyset ] \ge 1-\alpha
\]
has size bounded by
\[
|A| \le 2^{-\delta \sqrt{n}} \binom{n}{\sqrt{n}}
\qquad \mbox{or} \qquad
|B| \le 2^{-\delta \sqrt{n}} \binom{n}{\sqrt{n}}.
\]
\end{proposition}

\begin{proof}
\replacethistext{Enter your proof here, if you complete it.}
\end{proof}

Use the claim to complete the lower bound on the communication complexity of the set disjointness function.

\begin{theorem}
Let $\mu$ be the uniform distribution on pairs $(S,T) \in 2^{[n]} \times 2^{[n]}$ that satisfy $|S| = |T| = \sqrt{n}$. Then
\[
D_\epsilon^{\mu}(\textsc{Disj}) \ge \Omega(\sqrt{n}).
\]
\end{theorem}

\begin{proof}
From Proposition $1$, if $R = A\times B \subseteq 2^{[n]}\times 2^{[n]}$ is rectangle such that
$$\mu(R) \geq 2^{-\delta\sqrt{n}}$$ then
$$\mu(R\cap \Disj^{-1}(0)) > \alpha \mu(R).$$	
Thus any rectangle $R$ satisfies
$$\mu(R\cap \Disj^{-1}(0)) > \alpha \mu(R) - 2^{-\delta\sqrt{n}}.$$
So by the corruption bound with $\alpha$ and $\beta = 2^{-\delta\sqrt{n}}$ we obtain
$$D^{\mu}_\epsilon(\Disj) \geq \log(\frac{\alpha\cdot \mu(f^{-1}(1)) - \epsilon}{2^{-\delta\sqrt{n}}}) \geq \Omega(\sqrt{n}).$$\end{proof}



\end{document}