\documentclass[letterpaper,12pt,oneside,onecolumn]{article}
\usepackage[margin=1in, bottom=1in, top=1in]{geometry} %1 inch margins
\usepackage{amsmath, amssymb, amstext}
\usepackage{fancyhdr}
\usepackage{mathtools}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{theorem}
\usepackage{tikz}
\usepackage{tkz-berge}
\usepackage[colorlinks]{hyperref}
\usepackage{bookmark}

%Macros
\newcommand{\A}{\mathbb{A}} \newcommand{\C}{\mathbb{C}}
\newcommand{\D}{\mathbb{D}} \newcommand{\F}{\mathbb{F}}
\newcommand{\N}{\mathbb{N}} \newcommand{\R}{\mathbb{R}}
\newcommand{\T}{\mathbb{T}} \newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
 
 
\newcommand{\cA}{\mathcal{A}} \newcommand{\cB}{\mathcal{B}}
\newcommand{\cC}{\mathcal{C}} \newcommand{\cD}{\mathcal{D}}
\newcommand{\cE}{\mathcal{E}} \newcommand{\cF}{\mathcal{F}}
\newcommand{\cG}{\mathcal{G}} \newcommand{\cH}{\mathcal{H}}
\newcommand{\cI}{\mathcal{I}} \newcommand{\cJ}{\mathcal{J}}
\newcommand{\cK}{\mathcal{K}} \newcommand{\cL}{\mathcal{L}}
\newcommand{\cM}{\mathcal{M}} \newcommand{\cN}{\mathcal{N}}
\newcommand{\cO}{\mathcal{O}} \newcommand{\cP}{\mathcal{P}}
\newcommand{\cQ}{\mathcal{Q}} \newcommand{\cR}{\mathcal{R}}
\newcommand{\cS}{\mathcal{S}} \newcommand{\cT}{\mathcal{T}}
\newcommand{\cU}{\mathcal{U}} \newcommand{\cV}{\mathcal{V}}
\newcommand{\cW}{\mathcal{W}} \newcommand{\cX}{\mathcal{X}}
\newcommand{\cY}{\mathcal{Y}} \newcommand{\cZ}{\mathcal{Z}}

\newcommand{\tight}{tight}
\newcommand{\Mtight}{\cM_{\tight}}
\newcommand{\uni}{uni}
\newcommand{\Muni}{\cM_{\uni}}
\newcommand{\core}{\ensuremath{\mbox{core}}}
\DeclareMathOperator{\suppOp}{supp}
\newcommand{\supp}{\suppOp}

\DeclareMathOperator{\convOp}{conv}
\newcommand{\conv}{\convOp}

\newcommand{\level}{\mathrm{lev}}
\newcommand{\set}{\mathrm{set}}
\newcommand{\fix}{\mathrm{Fix}}

\DeclareMathOperator{\exOp}{excess}
\newcommand{\ex}{\exOp}

\DeclareMathOperator{\symOp}{diff}
\newcommand{\sym}{\symOp}

\DeclareMathOperator{\parent}{parent}
\DeclareMathOperator{\optop}{top}
\DeclareMathOperator{\excess}{excess}
\DeclareMathOperator{\opspan}{span}


\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}


\newenvironment{proof}{{\bf Proof:  }}{\hfill\rule{2mm}{2mm}}
\newenvironment{proofof}[1]{{\bf Proof of #1:  }}{\hfill\rule{2mm}{2mm}}
\newenvironment{proofofnobox}[1]{{\bf#1:  }}{}\newenvironment{example}{{\bf Example:  }}{\hfill\rule{2mm}{2mm}}

%\renewcommand{\thesection}{\lecnum.\arabic{section}}
%\renewcommand{\theequation}{\thesection.\arabic{equation}}
%\renewcommand{\thefigure}{\thesection.\arabic{figure}}

\newtheorem{fact}{Fact}[section]
\newtheorem{lemma}[fact]{Lemma}
\newtheorem{theorem}[fact]{Theorem}
\newtheorem{definition}[fact]{Definition}
\newtheorem{corollary}[fact]{Corollary}
\newtheorem{proposition}[fact]{Proposition}
\newtheorem{claim}[fact]{Claim}
\newtheorem{exercise}[fact]{Exercise}
\newtheorem{note}[fact]{Note}
\newtheorem{conjecture}[fact]{Conjecture}
\newtheorem{question}[fact]{Question}

\newcommand{\size}[1]{\ensuremath{\left|#1\right|}}
\newcommand{\ceil}[1]{\ensuremath{\left\lceil#1\right\rceil}}
\newcommand{\floor}[1]{\ensuremath{\left\lfloor#1\right\rfloor}}

%END MACROS
%Page style
\pagestyle{fancy}

\listfiles

\raggedbottom

\lhead{CS860 Proposal}
\rhead{W. Justin Toth} %
\renewcommand{\headrulewidth}{1pt} %heading underlined
%\renewcommand{\baselinestretch}{1.2} % 1.2 line spacing for legibility (optional)

\begin{document}
\section{Extended Formulations and Communication Complexity}
\paragraph{}
Consider a polytope $P\subseteq \R^n$ and a polyhedron $Q\subseteq \R^n$ containing $P$ (called a \emph{polyhedral pair}). For intuition, think of $P$ as the convex hull of the set of solutions to some NP-hard problem, for which we do not know a compact linear programming description, and think of $Q$ as a relaxation of $P$ for which are interested in finding a compact description of.

We say that a system 
$$E^\leq x + F^\leq y \leq g^\leq, E^= x + F^= y = g^=$$
in $\R^{n+k}$ is an \emph{extended formulation} of $(P,Q)$ if the polyhedron
$$R:=\{x \in \R^n: \exists y \in \R^k: E^\leq x + F^\leq y \leq g^\leq, E^= x + F^= y = g^=\}$$
contains $P$ and is contained in $Q$. The number of inequalities in the system determines the size of the extended formulations. The \emph{extension complexity} of $(P,Q)$, $xc(P,Q)$, is the minimum size of an extended formulation of $(P,Q)$.

Usually one studies the case $P=Q$, but for our purposes we are interested in the extension of complexity of a specific relaxation, and hence need the fully general definiton above. Yannakakis' seminal result \cite{yannakakis1991expressing} gives an algebraic expression of $xc(P,Q)$. To state the result we will need two more pieces of terminology.

Let $P = \text{conv}(\{v_1,\dots,v_p\})$ and $Q = \{x\in \R^n: Ax\geq b\}$ where $A \in \R^{m \times n}$ and $b \in \R^m$. The \emph{slack matrix} $S$ of $(P,Q)$ is defined as follows. For $i \in[m]$ and $j \in [p]$,
$$S_{i,j} = A_iv_j - b_i.$$
Note that the slack matrix depends on the inner and outer descriptions of $P$ and $Q$ respectively. We say that $T\in \R^{m\times r}_{\geq 0}$, $U \in \R^{r\times n}_{\geq 0}$ is a \emph {rank $r$ non-negative factorization} of $S$ if $S = TU$. The \emph{non-negative rank of $S$} is 
$$\text{rk}_x(S) L= \min\{r: S \text{ has a rank $r$ non-negative factorization}\}.$$ 
Equivalently a non-negative factorization of $S$ of rank at most $r$ is a decomposition of $S$ into the sum of at most $r$ rank $1$ matrices.
\begin{theorem}
    (Yannakakis) For $P$ of dimension at least $1$, we have $\text{xc}(P,Q) \in \{\text{rk}_+(S), \text{rk}_+(S) - 1\}.$
\end{theorem}
\paragraph{}
Faenza, Fiorini, Grappe, and Tiwary~\cite{faenza2015extended} identified a model of communication which is intimately connected to non-negative rank (and thus to extension complexity). Let $\Pi$ be a communication protocol where Alice and Bob each have private randomness. Alice receives $i \in [n]$ and Bob receives $j \in [p]$. They exchange bits in the usual way according to a protocol tree and at the end \emph{either one} of the players outputs some non-negative number $\alpha$. We say $\Pi$ computes $S$ in expectation if for every $i \in [n]$, $j \in [p]$, the expected value of $\alpha$ is $S_{i,j}$. The cost of $\Pi$ is the maximum number of bits exchanged by Alice and Bob on any input pair, not counting the encoding size of $\alpha$. The expected communication complexity of $S$, denoted $R^\text{cc}_{\text{exp}}(S)$, is the minimum cost of a protocol computing $S$ in expectation.
\begin{theorem}(Faenza et al.) Provided $\text{rk}_x(S) \neq 0$, $$R^\text{cc}_{\text{exp}}(S) = \log \text{rk}_x(S) + \Theta(1) = \log\text{xc}(P,Q) + \Theta(1).$$
The second equality is a direct consequence of Yannakakis' Theorem. 
\end{theorem}
\subsection{The goal of this section is Faenza's 2015 result}
\section{The Knapsack Cover Problem}
\subsection{The goal here is to describe all the knapsack cover stuff that has been done in the natural space}
\section{Extended Formulations for Knapsack Cover}
\subsection{Here we discuss extended formulations for KC, and communication protocols}
\section{Lower Bounds}
\subsection{some ideas for lower bounding communication cost of KCI slack matrix}

\bibliography{references.bib}
\bibliographystyle{plain}
\end{document}
