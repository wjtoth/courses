\documentclass[letterpaper,12pt,oneside,onecolumn]{article}
\usepackage[margin=1in, bottom=1in, top=1in]{geometry} %1 inch margins
\usepackage{amsmath, amssymb, amstext}
\usepackage{fancyhdr}
\usepackage{mathtools}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{theorem}
\usepackage{tikz}
\usepackage{tkz-berge}

%Macros
\newcommand{\A}{\mathbb{A}} \newcommand{\C}{\mathbb{C}}
\newcommand{\D}{\mathbb{D}} \newcommand{\F}{\mathbb{F}}
\newcommand{\N}{\mathbb{N}} \newcommand{\R}{\mathbb{R}}
\newcommand{\T}{\mathbb{T}} \newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
 
 
\newcommand{\cA}{\mathcal{A}} \newcommand{\cB}{\mathcal{B}}
\newcommand{\cC}{\mathcal{C}} \newcommand{\cD}{\mathcal{D}}
\newcommand{\cE}{\mathcal{E}} \newcommand{\cF}{\mathcal{F}}
\newcommand{\cG}{\mathcal{G}} \newcommand{\cH}{\mathcal{H}}
\newcommand{\cI}{\mathcal{I}} \newcommand{\cJ}{\mathcal{J}}
\newcommand{\cK}{\mathcal{K}} \newcommand{\cL}{\mathcal{L}}
\newcommand{\cM}{\mathcal{M}} \newcommand{\cN}{\mathcal{N}}
\newcommand{\cO}{\mathcal{O}} \newcommand{\cP}{\mathcal{P}}
\newcommand{\cQ}{\mathcal{Q}} \newcommand{\cR}{\mathcal{R}}
\newcommand{\cS}{\mathcal{S}} \newcommand{\cT}{\mathcal{T}}
\newcommand{\cU}{\mathcal{U}} \newcommand{\cV}{\mathcal{V}}
\newcommand{\cW}{\mathcal{W}} \newcommand{\cX}{\mathcal{X}}
\newcommand{\cY}{\mathcal{Y}} \newcommand{\cZ}{\mathcal{Z}}

\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}


\newenvironment{proof}{{\bf Proof:  }}{\hfill\rule{2mm}{2mm}}
\newenvironment{proofof}[1]{{\bf Proof of #1:  }}{\hfill\rule{2mm}{2mm}}
\newenvironment{proofofnobox}[1]{{\bf#1:  }}{}\newenvironment{example}{{\bf Example:  }}{\hfill\rule{2mm}{2mm}}

%\renewcommand{\thesection}{\lecnum.\arabic{section}}
%\renewcommand{\theequation}{\thesection.\arabic{equation}}
%\renewcommand{\thefigure}{\thesection.\arabic{figure}}

\newtheorem{fact}{Fact}[section]
\newtheorem{lemma}[fact]{Lemma}
\newtheorem{theorem}[fact]{Theorem}
\newtheorem{definition}[fact]{Definition}
\newtheorem{corollary}[fact]{Corollary}
\newtheorem{proposition}[fact]{Proposition}
\newtheorem{claim}[fact]{Claim}
\newtheorem{exercise}[fact]{Exercise}
\newtheorem{note}[fact]{Note}
\newtheorem{conjecture}[fact]{Conjecture}

\newcommand{\size}[1]{\ensuremath{\left|#1\right|}}
\newcommand{\ceil}[1]{\ensuremath{\left\lceil#1\right\rceil}}
\newcommand{\floor}[1]{\ensuremath{\left\lfloor#1\right\rfloor}}

%END MACROS
%Page style
\pagestyle{fancy}

\listfiles

\raggedbottom

\lhead{\today}
\rhead{W. Justin Toth CS761-Randomized Algorithms Assignment 2} %CHANGE n to ASSIGNMENT NUMBER ijk TO COURSE CODE
\renewcommand{\headrulewidth}{1pt} %heading underlined
%\renewcommand{\baselinestretch}{1.2} % 1.2 line spacing for legibility (optional)

\begin{document}
%Problem 1
\section{}
\paragraph{}
Let $v_1, \dots, v_m \in \{0,1\}^\ell$ be a set of vectors such that any subset of size at most $k$ is linearly independent modulo $2$. We denote the $j$-th entry of $v_i$ by $v_{i,j}$. Let $u$ be chosen uniformly at random from $\{0,1\}^\ell$. Let $X_i$ be the random variable defined by
$$X_i = (\sum_{j=1}^\ell v_{i,j}u_j) \mod 2.$$
We want to show that $X_1, \dots, X_m$ are $k$-wise independent random bits. That is to say, for all $I \subseteq [m]$ with $1\leq |I| \leq k$, and $x \in \{0,1\}^m$,
$$Pr(\bigcap_{i\in I} X_i = x_i) = \Pi_{i\in I} Pr(X_i = x_i).$$
We begin by observing that if the vector $v_i$ is the $0$ vector for any $i \in I$ then the set of vectors $V := \{v_i : i \in I\}$ fails to be linearly independent (modulo $2$), which contradicts our hypothesis. Hence for all $i \in I$, there exists $j \in [\ell]$ such that $v_{i,j} \neq 0$. Further from linear independence we observe that $\ell \geq k$.
\paragraph{}
On the one hand, for any $i \in I$, $v_i \neq 0$ and so
$$Pr(X_i = x_i) = \frac{1}{2}$$
since $X_i = x_i$ if and only if
$$P_i := \sum_{j \in s(v_i)} u_j = \begin{cases} \text{odd}, &\text{if $x_i = 1$} \\
\text{even}, &\text{otherwise}
\end{cases}$$
where $s(v_i) := \{j \in [\ell] : v_{i,j} \neq 0\}$. Note that since $s(v_i) \neq \emptyset$ and $u$ is chosen uniformly at random, the parity $P_i$ is chosen uniformly at random. Therefore
$$\Pi_{i \in I} Pr(X_i = x_i) = \Pi_{i \in I} \frac{1}{2} = \frac{1}{2^{|I|}}.$$
\paragraph{}
Now on the other hand, $\bigcap_{i \in I} X_i = x_i$ happens if and only if 
$$ Vu = x_I$$
where we define the matrix $V$ with $|I|$ rows and $\ell$ columns so that $V_{i,j} = v_{i,j}$, and $x_I$ is the vector $x$ restricted to entries indexed by $I$. Here we are taking all operations in the field $\F_2$. Since the rank of $V$ is $|I|$ we can find by Gaussian elimination a permuation matrix $E$ (after possibly permuting the columns and associated indices of $u$, without loss of generality) so that
$$EV = \begin{bmatrix}
I & B
\end{bmatrix}$$
for some binary matrix $B$ with $\ell - |I|$ columns. Then
$$\begin{bmatrix} I & B \end{bmatrix}u = Ex_I$$
and hence the choices of entries for $u_1, \dots, u_{|I|}$ are uniquely fixed given the choices of entries for $u_{|I| + 1}, \dots, u_{\ell}$. So there are $2^{\ell - |I|}$ choices for $u$ satisfying our matrix equation. There are $2^\ell$ total choices for $u$ overall, and so we have
$$Pr(\bigcap_{i \in I} X_i = x_i) = \frac{2^{\ell-|I|}}{2^\ell}  = \frac{1}{2^{|I|}} = \Pi_{i \in I} Pr(X_i = x_i)$$
as desired. $\blacksquare$
%Problem 2
\newpage
\section{}
\paragraph{}
Suppose we have $n$ items to be hashed into $n$ bins. Let $h$ be a hash function chosen from a $k$-universal family. We $\textit{overload}$ the notation ${n \choose k}$ to mean both the number of ways to choose $k$ elements from $n$, and the set of subsets of $[n]$ of size $k$. Which version we use will always be clear from context. Define a $k$-collision to be the event that $k$ distinct items are hashed to the same bin. For a $\textit{set}$ $S \in {n\choose k}$ let $X_S$ be the random indicator variable which is $1$ when $h(x) = h(y)$ for all $x,y \in S$, and $X_S$ is $0$ otherwise.
\paragraph{}
Let $X$ be a random variable counting the number of $k$-collisions when our $n$ items are hased using $h$ chosen from a $k$-universal family. Then $X = \sum_{S \in {n\choose k}} X_S$ and so we have
$$E[X] = E[\sum_{S \in {n\choose k}} X_S] = \sum_{S \in {n\choose k}} E[X_S] = \sum_{S \in {n \choose k}} Pr(|h(S)| = 1) \leq  \sum_{S \in {n \choose k}} \frac{1}{n^{k-1}}$$
where the inequality follows since $h$ is a $k$-universal hash function, and $S$ is a set of size $k$. So then
$$E[X] \leq {n\choose k} \frac{1}{n^{k-1}}.$$
Therefore if we let $t = 2n^{1-k}{n\choose k}$ then by Markov's inequality
$$Pr(X \geq t) \leq Pr(X \geq 2E[X]) \leq \frac{1}{2}.$$
\paragraph{}
Now let $Y$ be the maximum number of items hashed into a single bin. Then $X \geq {Y \choose k}$ and so
$$Pr({Y\choose k}\geq t) \leq Pr(X \geq t) \leq \frac{1}{2}$$
Which implies that 
$$Pr(Y \geq (2n)^{\frac{1}{k}}) \leq \frac{1}{2}.$$
That is, with probability at most $\frac{1}{2}$, the maximum load is larger than $(2n)^{\frac{1}{k}}$. 
\paragraph{}
We want to find the smallest $k$ for which 
$$Pr(Y \leq 3\ln n / \ln \ln n) \geq \frac{1}{2}.$$
Thus we need to choose the smallest $k$ satisfying
$$(2n)^{\frac{1}{k}} \leq \frac{3\ln n}{\ln \ln n}.$$
Rearranging we see that setting 
$$k \geq \frac{\ln2 + \ln n}{\ln(3) + \ln\ln n - \ln\ln\ln n}$$
would do the job. Hence the smallest choice of $k$ will be the first integer greater than this value, i.e.
$$k = \ceil{\frac{\ln2 + \ln n}{\ln(3) + \ln\ln n - \ln\ln\ln n}}.$$
$\blacksquare$
%Problem 3
\newpage
\section{}
I discussed this problem with Sharat.

\paragraph{}
Suppose we have a list $L$ of $n$ distinct numbers, whose sorted list would we $\{x_1,\dots, x_n\}$. We want to find a $k$-approximate median. I.e. we want to return some $x \in [x_{\frac{n}{2} - k}, x_{\frac{n}{2} + k}]\cap L$. Consider sampling $m$ numbers from $L$ and returning the median $c$ of these $m$ numbers as the true median of $L$.
\paragraph{}
Let $L'$ be list of $m$ sampled numbers. For each $i \in L'$ let $X_{i,<}$ be an indicator variable for the event that $i \in \{x_1, \dots, x_{\frac{n}{2} -k -1}\}$. Then, since we sample without replacement,
$$Pr(X_{i, <} =1) = (\frac{n}{2} -k -1) / n = \frac{n-2k-2}{2n}$$
Let $X_<$ be the number of elements in $L'$ which lie in $\{x_1, \dots, x_{\frac{n}{2} -k -1}\}$. Then $X_< = \sum_{i \in L'} X_{i,<}$.
Let $X_>$ be the number of elements in $L'$ which lie in $\{x_{\frac{n}{2} + k + 1}, \dots, x_n\}$. Since we sample uniformly and
	$$|\{x_{\frac{n}{2} + k + 1}e, \dots, x_n\}| =  |\{x_1, \dots, x_{\frac{n}{2} -k -1}\}| = \frac{n}{2} - k - 1$$
we observe that $X_<$ and $X_>$ have the same distribution.
\paragraph{}
Now we see that $c$ fails to be a $k$-approximate median for $L$ if and only if $X_< \geq \frac{m}{2}$ or $X_> \geq \frac{m}{2}$. Since $X_<$ and $X_>$ have the same distribution, the probability of failure is thus
$$2Pr(X_< \geq \frac{m}{2}).$$
We plan to bound this using a Chernoff bound. First we compute the expectation,
$$E[X_<] = \sum_{i\in L'} Pr(X_{i,<} = 1)  = \sum_{i \in L'} \frac{n-2k-2}{2n} = \frac{m(n-2(k+1))}{2n}.$$
So 
$$Pr(X_< \geq \frac{m}{2}) = Pr(X_< \geq (1+\delta) E[X_<])$$
where $$\delta =\frac{2(k+1)}{n-2(k+1)}.$$
We proceed by assuming $k < \frac{n}{4}-1$ so that $0<\delta <1$. Ultimately we are interested in results on how many samples $m$ we need to take to obtain a certain quality of approximation, and so bounding $k$ is reasonable.
\paragraph{}
Since $0 < \delta < 1$, and $X_<$ is a sum of independent random coin flips, we can apply the second Chernoff bound from Theorem on page $4$ of Lecture $2$, obtaining
$$Pr(X_< \geq \frac{m}{2}) < \exp(\frac{-\delta^2E[X_<]}{3}) = \exp(\frac{-2(k+1)^2m}{3n(n-2(k+1))}).$$
Hence the probability of failure is at most
$$2\exp(\frac{-2(k+1)^2m}{3n(n-2(k+1))})$$
To succeed with probability at least $0.9999$ we want this value to be at most $10^{-5}$. So it will suffice to choose $m$ and $k$ to satisfy
$$\exp(\frac{-2(k+1)^2m}{3n(n-2(k+1))}) \leq 10^{-6}$$
Rearranging yields the expression
$$(k+1)^2m \geq 9\ln(10) n^2 - 18\ln(10) n(k+1)$$
Dropping the non-positive term on the left-hand side, as this can only improve our probability of success, gives us the following expression for the trade-off between $m$ and $k$:
$$(k+1)^2m = 9\ln(10)n^2 = \Omega(n^2).$$
\paragraph{}
If we want $k \leq \epsilon n$ for small $\epsilon$ (say $<\frac{1}{4}$) then we need $m$ to satisfy
$$(\epsilon n+ 1)^2 m = 9\ln(10)n^2$$
i.e. 
$$m = \frac{9\ln(10)n^2}{(\epsilon n + 1)^2}.$$
\paragraph{}
Similarly if we want $k \leq n^{1-\eta}$ for $\eta \leq \frac{1}{2}$ then we take (again assuming desired $k$ is less than $\frac{1}{4}n-1$)
$$m =  \frac{9\ln(10)n^2}{(n^{1-\eta} + 1)^2}.$$
$\blacksquare$
%Problem 4
\newpage
\section{}
\paragraph{}
We assume that the determinant of an $n \times n$ matrix where each element is an entry of $\F[x]$ of degree at most $d$ can be computed in $O(dn^\omega)$ field operations.
\subsection{a}
Let $G=(U\cup V, E)$ be a bipartite graph where each edge is coloured either red or blue. We may assume $|V| = |U|$ by adding dummy vertices as needed. Let $k \in \Z$ be such that $1 \leq k \leq |E|$. We construct an $n\times n$ matrix $A$, where $|A| = n$, of indeterminants as follows: for every pair of vertices $u \in U$, $v\in V$ set
$$A_{uv} = \begin{cases}
x_{uv}, \text{if $uv$ is in $E$ and is coloured blue} \\
rx_{uv}, \text{if $uv$ is in $E$ and is coloured red} \\
0, \text{otherwise}.
\end{cases}$$
with indeterminants $x_{uv}$ for each edge $uv$ and indeterminant $r$. For a perfect matching $M$ on $G$ define $s(M)$ to be the sign of the permutation that sends $i$ to $j$ if $ij \in M$. Also define $c_M$ to be the number of edges coloured red in $M$. Let $\cM$ be the set of perfect matchings on $G$. Then we observe that
$$\det(A) = \sum_{\text{permutations } \sigma}\text{sign}(\sigma) \Pi_{i=1}^n A_{i,\sigma(i)} = \sum_{M \in \cM} s(M) r^{c_M}\Pi_{uv \in M}x_{uv}.$$
Let $\cM_\ell$ be the set of perfect matchings on $G$ with $\ell$ red edges. Then we can rearrange the determinant of $A$ as
$$\det(A) = \sum_{\ell = 1}^n (\sum_{M \in \cM_\ell} s(M)\Pi_{uv \in M}x_{uv})r^\ell.$$
As in the proof of Edmonds' theorem from Lecture $8$ the coefficient of the term corresponding to $r^k$ is not identically zero if and only there is a perfect matching in $G$ with $k$ red edges. This coefficient is a polynomial in variables $x_{uv}$ over $uv \in E$, of degree $n$. So if we substitute values into the variables $x_{uv}$, $uv \in E$, drawn independently and uniformly at random from a finite field $\F$ then by the Schwartz-Zippel Lemma this coefficient will be $0$ with probability at most $\frac{n}{|\F|}$. Choose $\F$ so that this probability is as low as desired.
\paragraph{}
Then after choosing random values for entries $x_{uv}$, $A$ becomes a matrix whose entries are in $\F[r]$ with degree at most $1$. So in $O(n^\omega)$ field operations we can compute $\det(A)$. Then we query the coefficient of the $k$-th term, which will always be $0$ if there are no perfect matchings with $k$ red edges, and with high probability will be non-zero if there is a perfect matching with $k$ red edges. $\blacksquare$.
\subsection{b}
\paragraph{}
Now we suppose that each edge $uv \in E$ has an associated weight $w_{uv}$, and the maximum weight of an edge is $W$. We use a similar idea to the previous problem, this time creating our $n\times n$ matrix $A$ as follows: for each pair $u \in U$, $v\in V$ set
$$A_{uv} = \begin{cases}
	x_{uv}r^{w_{uv}}, &\text{if $uv \in E$} \\
	0, &\text{otherwise.}
	\end{cases}$$
Let $w^*$ be the maximum weight of a perfect matching on $G$. Let $\cM^\ell$ be the set of perfect matchings on $G$ of weight $\ell$. Now $\det(A)$ is a polynomial of the form
$$\det(A) = \sum_{\ell = 1}^{w^*} (\sum_{M \in \cM^\ell} s(M)\Pi_{uv \in M}x_{uv})r^\ell.$$
As previously argued, if we choose large enough $\F$ then drawing the values for $x_{uv}$, $uv \in E$, from $\F$ uniformly independently at random, then by Schwartz-Zippel the coefficient on $r^{w^*}$ in the determinant polynomial will be non-zero with high probability. After assigning values to each $x_{uv}$, $A$ is matrix whose entries are in $\F[r]$ with degree at most $W$. So we can compute $\det(A)$ in O$(Wn^\omega)$ field operations. Querying the degree of the leading term in the resulting polynomial will give the maximum weight of a perfect matching on $G$ with high probability. $\blacksquare$
%Problem 5
\newpage
\section{}
\paragraph{}
Let $G=(V,E)$ be a directed acyclic graph and let $s \in V$ be the only vertex with indegree zero. Let $\F$ be a field such that $|\F| = \Theta(\text{poly}|V|)$. Suppose $s$ has $d$ out-going edges, and to edge $i$ out-going from $s$ we assign the $i$-th standard basis vector of $\F^d$. Apply a topological ordering to the vertices of $G$, and so then $V = \{s=v_1, v_2, \dots, v_n\}$ where $v_iv_j \in E$ implies $j>i$. We refer to the algorithm described in the presentation of question $5$ in the homework as the ``edge connectivity algorithm" or simply ``the algorithm".
\paragraph{}
Each edge $e$ is sending a linear combination of $e_1, \dots, e_d$ of the form $a=c_1e_1 + \dots + c_d e_d$. we call $(c_1,\dots,c_d)$ the global encoding vector of edge $e$. Further if edge $e=uv$ and $u$ has indegree $\ell$ then $a$ is a random linear combination of the $\ell$ incoming messages $a_1 \dots a_\ell$ to $u$. So $a = b_1^{(e)} a_1 + \dots + b_\ell^{(e)} a_\ell$. We call $\{b_i^{(e)}\}$ the local encoding coefficients of edge $e$.
\paragraph{Claim}
For each vertex $v_i \in V$, for each outgoing edge $e$ of $v_i$, each entry $c_j$ of $e$'s global encoding vector is a multivariate polynomial of the local encoding coefficients of total degree at most $i-1$.

\begin{proof}
	We proceed by induction on $i$. Since $s=v_1$ sends $e_j$ on its $j$-th outgoing edge the base case is true.
	
	In the inductive step, consider an edge $e=v_iv_j$. Let $a_1 \dots a_\ell$ be the global encoding vectors of incoming edges to $v_i$. Then each $a_i$ is a polynomial in the local encoding coefficients of degree at most $i-2$ by induction. Now, the global encoding vector for $e$ is
	$$b_1^{(e)}a_1 + \dots + b_\ell^{(e)}a_\ell$$
	which is a sum of multivariate polynomials of degree at most $i-1$, and hence is a polynomial of degree at most $i-1$ as desired.
\end{proof}
\paragraph{}
For vertex $t \in V\backslash \{s\}$ with $\ell$ incoming arcs, let $(c_{i1}, \dots, c_{id})$ be the global encoding vector of the $i$-th incoming edge to $t$. Then we can define the $\ell \times d$ receiver matrix $C$, whose entries are multivariate polynomails over $\F$, where entry $C_{ij} = c_{ij}$ for all $i,j$.
\paragraph{}
Let $C'$ be the largest non-singular submatrix of $C$. The size of $C'$ corresponds to the rank of $C$. So if $\det(C')$ is not $0$ after assigning values to its entries, then the rank of $C$ will be preserved. Since $\det(C')$ is not identically $0$, by Schwartz-Zippel the probability that $\det(C') =0$ after assigning random values for the local encoding coefficients is at most 
$$\frac{dn}{|\F|}$$
Since our Claim implies that the entries of $C'$ are each of degree at most $n$ and so $\det(C')$ is a polynomial of degree at most $dn$. Since $d\leq n$, if we take $|\F| \geq n^4$ this probability of failure is at most 
$$\frac{1}{n^2}.$$
So by the union bound, the probability that the rank of the receiver matrix for each vertex is preserved is at least
$$1-\frac{1}{n}.$$
\paragraph{}
It remains to show that the rank of $C$ is equal to the edge connectivity of $t$. Suppose the edge-connectivity of $t$ is $k$. On the one hand there are $k$ edge-disjoint paths to $t$. If we set the message on each edge of the $i$-th such path to be $e_i$ and the message on every other edge to be $0$ then $C$ will contain (possibly up to permuting the rows) the size $k$ identity matrix, and hence as a matrix of polynomials, the rank of $C$ is at least $k$. On the other hand, by the max-flow min-cut theorem there is an $s$-$t$ cut with exactly $k$ edges. So every row of $C$ is a linear combination of the $k$ vectors along that cut. Hence the rank of $C$ is at most $k$. Therefore the rank of $C$ is equal to $k$. So we have shown the edge-connectivity algorithm works with high probability when you choose a finite field $\F$ so that $|F| \geq n^4 = \Theta(\text{poly}(n))$. $\blacksquare$
\paragraph{Implementation}
We now present an efficient implementation. For each vertex $v \in V$ we replace $v$ with a super-concentrator of size
$$O(\text{in-degree}(v) + \text{out-degree}(v)\}) = O(\deg(v))$$
where $\deg(v)$ is the number of edges incident to $v$ in the underlying undirected graph to $G$. Since the size of the super-concentrator we replace each vertex with is an upper-bound on the number of edge-disjoint paths in $G$ which can use that vertex, we preserve edge-connectivity after this transformation. From the notes, the encoding at each vertex can be done in $O(d)$ time after the transformation. The number of vertices in the new graph is 
$$\sum_{v \in V} O(\deg(v)) = O(2|E|) = O(|E|)$$
by the handshaking-lemma. So the entire encoding can be done in $O(d|E|)$ time.
\paragraph{}
Using the random matrix rank algorithm for Lecture $9$ we can compute the rank of the matrix associated with each vertex in time
$$O(d\cdot \deg(v) + \min\{d,\deg(v)\}^\omega)$$
where $\omega$ is the matrix multiplication constant, with high probability. Summing over vertices $v \in V$, we can compute all the edge connectivities in time
$$O(d|E| + |V|d^\omega).$$
Thus our implementation will succeed in computing the edge-connectivity from $s$ to $t$ for each $t \in V$ with high probability, in time
$$O(d|E| +|V|d^{\omega}).$$
$\blacksquare$
\end{document}
