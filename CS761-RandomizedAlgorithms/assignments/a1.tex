\documentclass[letterpaper,12pt,oneside,onecolumn]{article}
\usepackage[margin=1in, bottom=1in, top=1in]{geometry} %1 inch margins
\usepackage{amsmath, amssymb, amstext}
\usepackage{fancyhdr}
\usepackage{mathtools}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{theorem}
\usepackage{tikz}
\usepackage{tkz-berge}

%Macros
\newcommand{\A}{\mathbb{A}} \newcommand{\C}{\mathbb{C}}
\newcommand{\D}{\mathbb{D}} \newcommand{\F}{\mathbb{F}}
\newcommand{\N}{\mathbb{N}} \newcommand{\R}{\mathbb{R}}
\newcommand{\T}{\mathbb{T}} \newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}


\newcommand{\cA}{\mathcal{A}} \newcommand{\cB}{\mathcal{B}}
\newcommand{\cC}{\mathcal{C}} \newcommand{\cD}{\mathcal{D}}
\newcommand{\cE}{\mathcal{E}} \newcommand{\cF}{\mathcal{F}}
\newcommand{\cG}{\mathcal{G}} \newcommand{\cH}{\mathcal{H}}
\newcommand{\cI}{\mathcal{I}} \newcommand{\cJ}{\mathcal{J}}
\newcommand{\cK}{\mathcal{K}} \newcommand{\cL}{\mathcal{L}}
\newcommand{\cM}{\mathcal{M}} \newcommand{\cN}{\mathcal{N}}
\newcommand{\cO}{\mathcal{O}} \newcommand{\cP}{\mathcal{P}}
\newcommand{\cQ}{\mathcal{Q}} \newcommand{\cR}{\mathcal{R}}
\newcommand{\cS}{\mathcal{S}} \newcommand{\cT}{\mathcal{T}}
\newcommand{\cU}{\mathcal{U}} \newcommand{\cV}{\mathcal{V}}
\newcommand{\cW}{\mathcal{W}} \newcommand{\cX}{\mathcal{X}}
\newcommand{\cY}{\mathcal{Y}} \newcommand{\cZ}{\mathcal{Z}}

\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}


\newenvironment{proof}{{\bf Proof:  }}{\hfill\rule{2mm}{2mm}}
\newenvironment{proofof}[1]{{\bf Proof of #1:  }}{\hfill\rule{2mm}{2mm}}
\newenvironment{proofofnobox}[1]{{\bf#1:  }}{}\newenvironment{example}{{\bf Example:  }}{\hfill\rule{2mm}{2mm}}

%\renewcommand{\thesection}{\lecnum.\arabic{section}}
%\renewcommand{\theequation}{\thesection.\arabic{equation}}
%\renewcommand{\thefigure}{\thesection.\arabic{figure}}

\newtheorem{fact}{Fact}[section]
\newtheorem{lemma}[fact]{Lemma}
\newtheorem{theorem}[fact]{Theorem}
\newtheorem{definition}[fact]{Definition}
\newtheorem{corollary}[fact]{Corollary}
\newtheorem{proposition}[fact]{Proposition}
\newtheorem{claim}[fact]{Claim}
\newtheorem{exercise}[fact]{Exercise}
\newtheorem{note}[fact]{Note}
\newtheorem{conjecture}[fact]{Conjecture}

\newcommand{\size}[1]{\ensuremath{\left|#1\right|}}
\newcommand{\ceil}[1]{\ensuremath{\left\lceil#1\right\rceil}}
\newcommand{\floor}[1]{\ensuremath{\left\lfloor#1\right\rfloor}}

%END MACROS
%Page style
\pagestyle{fancy}

\listfiles

\raggedbottom

\lhead{\today}
\rhead{W. Justin Toth CS761-Randomized Algorithms Assignment 1} %CHANGE n to ASSIGNMENT NUMBER ijk TO COURSE CODE
\renewcommand{\headrulewidth}{1pt} %heading underlined
%\renewcommand{\baselinestretch}{1.2} % 1.2 line spacing for legibility (optional)

\begin{document}
\section{}
\paragraph{}
We modify Karger's algorithm as follows:
\begin{enumerate}
	\item While there are more than $\bf{k}$ vertices in the graph
	\begin{itemize}
		\item Pick an edge uniformly at random and contract the two endpoints.
	\end{itemize}
	\item Output the edges between the remaining $k$ vertices.
\end{enumerate}
\begin{lemma}
	Let $G=(V,E)$ be a graph. The probability that the algorithm outputs a minimum $k$-way cut in $G$ is at least $$\frac{2(k-1)}{n(n-1)\dots(n-2(k-1)+1)}$$ where $n = |V|$.
\end{lemma}
\begin{proof}
	Let $F\subseteq E$ be a minimum $k$-way cut in $G$. Let $G^i=(V^i,E^i)$ be the contracted graph at iteration $i$ of Karger's algorithm. By our stopping condition $|V^i| \geq k$. Since a cut in the contracted graph at each iteration is a cut in the original graph, the minimum $k$-way cut value of $G^i$ is at least $|F|$.
	\paragraph{}
	Let $S\subseteq V^i$ such that $|S|=k-1$. Then $\sum_{v \in S} |\delta(v)| \geq |F|$, for otherwise we have a cut $\bigcup_{v\in S} \delta(v)$ of size smaller than $F$ in $G^i$. Thus by partitioning the vertices of $G^i$ into sets of size $k-1$ we see that
	$$|E^i| \geq \floor{\frac{(n-i+1)|F|}{2(k-1)}}.$$
	Hence the probably that we pick an edge to contract in iteration $i$ from $F$ is at most
	$$ \frac{|F|}{\frac{(n-i+1)|F|}{2(k-1)}} \leq \frac{2(k-1)}{n-i+1}.$$
	Therefore the probability that $F$ survives until the end of the algorithm is at least
	$$(1-\frac{2(k-1)}{n})(1-\frac{2(k-1)}{n-1})(1-\frac{2(k-1)}{n-2})\dots(1-\frac{2(k-1)}{k+1}) = \frac{2(k-1)}{n(n-1)\dots(n-2(k-1)+1)}$$
	as desired.
\end{proof}
\paragraph{}
Using this lemma, we observe that if we repeat Karger's algorithm $t$ times, and take the best cut we find, then our failure probability is at most 
$$\big(1 -  \frac{2(k-1)}{n(n-1)\dots(n-2(k-1)+1)}\big)^t \leq e^\frac{-2(k-1)t}{n(n-1)\dots(n-2(k-1)+1)}.$$
So we can achieve a fixed failure probability of at most $\epsilon \in (0,1)$ if we take 
$$t = \ceil{\frac{\ln(\epsilon)}{-2(k-1)}}n(n-1)\dots(n-2(k-1)+1) \leq n^{ck}\cdot n^{2(k-1)+1} \leq n^{O(k)}$$
for some fixed constant $c$ depending on $\epsilon$.

\newpage
\section{}
\paragraph{}
Let $i > m$. Let $H(i)$ be the event that you hire candidate $i$. Let $B(i)$ be the event that candidate $i$ is the best candidate. Then $E_i = H(i) \cap B(i)$. So using Bayes formula we see that
$$Pr(E_i) = Pr(H(i) \mid B(i)) \cdot Pr(B(i)).$$
Now since the candidates are distributed uniformly at random $Pr(B(i)) = \frac{1}{n}$.
\begin{lemma}
	Let $F(i)$ be the event that the best candidate among the first $i$ candidates appears in the first $m$ candidates. Then
	$$Pr(H(i) \mid B(i)) = Pr(F(i-1)).$$
\end{lemma}
\begin{proof}
	\paragraph{}
	Given that $i$ is the best candidate, we hire $i$ if and only if we reject the first $i-1$ candidates. If we reject the first $i-1$ candidates then the best candidate among the first $i-1$ appears in the first $m-1$ candidates. Otherwise, it would not be rejected when it appears, as it is better than all previously seen candidates. On the other hand if the best candidates among the first $i-1$ appears in the first $m$ candidates, then we will reject the first $i-1$ candidates as none of them are better than a candidate which appears in the first $m$. Thus $H(i)$ happens given $B(i)$ if and only if $F(i-1)$ happens, i.e. the claim holds.
\end{proof}
\paragraph{}
Using this lemma we have
$$Pr(H(i) \mid B(i)) = Pr(F(i-1)) = \frac{m}{i-1}$$
since there are $m$ chances among $i-1$ that the best candidate among the first $i-1$ appears in the first $m$.
Therefore $Pr(E_i) = \frac{m}{n(i-1)}$.
\paragraph{}
Now the event $E$ is the disjoint union of events $E_1, \dots, E_n$. So we observe 
$$Pr(E) = Pr(\bigcup_{i} E_i) = \sum_{i} Pr(E_i) = \frac{m}{n}\sum_{i=m+1}^n \frac{1}{i-1}$$
with the last equality following from our previous observations and the fact that $Pr(E_i) = 0$ for $i \leq m$.
\paragraph{}
We want to show that $Pr(E)$ can get arbitrarily close to $\frac{1}{e}$ for an appropriate choice of $m$. First we see that
$$Pr(E) =  \frac{m}{n}\sum_{i=m+1}^n \frac{1}{i-1} = \frac{m}{n}(\sum_{j=1}^{n-1}\frac{1}{j} - \sum_{j=1}^{m-1}\frac{1}{j}) =\frac{m}{n}(H_{n-1} - H_{m-1}).$$
Using that $\ln(n) \leq H_n \leq \ln(n+1)$ we have
$$Pr(E) \geq \frac{m}{n}(\ln(n-1) - \ln(m)) = \frac{m}{n}\ln(\frac{n-1}{m}).$$
If we choose $m = \frac{n-1}{e}$ then
$$Pr(E) \geq \frac{1}{e}\cdot\frac{n-1}{n}\ln(e) = \frac{1}{e}\cdot\frac{n-1}{n}$$
and taking the limit as $n$ tends to $\infty$ yields
$$\lim_{n\rightarrow \infty} Pr(E) \geq \frac{1}{e}.$$
\newpage
\section{}
\paragraph{}
Let $X$ be a random variable counting the number of cycles in a random permutation. Let $X_i$ be a random variable counting the number of cycles of length $i$ in a permutation. Then $X = \sum_i X_i$. So by linearity of expectation
$$E[X] = \sum_i E[X_i].$$
Now let $X_i^j$ be an indicator variable for $j \in {n\choose i}$ forms a length $i$ cycle in the random permutation. Then 
$$Pr(X^j_i) = \frac{(i-1)!(n-i)!}{n!}$$
since there are $n!$ permutations, and $(i-1)!(n-i)!$ permutations where $j$ forms a length $i$ cycle. To see this, consider a permutation where $j$ forms a length $i$ cycle. There are $(i-1)!$ ways to arrange the elements of $j$ in a length $i$ cycle, and $(n-i)!$ ways to permute the other elements. Therefore
$$E[X_i] = \sum_{j \in {n\choose i}}E[X_i^j] = {n\choose i} \frac{(i-1)!(n-i)!}{n!} = \frac{1}{i}.$$
Then
$$E[X] = \sum_{i=1}^n E[X_i] = \sum_{i=1}^n \frac{1}{i} = H_n.$$
\newpage
\section{}
\paragraph{}
We want to apply a Chernoff bound. So we will need to measure our run time in terms of some independent coin flips. If we can bound, for each element $x$ of our array, the depth of recursion before it is assigned a singleton array, then we can bound the runtime. Let $x$ be an element of our input array. Define $\ell_{x,i}$ to be the length of the array $x$ is assigned to at recursion depth $i$ of our quicksort algorithm. Note that our algorithm terminates at recursion depth $i$ if $\ell_{x,i}=1$ for all $x$.
\paragraph{}
Let $X_{x,i}$ be an indicator random variable indicating $1$ if $\ell_{x,i+1} \leq \frac{3}{4}\ell_{x,i}$, and taking value $0$ otherwise. Intuitively, $X_{x,i}$ indicates that we make a good split of the array containing $x$ at depth $i$ when quicksort is called on that array segment. Let $S_{x,i}$ be the array containing $x$ at recursion depth $i$. If our algorithm selects any element the interval $[\frac{\ell_{x,i}}{4}, \frac{3\ell_{x,i}}{4}]$ from the sorted version $S_{x,i}$ then $\ell_{x,i+1} \leq \frac{3}{4}\ell_{x,i}$. That means choosing one of at least $\frac{1}{2}$ the elements of $S$ as the pivot will yield to a good split, i.e.
$$Pr(X_{x,i}) \geq \frac{1}{2}.$$
So we can think of $X_{x,i}$ as a fair coin flip. Further each $X_{x,i+1}$ is independent of $X_{x,i}$ for all $i$.
\paragraph{}
Let $X_{x} = \sum_{i=1}^{k} X_{x,i}$ where $k=100\ln(n)$. Then if $X_{x} \geq d$ the length of the array containing $x$ at recursion depth $k$ is at most $\frac{3}{4}^d n$. If we want this to be a singleton then we need $$d = \frac{\ln(n)}{\ln{4/3}} \leq 4\ln(n) < 50\ln(n) = \frac{k}{2}$$
So now observing $E[X_{x}] =\frac{k}{2}$ we apply a Chernoff bound (for the lower tail, with $\mu = \frac{k}{2}$ and $\delta = \frac{1}{2}$)
$$Pr(X_{x} \leq \frac{1}{2}\frac{k}{2}) \leq e^{-\frac{k}{2}\frac{1}{8}} \leq e^{\frac{-50\ln(n)}{8}} \leq \frac{1}{n^6}$$
Applying the union bound we see that
$$Pr(\bigcup_x(X_{x} \leq \frac{1}{2}\frac{k}{2})) \leq \sum_{x} Pr(X_{x} \leq \frac{1}{2}\frac{k}{2})  \leq \frac{1}{n^5}.$$
Therefore with inverse polynomial probability, there is some element that is not in a singleton array after recursion depth of $100 \ln(n)$. Since each level of the recursion takes $n$ time, we see that with high probability the quicksort algorithm terminates in $100n\ln(n)$ time.

\newpage
\section{}
\subsection{a}
\paragraph{}
\begin{align*}
M_{X^2}(t) &= E[e^{tX^2}] \\
&= \int_{-\infty}^\infty e^{tx^2}f(x)dx \\
&=\int_{-\infty}^\infty \frac{1}{\sqrt{2\pi}}e^{tx^2-\frac{1}{2}x^2}dx \\
&= \int_{-\infty}^\infty \frac{1}{\sqrt{2\pi}}e^{\frac{-1}{2}(1-2t)x^2}dx \\
&= \frac{1}{\sqrt{1-2t}}\int_{-\infty}^\infty \frac{1}{\sqrt{2\pi}}e^{z^2}dz &\text{Taking $z=\sqrt{1-2t}x$} \\
&= \frac{1}{\sqrt{1-2t}} &\text{Using $\int_{-\infty}^\infty f(x)dx = 1$}.
\end{align*}
\subsection{b}
\paragraph{}
Using Taylor expansion we have
\begin{align*}
E[X^4] = M_{X^2}^{(2)}(0).
\end{align*}
Now observe that
$$M_{X^2}^{(2)}(t) = \frac{3}{(1-2t)^{5/2}}$$
and so
$$M_{X^2}^{(2)}(0) = 3.$$
Therefore $E[X^4] = 3$.
\subsection{c}
(Collaboration with Sharat, Harry and Logan)
\paragraph{}
Let $t$ be a value to be chosen later. We have the following
\begin{align*}
Pr(Y > 1+\epsilon) &= Pr(e^{tY}> e^{t(1+\epsilon)} \\
&\leq E[e^{tY}]\cdot e^{-t(1+\epsilon)}  &\text{Using Markov's inequality} \\
&= (\Pi_{i=1}^k E[e^{\frac{t}{k}X_i^2}])e^{-t(1+\epsilon)} &\text{Using independence of $X_i^2$'s} \\
&=(\frac{1}{1-\frac{2}{k}t})^{\frac{k}{2}}e^{-t(1+\epsilon)} &\text{by $5a$}.
\end{align*}
Taking $t=\frac{k}{2}\frac{\epsilon}{1+\epsilon}$ we have
\begin{align*}
Pr(Y>1+\epsilon) &\leq (1 - \frac{\epsilon}{1+\epsilon})^\frac{-k}{2}e^{\frac{-k}{2}\epsilon}\\
&=e^{-\frac{k}{2}(\ln(1-\frac{\epsilon}{1+\epsilon}) + \epsilon)}
\end{align*}
Therefore it suffices to show $$\ln(1+\epsilon) + \epsilon \geq \frac{\epsilon^2}{4}.$$
This is equivalent to showing
$$\epsilon^2 - 4\epsilon +4\ln(1+\epsilon) \leq 0$$
for $0 \leq \epsilon \leq 1$.
Let $f(\epsilon) = \epsilon^2 - 4\epsilon +4\ln(1+\epsilon)$. We want to take the derivative, showing $f(\epsilon)$ is decreasing on the interval $[0,1]$.
We compute
$$f'(\epsilon) = 2\epsilon - 4 + \frac{4}{1+\epsilon} = \frac{2\epsilon(\epsilon-1)}{1+\epsilon} \leq 0$$
where the last inequality follows since $\epsilon \leq 1$. So $f'(\epsilon) \leq 0$ on $[0,1]$ and so therefore $f(\epsilon)$ is a continuous decreasing function on $[0,1]$.
\paragraph{}
Therefore we can bound $f(\epsilon)$ on $[0,1]$ by $f(1)$. We have
$$f(1) = 1 - 4 + 4\ln(2) = -3 + 4\ln(2) \leq -3 +2.8 <0$$
and so $f(\epsilon) \leq 0$ as desired.
\newpage
\section{}
\paragraph{}
We apply the following theorem from Lecture $4$ (here $H$ is the graph obtain by the uniform sampling algorithm):
\begin{theorem}
	Set $p=\frac{9\ln n}{\epsilon^2 c}$ where $c$ is the min-cut value of $G$. Then $H$ is a $(1\pm \epsilon)$-cut-approximator of $G$ with $O(p\cdot |E(G)|)$ edges with probability $1-\frac{4}{n}$.
\end{theorem}
We suppose that $G$ is a complete graph where each edge has the same weight, say $w$. In $G$ the value of a cut $\delta(S)$ is
$$w|\delta(S)| = w|S|(n-|S|)$$
and this function is minimized when $|S| = 1$, i.e. $S$ is a singleton. Therefore the minimum value of a cut in $G$ is $c = w(n-1)$. Therefore applying the above theorem we have
$$|E(H)| \leq O(p|E(G)|) = O(\frac{9\ln n}{\epsilon^2 w (n-1)}n(n-1)) \leq O(\frac{n\ln n}{\epsilon^2})$$
as desired.
\paragraph{Part 2}
Now we consider an unweighted complete graph $G$. Suppose for a contradiction the uniform sampling algorithm can obtain a  $(1\pm \epsilon)$-cut-approximator $H$ of $G$ with $o(n\ln n)$ edges. To achieve this our sampling probability $p$ can be at most $\frac{\ln n}{c n}$ for some constant $c>1$. Heuristically, lets take
$$p= \frac{\ln n}{100 n}.$$
Now let $v \in V(G)$. Then the probability that the degree of $v$ in $H$ is $0$ is
$$Pr(d_H(v) = 0) = (1- \frac{\ln n}{100n})^{n-1}$$
since the the degree of $v$ is $0$ in $H$ if and only if every one of the $n-1$ edges incident with $v$ is not sampled by the algorithm. So now we bound the probability that this does not happen at any vertex of the graph. Heuristically we treat the events $d_H(v) = 0$ over $v \in V$ as independent events. Then
\begin{align*}
Pr(d_H(v) \geq 1, \forall v \in V) &= \Pi_{v\in V} (1-Pr(d_H(v) = 0)) \\
&=(1-(1- \frac{\ln n}{100n})^{n-1})^n \\
&\leq e^{-n(1- \frac{\ln n}{100n})^{n-1}} &\text{using $1+x \leq e^x$} \\
\end{align*} 
So with extremely low probability $d_H(v) \geq 1$, for all $v \in V$. Hence, with high probability there exists $v \in V$ with $d_H(v) = 0$. Therefore for the cut $\delta(v)$, 
$$w(\delta_H(v)) = 0$$
while 
$$(1\pm \epsilon)\delta_G(v) = (1\pm \epsilon)(n-1)$$
so when $0 <\epsilon < 1$ and $n\geq 2$, $H$ utterly fails to be a $(1\pm \epsilon)$-cut-approximator of $G$.
\newpage
\section{}
\end{document}