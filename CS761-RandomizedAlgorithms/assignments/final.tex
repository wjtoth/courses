\documentclass[letterpaper,12pt,oneside,onecolumn]{article}
\usepackage[margin=1in, bottom=1in, top=1in]{geometry} %1 inch margins
\usepackage{amsmath, amssymb, amstext}
\usepackage{fancyhdr}
\usepackage{mathtools}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{theorem}
\usepackage{tikz}
\usepackage{tkz-berge}

%Macros
\newcommand{\A}{\mathbb{A}} \newcommand{\C}{\mathbb{C}}
\newcommand{\D}{\mathbb{D}} \newcommand{\F}{\mathbb{F}}
\newcommand{\N}{\mathbb{N}} \newcommand{\R}{\mathbb{R}}
\newcommand{\T}{\mathbb{T}} \newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
 
 
\newcommand{\cA}{\mathcal{A}} \newcommand{\cB}{\mathcal{B}}
\newcommand{\cC}{\mathcal{C}} \newcommand{\cD}{\mathcal{D}}
\newcommand{\cE}{\mathcal{E}} \newcommand{\cF}{\mathcal{F}}
\newcommand{\cG}{\mathcal{G}} \newcommand{\cH}{\mathcal{H}}
\newcommand{\cI}{\mathcal{I}} \newcommand{\cJ}{\mathcal{J}}
\newcommand{\cK}{\mathcal{K}} \newcommand{\cL}{\mathcal{L}}
\newcommand{\cM}{\mathcal{M}} \newcommand{\cN}{\mathcal{N}}
\newcommand{\cO}{\mathcal{O}} \newcommand{\cP}{\mathcal{P}}
\newcommand{\cQ}{\mathcal{Q}} \newcommand{\cR}{\mathcal{R}}
\newcommand{\cS}{\mathcal{S}} \newcommand{\cT}{\mathcal{T}}
\newcommand{\cU}{\mathcal{U}} \newcommand{\cV}{\mathcal{V}}
\newcommand{\cW}{\mathcal{W}} \newcommand{\cX}{\mathcal{X}}
\newcommand{\cY}{\mathcal{Y}} \newcommand{\cZ}{\mathcal{Z}}

\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}


\newenvironment{proof}{{\bf Proof:  }}{\hfill\rule{2mm}{2mm}}
\newenvironment{proofof}[1]{{\bf Proof of #1:  }}{\hfill\rule{2mm}{2mm}}
\newenvironment{proofofnobox}[1]{{\bf#1:  }}{}\newenvironment{example}{{\bf Example:  }}{\hfill\rule{2mm}{2mm}}

%\renewcommand{\thesection}{\lecnum.\arabic{section}}
%\renewcommand{\theequation}{\thesection.\arabic{equation}}
%\renewcommand{\thefigure}{\thesection.\arabic{figure}}

\newtheorem{fact}{Fact}[section]
\newtheorem{lemma}[fact]{Lemma}
\newtheorem{theorem}[fact]{Theorem}
\newtheorem{definition}[fact]{Definition}
\newtheorem{corollary}[fact]{Corollary}
\newtheorem{proposition}[fact]{Proposition}
\newtheorem{claim}[fact]{Claim}
\newtheorem{exercise}[fact]{Exercise}
\newtheorem{note}[fact]{Note}
\newtheorem{conjecture}[fact]{Conjecture}

\newcommand{\size}[1]{\ensuremath{\left|#1\right|}}
\newcommand{\ceil}[1]{\ensuremath{\left\lceil#1\right\rceil}}
\newcommand{\floor}[1]{\ensuremath{\left\lfloor#1\right\rfloor}}

%END MACROS
%Page style
\pagestyle{fancy}

\listfiles

\raggedbottom

\lhead{\today}
\rhead{W. Justin Toth CS761-Randomized Algorithms Final Exam} %CHANGE n to ASSIGNMENT NUMBER ijk TO COURSE CODE
\renewcommand{\headrulewidth}{1pt} %heading underlined
%\renewcommand{\baselinestretch}{1.2} % 1.2 line spacing for legibility (optional)

\begin{document}

%Problem 1
\section{}
\paragraph{}
Denote the applicants $A_1, \dots, A_n$ in the order which they appear according to the random permutation. Let $X_i$ be the $0$-$1$ indicator random variable for the event that we pay penalty $C$ for  $A_i$. Hence $X_i=1$ if and only if we hire $A_i$ and later fire $A_i$.
\paragraph{}
We hire $A_i$ if and only if $A_i$ is ranked highest among the first $A_1, \dots, A_i$ applicants. This happens with probability
$$\frac{1}{i}$$
since the ordering was chosen uniformly at random. Given that we hire $A_i$, we fire $A_i$ if and only if $A_i$ is not ranked highest among the following $A_i, \dots, A_n$ applicants. This happens with probability
$$\frac{n-i}{n-i+1}.$$
Since the events that $A_i$ is ranked first among the first $i$ candidates, and not ranked first among the last $n-i+1$ candidates are independent we have
$$Pr(X_i) = \frac{n-i}{i(n-i+1)}.$$
\paragraph{}
Thus our expected total penalty is
$$\sum_{i=1}^n C\cdot Pr(X_i) = C \sum_{i=1}^n\frac{n-i}{i(n-i+1)} = CS_n$$
where $S_n$ denotes $\sum_{i=1}^n\frac{n-i}{i(n-i+1)}$. Now observe that
$$S_n = n\sum_{i=1}^n\frac{1}{i(n-i+1)} - \sum_{i=1}^n\frac{1}{n-i+1}= n\sum_{i=1}^n\frac{1}{i(n-i+1)} - H_n =  n\sum_{i=1}^n\frac{1}{i(n-i+1)} - \log(n)$$
where $H_n$ denotes the $n$-th harmonic number. Now since
$$\int_{x=1}^n \frac{1}{x(n-x+1)} dx = \frac{2\log(n)}{n+1}$$
we have that
$$S_n \leq \frac{2n\log(n)}{n+1} -\log(n) = \frac{(n-1)\log(n)}{n+1}.$$
Therefore our expected total payoff is at most
$$C\cdot \frac{(n-1)\log(n)}{n+1}. \blacksquare$$
%Problem 2
\newpage
\section{}
Let $G$ be an undirected graph, and $C$ the size of a minimum cut in $G$. Let $\alpha \in \R_{\geq 1}$.
\begin{lemma}\label{lemma:2}
Let $F \subseteq E(G)$ be a cut such that $|F| \leq \alpha C$. Then the probability that $F$ is output by Karger's random contraction algorithm is at least
$$\Omega({\frac{1}{n^{2\alpha}}}).$$
\end{lemma}
\begin{proof}
We say the algorithm ``succeeds" if $F$ is output by the algorithm. The algorithm succeeds if and only if we never contract an edge of $F$. At each iteration of the algorithm, every subset of vertices in the contracted graph corresponds to a subset of vertices  in the original graph (by expanding contracted vertices), and hence the minimum cut value of contracted graph at iteration $i$ is at least $C$.
\paragraph{}
In particular the degree of each vertex in the contracted graph is at least $C$. So the number of edges in the $i$-th iteration is at least
$$\frac{(n-i+1)C}{2}.$$
So when we pick a random edge to contract in this iteration, the probability that we pick an edge of $F$ is at most
$$\frac{|F|}{\frac{(n-i+1)C}{2}} \leq \frac{2\alpha}{n-i+1}$$
using that $|F| \leq \alpha C.$ So the probability that the algorithm succeeds is at least
$$\Pi_{i=1}^n(1-\frac{2\alpha}{n-i+1}) = \frac{2\alpha!}{n(n-1)\dots(n-2\alpha + 1)} = \Omega(\frac{1}{n^{2\alpha}})$$
\end{proof}
\begin{claim}\label{claim2}
The number of cuts in $G$ with at most $\alpha C$ edges is at most $O(n^{2\alpha})$.
\end{claim}
\begin{proof}
Let $\cF$ be the set of cuts in $G$ with at most $\alpha C$ edges. For $F \in \cF$ let $X_F$ be the event that Karger's algorithm outputs $F$. By Lemma \ref{lemma:2}
$$Pr(X_F) \geq \Omega(\frac{1}{n^{2\alpha}}).$$
Now we see that, one the one hand
$$Pr(\bigcup_{F \in \cF} X_F) \leq 1$$
and on the other hand, as the events events $X_F$ are all disjoint (the algorithm only outputs one answer),
$$Pr(\bigcup_{F \in \cF} X_F) = \sum_{F \in \cF} Pr(X_F) \leq \sum_{F \in \cF} \Omega(\frac{1}{n^{2\alpha}}) = |\cF|\Omega(\frac{1}{n^{2\alpha}}).$$
Combining inequality and rearranging for $|\cF|$ we have
$$|\cF| \leq O(n^{2\alpha})$$
as desired.
\end{proof}
\paragraph{}
If we repeat Karger's algorithm $t$ times waiting for a particular cut $F \in \cF$ (defined in above proof) we have a failure probability at most
$$(1-\Omega(\frac{1}{n^{2\alpha}}))^t \leq \exp(\frac{t}{O(n^{2{\alpha}})})$$
and hence we can make this failure probability small with a choice of $t \in O(n^{2\alpha})$ (the constant would depend on how good you want the success probability).
\paragraph{}
So we give the following randomized algorithm for enumerating all cuts of $G$ with at most $\alpha C$ edges. Let $k$ be some parameter to be chosen later. Our algorithm proceeds as follows:
\begin{enumerate}
\item Let $\cL = \emptyset$.
\item While fail$<k$
	\begin{enumerate}
	\item Repeat Karger's algorithm at most $t$ times hoping for a cut $F\in \cF\backslash \cL$.
	\item If such $F$ is found, add $F$ to $\cL$ and set fail$=0$.
	\item Otherwise set fail$=$fail$+1$.
	\end{enumerate}
\item Output $\cL$.
\end{enumerate}
\paragraph{}
In words the algorithm uses the boosted version of Karger's to try to find a cut with at most $\alpha C$ edges that it has not seen before. It stops if it fails to find such a cut $k$ times in a row, and prints all the cuts found this way.
\paragraph{}
The algorithm fails if in some $i$-th round it does not find a cut $F \in \cF\backslash \cL$ $k$ times in a row, while $\cF \backslash \cL\neq \emptyset$. We know from \ref{claim2} there are at most $O(n^{2\alpha})$ such rounds $i$. Let $X_i$ be a random variable indicating failure in the $i$-th round. We can choose $t$ so that $t$ repetitions of Karger's algorithm finds $F \in \cF\backslash \cL$ (provided such $F$ exists) with high probability, say $0.99$.
\paragraph{}
Then the probability that we fail in the $i$-th round is
$$(0.01)^k.$$
So the probability we fail in some round is at most
$$\frac{O(n^{2\alpha})}{100^k}.$$
Since $\alpha$ is constant, we can choose $k \in O(\log n)$ to make this failure probability arbitrarily small. Let $T$ be the time to run Karger's algorithm once.  Since there are $O(n^{2\alpha})$ rounds our algorithm takes
$$O(n^{2\alpha}ktT) = O(n^{4\alpha}\log n T)$$
time.$ \blacksquare$

%Problem 3
\newpage
\section{}
\paragraph{}
We throw $n^{0.99}$ balls into $n$ bins uniformly at random.  The probability that a given bin has $k$ balls is 
$$p_k = {n^{0.99} \choose k} (\frac{1}{n})^k(1-\frac{1}{n})^{n^{0.99}-k} = \frac{1}{k!}\frac{n^{0.99}(n^{0.99}-1)\dots(n^{0.99} - k + 1)}{n^k}(1-\frac{1}{n})^{n^{0.99}-k}.$$
This value is approximately
$$p_k \approx \frac{e^{-n^{0.01}}(n^{0.01})^k}{k!}.$$
In other words, the probability that a given has load at least $k$ is a Poisson random variable.  In Poisson, all bins can be treated as independent so the probability that no bin has load at least $k$ is at most
$$(1-\frac{e^{-n^{0.01}}(n^{0.01})^k}{k!})^n\leq \exp(\frac{-e^{-n^{0.01}}n^{1 + \frac{k}{100}}}{k!})$$
To translate the Poisson case back to the exact case, Theorem $5.7$ tells us our final probability will be effected by a factor of at most $e\sqrt{n^{0.99}}$. Since we want a final probability of at most $\frac{1}{2}$ we need to choose $k$ so that 
$$\exp(\frac{-e^{-n^{0.01}}n^{1 + \frac{k}{100}}}{k!}) \leq \frac{1}{2e\sqrt{n^{0.99}}}$$
Or equivalently
$$\frac{-e^{-n^{0.01}}n^{1 + \frac{k}{100}}}{k!} \leq -\ln(2e\sqrt{n^{0.99}}).$$
Equivalently yet again,
$$k! \leq \frac{e^{-n^{0.01}}n^{1 + \frac{k}{100}}}{\ln(2e\sqrt{n^{0.99}})}$$
From $(5.5)$ of Probability and Computing, 
$$k! \leq k(\frac{k}{e})^k.$$
So again we can rewrite our desired inequality as
$$k(\frac{k}{e})^k \leq \frac{e^{-n^{0.01}}n^{1 + \frac{k}{100}}}{\ln(2e\sqrt{n^{0.99}})}$$
and taking logarithms
$$2k\ln(k) \leq \ln(e^{-n^{0.01}}n^{1 + \frac{k}{100}}) - \ln(\ln(2e\sqrt{n^{0.99}}) )= -n^{0.01} + (1+\frac{k}{100})\ln(n) - \ln(\ln(2e\sqrt{n^{0.99}})).$$
Now if we move the $k$ over from the right hand side and multiply the constants through we get the desired inequality
$$k(2\ln(k) - \frac{\ln(n)}{100}) \leq -n^{0.01} + \ln(n) - \ln(\ln(2e\sqrt{n^{0.99}})).$$
I ran out of time, but I hope that solving this equation for $k$ would yield a decent bound.
%Problem 4
\newpage
\section{}
\paragraph{}
We always use addition, $+$, over the appropriate field (ie. $\F_n$ for inputs and $\F_m$ for outputs). Our algorithm is very simple. Given input $z$ choose $x \in \{0,\dots, n-1\}$ uniformly at random. Let $y = z-x$. Lookup $F(x)$ and $F(y)$ and return $F(x) + F(y)$.
\paragraph{}
If both $F(x)$ and $F(y)$ are not modified by our adversary then our return value satisfies
$$F(x) + F(y) = F(x+y) = F(z)$$
using the linearity of $F$ and the fact that $x+y = z$. So a necessary condition for our algorithm to fail is that one of $F(x)$ or $F(y)$ was modified by our adversary. For a uniform random element of the table, the probability of modification is at most $\frac{1}{5}$. Hence
$$Pr(F(x)\text{ or } F(y)\text{ was modified}) \leq Pr(F(x) \text{ was modified}) + Pr(F(y)\text{ was modified}) \leq \frac{2}{5}.$$
Therefore the probability of failure is at most
$$\frac{2}{5} < \frac{1}{2}.$$
Hence our algorithm succeeds with probability at least $\frac{1}{2}$ as desired. $\blacksquare$

%Problem 5
\newpage
\section{}
\paragraph{}
Let $S_1$ and $S_2$ be multisets of positive integers. Let $|S_1| = |S_2| = n$ be the number of elements in each multiset. If the sets are of different sizes it is trivial to decide they are not equal. Let $h:[n] \rightarrow [cn]$ be a $2$-universal hash function chosen uniformly at random.
\paragraph{}
We're going to define multiset operations. They are exactly what you'd expect them to be, so feel free to skip this paragraph as I only add them for sake of completeness. Let $S_1\cap S_2$ denote the largest multiset that is a sub-multiset of $S_1$ and a sub-multiset of $S_2$. By sub-multiset I mean $A$ is a sub-multiset of $B$ ($A\subseteq B$) if and only if for every element $e$ of $A$, the number of times $e$ appears in $A$ is at most the number of times $e$ appears in $B$. We define multiset addition so that $A+B$ is the multiset of elements $e$ so that $e$ appears as many times as it does in $A$ plus as many times as it does in $B$. We also need multiset difference. For multisets $A$ and $B$ we define $A-B$ to be the unique multiset so to that $(A-B) + (A\cap B) = A$. My multiset equality, $A=B$, we meant that $A$ and $B$ have precisely the same elements appearing the same number of times.
\paragraph{}
If $S_1 = S_2$ then $h(S_1) = h(S_2)$ and so the algorithm never gives false negatives. Observe that $h(S_1) = h(S_1\cap S_2) + h(S_1-S_2)$ and $h(S_2) = h(S_1\cap S_2) + h(S_2-S_1)$. Hence the algorithm can only give a false positive if $h(S_1-S_2) = h(S_2 - S_1)$. We will bound the probability of failure by the probability of this happening. Let $d = |S_1-S_2| =|S_2-S_1|$. For $a \in S_1 - S_2$ and $b \in S_2 - S_1$ define $X_{a,b}$ to be the event that $h(a) = h(b)$. Further for each $a \in S_1 - S_2$ define
$$X_a  = \bigcup_{b \in S_2 - S_1} X_{a,b}.$$
Notice that $X_a$ is the disjoint union of events $X_{a,b}$. Let $X$ be a random variable counting the number of collisions between elements in $S_1-S_2$ and elements in $S_2-S_1$. We have
$$E[X] = \sum_{a \in S_1 - S_2} E[X_a] = \sum_{a\in S_1 - S_2} \sum_{b \in S_2-S_1} Pr(h(a) = h(b)) \leq  \sum_{a\in S_1 - S_2} \sum_{b \in S_2-S_1} \frac{1}{cn} = \frac{d^2}{cn}.$$
The inequality follows from $h$ being $2$-universal. For the algorithm to fail it is necessary to have $X \geq d$, for otherwise there are an insufficient number of collisions to have $h(S_1-S_2) = h(S_2-S_1)$. Therefore we can bound the probability of failure by $Pr(X  \geq d)$. By Markov's inequality we see that,
$$Pr(X \geq d) \leq \frac{E[X]}{d} \leq \frac{d}{cn}.$$
Hence our probability of failure is at most $\frac{d}{cn}$. Assuming we can hash in constant time and lookup in a hash table in constant time, the algorithm takes $O(n)$ time to hash each element in $S_1$ and $S_2$, then takes $O(cn)$ time to verify the $cn$ counters each associated with an entry of each of the two hash tables. Since we need $c \geq 1$ to avoid guaranteed collisions, this implies the algorithm runs in $O(cn)$ time with failure probability at most $\frac{d}{cn}$.
\paragraph{}
In the previous paragraph we demonstrated a trade-off between running time and failure probability with respect to choice of parameter $c$. To obtain a Monte Carlo algorithm, we need a failure probability which is independent of the input size. We make our extension by giving an explicit choice of $c$. Let $p \in (0,1)$. Then if we choose $c = \ceil{\frac{d}{pn}}$ then we obtain a failure probability at most $p$ and a running time $O(pd)$. $\blacksquare$
%Problem 6
\newpage
\section{}
\paragraph{}
Let $\cA$ be an algorithm which obtains an $(\epsilon, \frac{1}{4})$-approximation to some parameter $\mu$. That is, when $X$ is the output value of $\cA$,
$$Pr(|X - \mu| \leq \epsilon\mu) \leq \frac{1}{4}.$$
We want to show how to obtain an $(\epsilon,\delta)$-approximation to $\mu$. Suppose we perform $k = \ceil{\log(\frac{1}{\delta})}$ independent runs of $\cA$ and output the median. We may assume $k$ is even, if not replace $k$ with $k+1$. Let $X_1, \dots, X_k$ be random variables each denoting the output value of one run of $\cA$. Instead of ordering them by the order the trials happen, order them sorted by value, i.e.
$$X_1 \leq X_2 \leq \dots\leq X_k.$$
Our $(\epsilon,\delta)$-approximation algorithm outputs $X_\frac{k}{2}$, the median value of our independent trials. Let $X$ denote the output of our new algorithm. We observe that $|X - \mu|\leq \epsilon \mu$ if and only if 
$$|X_i - \mu | \leq \epsilon \mu, \quad \forall i=1 \dots \frac{k}{2}.$$
Therefore
$$Pr(|X-\mu| \leq \epsilon \mu) = \Pi_{i=1}^\frac{k}{2} Pr(|X_i - \mu| \leq \epsilon \mu) \leq \frac{1}{4^\frac{k}{2}}.$$
Since $k = \ceil{\log(\frac{1}{\delta})}$,
$$\frac{1}{4^\frac{k}{2}} \leq \frac{1}{4^{\frac{1}{2}\log(\frac{1}{\delta})}} = \delta.$$
Therefore
$$Pr(|X-\mu| \leq \epsilon \mu) \leq \delta. \blacksquare$$
%Problem 7
\newpage
\section{}
\paragraph{}
Choose $c = 1$. Let $U = \{u_1,\dots, u_n\}$ be a set of ground elements, and let $S_1, \dots, S_m \subseteq U$ be a set of subsets of $U$ such that $|S_i| = \frac{n}{2}$ for all $i\in [m]$. Consider a uniform random coloring of the elements of $U$ either red or blue. For each $i \in [n]$ let $R_i$ be an indicator random variable for the event that $u_i$ is colored red, and similarly let $B_i$ be an indicator random variable for the event that $u_i$ is colored blue. For each $j \in [m]$ let $$X^j := \sum_{u_i \in S_j} R_i - \sum_{u_i \in S_j} B_i.$$
Then showing $S_j$ has between $\frac{n}{4} - c\sqrt{n\log m}$ and $\frac{n}{4} +c\sqrt{n\log m}$ red elements is equivalent to showing $$|X^j| \leq 2c\sqrt{n\log m}.$$
\paragraph{}
For each $i \in [n]$ let $X_i$ be a random variable such that $X_i = 1$ if $u_i$ is red and $X_i = -1$ if $u_i$ is  blue. Since each $S_j$ has $\frac{n}{2}$ elements, each $X^j$ is the sum of exactly $\frac{n}{2}$ such $X_i$. Using Corollary $4.8$ in Probability and Computing by Mitzenmacher and Upfal we have
\begin{align*}
Pr(|X^j| \geq 2c\sqrt{n\log m}) &\leq 2e^\frac{-(2c\sqrt{n\log m})^2}{2\frac{n}{2}} \\
&= 2e^{-4c^2\log m}\\
&=\frac{2}{m^4}\\
&< \frac{1}{m} &\text{ (since $m \geq 2$, otherwise the problem is trivial)}.
\end{align*}
Then, applying a union bound,  the probability that there exists $j\in[m]$ such that $|X^j| \geq 2c\sqrt{n\log m}$ is strictly less than $1$. Thus with positive probability, for all $j\in [m]$ $$|X^j| \leq 2c\sqrt{n\log m}$$
as desired.$\blacksquare$
%Problem 8
\newpage
\section{}
\subsection{a}
\paragraph{}
Suppose $s$ is a random $n$-bit string. Let $k \in [n]$ be a positive integer. Observe that a $k$-consecutive substring in $s$ is uniquely determined by the starting position of the substring.  Therefore there are at most $n$ potential $k$-consecutive substrings in $s$. Denote by $s(i)$ the substring $s_is_{i+1}\dots s_{i+k-1}$. For $s(i)$ to be $k$-consecutive, either bits of $s(i)$ are $0$ or all $1$. Thus,
$$Pr(s(i) \text{ is $k$-consecutive}) = \frac{2}{2^k} = \frac{1}{2^{k-1}}.$$
So by applying union bound, using there are at most $n$ such $s(i)$, the probability there exists a $k$-consecutive substring is at most
$$\frac{n}{2^{k-1}}.$$
To ensure there are no $k$-consecutive substrings with high probability we desire
$$\frac{n}{2^{k-1}} < 0.01$$
Hence we choose
$$k = \log(100n) +1 = O(\log n)$$
to achieve the desired result.$\blacksquare$
\subsection{b}
\paragraph{}
We can model this problem via a random walk. Our states will be $S_0, S_1, \dots, S_k$. $S_0$ transitions to $S_1$ with probability $1$. For $i \in \{1,\dots, k-1\}$, $S_i$ transitions to $S_{i+1}$ with probability $\frac{1}{2}$, and transitions to $S_1$ with probability $\frac{1}{2}$. $S_k$ transitions to $S_k$ with probability $1$. To interpret the original problem as this random walk imagine starting in $S_0$ and reading the random bits of $s$. After reading $1$ bit, $b$, you move from $S_0$ to $S_1$, then as long as you keep seeing the same bit value as $b$ when you read the next bit of $s$ you advance from your current $S_i$ to $S_{i+1}$. But once you see the opposite value to $b$ you return to $S_1$, and treat $\bar{b}$ as the value you're looking for to advance. If at any time you reach $S_k$, you stop, having just read a $k$-consecutive substring of $s$. 
\paragraph{}
We want to bound the expected time to reach $S_k$ from $S_0$. For each $i = 0, \dots k$ let $h_i$ denote the expected number of steps to reach $S_k$. Clearly, $h_0 = h_1 + 1$ and $h_k = 0$. For $j \in [k-1]$ we develop the following recurrence:
$$h_{k-j} = \frac{1}{2}(h_{k-j+1} + 1) + \frac{1}{2}h_1.$$
We claim that for all $j \in [k-1]$, $h_{k-j} = \sum_{i=1}^j \frac{1}{2^i}(1+h_1)$. We proceed by induction. In the base case $j=1$. From our recurrence
$$h_{k-1} = \frac{1}{2}(h_k+1) + \frac{1}{2}h_1 = \frac{1}{2}(1+h_1),$$
using $h_k = 0$. So the base case holds. Now consider $j \in [k-1]\backslash\{1\}$ and suppose the claim holds for $h_{k-j+1}$. Then we have
\begin{align*}
h_{k-j} &= \frac{1}{2}(h_{k-j+1} + 1) + \frac{1}{2}h_1\\
&= \frac{1}{2}( \sum_{i=1}^{j-1} \frac{1}{2^i}(1+h_1) + 1) + \frac{1}{2}h_1 \\
&=\sum_{i=1}^{j-1}\frac{1}{2^{i+1}}(1+h_1) + \frac{1}{2} + \frac{1}{2}h_1\\
&= \sum_{i=2}^{j}\frac{1}{2^{i}}(1+h_1) + \frac{1}{2} + \frac{1}{2}h_1 \\
&= \sum_{i=1}^j \frac{1}{2^i}(1+h_1).
\end{align*}
Therefore the claim holds by induction.
\paragraph{}
So, using partial sums of the geometric series, we have that 
$$h_1 = \sum_{i=1}^{k-1} \frac{1}{2^i}(1+h_1) = \frac{2^{k-1}-1}{2^{k-1}}(1+h_1).$$
Rearranging we see that
$$h_1 = 2^{k-1} - 1$$
which implies that
$$h_0 = 2^{k-1}.$$
Let $X$ denote the number of bits needed before seeing a $k$-consecutive substring. By Markov's inequality, our equation for $h_0$ implies
$$Pr(X \geq 100\cdot2^{k-1}) \leq 0.01.$$
Thus with high probability, $0.99$ we find a $k$-consecutive substring with at most $100 \cdot 2^{k-1}$ bits. That is to say, setting $n = 100\cdot 2^{k-1}$ ensures a $k$-consecutive substring with high probability. Rearranging for $k$, we see that for
$$k =\frac{n}{100}\log(n) + 1$$
$s$ has a $k$-consecutive substring with high probability. $\blacksquare$
%Problem 9
\newpage
\section{}
\paragraph{}
Consider a random walk that starts at a vertex $v \in V$ for $G=(V,E)$ an undirected graph, that stops when it reaches vertex $s$ or vertex $t$. Let $p(v)$ denote the probability that if the random walk starts at $v \in V$ then it reaches $s$ before $t$. It is easy to see that $p(s) = 1$ and $p(t) = 0$.
\paragraph{}
For $v \in G$ we argued in the Lecture $15$, Page $5$, that $h_{x,v} = \phi_{xv}$ for an appropriate flow.
\paragraph{}
Sorry I didn't have time for this problem. My vague idea was to draw a connection between hitting times and the desired probabilities, then relate them back to $\phi$ on some electrical network.
%Problem 10
\newpage
\section{}
\paragraph{}
Let $G$ be a connected, undirected, non-bipartite graph. Consider a Markov Chain $M$ with state space $V(G) \times V(G)$ and transition probabilties:
$$P_{ab, uv} = \begin{cases}
\frac{1}{d(a)d(b)}, &\text{ if } u \in N(a)\text{ and } v \in N(b) \\
0, &\text{otherwise}
\end{cases}$$
Then a state in $M$ represents a position $(a,b)$ where $a$ is the position of the cat and $b$ is the position of the mouse, and $P_{ab,uv}$ represents the probability of the cat being in position $u$ and the mouse being in position $v$ in the subsequent timestep. We can equivalently view $M$ as the Markov chain for a random walk on the graph $G'$ where $V(G') = V(G) \times V(G)$ and $\{(a,b), (u,v)\} \in E(G')$ if and only if $u \in N(a)$ and $v \in N(b)$.
\paragraph{}
Hence $|V(G')| = |V(G)|^2$ and 
$$2|E(G')| = \sum_{u,v \in V(G)} d(u)d(v) = 4|E(G)|^2$$
i.e. $|E(G')| = 2|E(G)|^2$. Note the cat catches the mouse when the random walk on $G'$ reaches vertex $(a,a)$ for some $a \in V(G)$. So if we upper bound $h_{x',y'}$ (the hitting time), for any $x', y' \in V(G')$ then we have successfully upper bounded the expected time for the cat to catch the mouse since it is simply $h_{x',y'}$ where $ux$ is the start state of the cat and mouse, and $y' = (a,a)$ for some $a \in V(G)$.
\paragraph{}
Using Lemma $7.15$ of Probability and Computing we see that 
$$h_{a',b'} < 2|E(G')|$$
provided that $a'b' \in E(G')$. Now consider any $x'=(a,b), y'=(u,v) \in V(G')$. Let $P_1$ be an $au$-path in $G$, which exists since $G$ is connected. Similarly let $P_2$ be a $bv$-path in $G$. Construct an $x'y'$-path $P$ in $G'$ by taking the sequence of vertices $(a,b) \dots (u,b)$ following $P_1$ in the first position and keeping $b$ fixed in the second position, then taking the sequence of vertices $(u,b) \dots (u,v)$ following $P_2$ in the second position and keeping $u$ fixed in the first. Then $|P_2|  = |P_1| + |P_2| \leq 2|V(G)|$.
\paragraph{}
As $P$ is an $x'y'$-path, $h_{x',y'}$ is upper bounded by the sum of hitting times on the edges of $P$. That is, we have
$$h_{x',y'} \leq \sum_{a'b' \in E(P)} h_{a'b'} <  \sum_{a'b' \in E(P)} 2|E(G')| \leq 4|V(G)||E(G')| \leq 8|V(G)||E(G)|^2$$
with the second inequality following from Lemma $7.15$. Thus we have shown an upper bound of $O(|V(G)|\cdot|E(G)|^2)$ on the expected time for the cat to catch the mouse.$\blacksquare$
\end{document}
